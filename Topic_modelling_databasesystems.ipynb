{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthikreddy1010/Automated-Scientific-Data-Paper-Linkage-with-Contextual-Summarization/blob/main/Topic_modelling_databasesystems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVUGtkAo95PJ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install bertopic\n",
        "!pip install umap-learn\n",
        "!pip install hdbscan\n",
        "!pip install sentence-transformers\n",
        "!pip install plotly\n",
        "!pip install wordcloud\n",
        "!pip install gensim\n",
        "!pip install keybert\n",
        "!pip install tqdm\n",
        "!pip install matplotlib\n",
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CLpeNStCpBU"
      },
      "outputs": [],
      "source": [
        "# For CTM support\n",
        "!pip install contextualized-topic-models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime, timezone\n",
        "import torch\n",
        "from bertopic import BERTopic\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import silhouette_score\n",
        "import pickle\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All imports completed successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "\n",
        "\n",
        "\n",
        "class DomainConfig:\n",
        "    # Paths\n",
        "    PROCESSED_TEXT_CSV = \"updated.csv\"\n",
        "    OUTPUT_DIR = \"domain_modeling_results\"\n",
        "\n",
        "    # Embedding model\n",
        "    EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "    # UMAP parameters - OPTIMIZED based on 1st pipeline success\n",
        "    UMAP_PARAMS = {\n",
        "        'n_neighbors': 8,           # Balanced for local/global structure\n",
        "        'n_components': 3,          # Reduced for better clustering (was 5)\n",
        "        'min_dist': 0.05,           # Balanced separation (was 0.01)\n",
        "        'metric': 'cosine',\n",
        "        'random_state': 42,\n",
        "        'low_memory': False\n",
        "    }\n",
        "\n",
        "    # HDBSCAN parameters - OPTIMIZED for better topics\n",
        "    HDBSCAN_PARAMS = {\n",
        "        'min_cluster_size': 8,      # Increased for better topics (was 6)\n",
        "        'min_samples': 3,           # Increased for stability (was 2)\n",
        "        'cluster_selection_epsilon': 0.02,\n",
        "        'metric': 'euclidean',\n",
        "        'cluster_selection_method': 'eom',  # Better than 'leaf'\n",
        "        'prediction_data': True\n",
        "    }\n",
        "\n",
        "    # BERTopic settings - OPTIMIZED based on 1st pipeline success\n",
        "    BERTOPIC_SETTINGS = {\n",
        "        'top_n_words': 12,\n",
        "        'n_gram_range': (1, 2),     # Bigrams better than trigrams\n",
        "        'min_topic_size': 8,        # Increased for better topics (was 6)\n",
        "        'calculate_probabilities': True,\n",
        "        'verbose': False,\n",
        "        'nr_topics': 20             # Set target instead of 'auto'\n",
        "    }\n",
        "\n",
        "    # Processing settings\n",
        "    MIN_DOC_LENGTH = 50\n",
        "    MAX_DOC_LENGTH = 2000\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    def __init__(self):\n",
        "        os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "config = DomainConfig()\n",
        "print(f\" Configuration initialized\")\n",
        "\n",
        "\n",
        "\n",
        "class DataProcessor:\n",
        "    def __init__(self):\n",
        "        # Scientific stop words - preserve domain terminology\n",
        "        self.scientific_stop_words = set([\n",
        "            'paper', 'study', 'research', 'result', 'method', 'approach',\n",
        "            'show', 'demonstrate', 'present', 'investigate', 'analyze',\n",
        "            'discuss', 'conclude', 'suggest', 'indicate', 'figure', 'table',\n",
        "            'author', 'journal', 'publication', 'reference', 'citation',\n",
        "            'section', 'abstract', 'introduction', 'background', 'conclusion'\n",
        "        ])\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean text while preserving scientific content\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        text = text.lower().strip()\n",
        "\n",
        "        if len(text) < config.MIN_DOC_LENGTH:\n",
        "            return \"\"\n",
        "\n",
        "        if len(text) > config.MAX_DOC_LENGTH:\n",
        "            text = text[:config.MAX_DOC_LENGTH]\n",
        "\n",
        "        # Remove URLs, citations, and metadata\n",
        "        patterns_to_remove = [\n",
        "            r'http\\S+|www\\S+|https\\S+',\n",
        "            r'doi:\\s*\\S+',\n",
        "            r'\\S*@\\S*\\s?',\n",
        "            r'copyright\\s+\\d{4}',\n",
        "            r'all rights reserved',\n",
        "            r'received|accepted|submitted',\n",
        "            r'creative commons',\n",
        "            r'peer review'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns_to_remove:\n",
        "            text = re.sub(pattern, ' ', text)\n",
        "\n",
        "        # Clean up whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Filter words - preserve scientific terms\n",
        "        words = text.split()\n",
        "        cleaned_words = []\n",
        "\n",
        "        for word in words:\n",
        "            if (len(word) > 2 and\n",
        "                len(word) < 25 and\n",
        "                word not in self.scientific_stop_words and\n",
        "                not word.isdigit()):\n",
        "                cleaned_words.append(word)\n",
        "\n",
        "        cleaned_text = ' '.join(cleaned_words).strip()\n",
        "        return cleaned_text if len(cleaned_text) >= config.MIN_DOC_LENGTH else \"\"\n",
        "\n",
        "    def load_and_process_data(self):\n",
        "        \"\"\"Load and process data\"\"\"\n",
        "        try:\n",
        "            print(\" Loading and processing data...\")\n",
        "\n",
        "            df = pd.read_csv(config.PROCESSED_TEXT_CSV)\n",
        "            print(f\" Loaded {len(df)} documents\")\n",
        "\n",
        "            # Apply cleaning\n",
        "            df['cleaned_text'] = df['processed_text'].apply(self.clean_text)\n",
        "\n",
        "            # Remove empty documents\n",
        "            initial_count = len(df)\n",
        "            df = df[df['cleaned_text'].str.len() > config.MIN_DOC_LENGTH].copy()\n",
        "            final_count = len(df)\n",
        "\n",
        "            if initial_count != final_count:\n",
        "                print(f\" Removed {initial_count - final_count} documents after cleaning\")\n",
        "\n",
        "            # Text statistics\n",
        "            text_lengths = df['cleaned_text'].str.len()\n",
        "            word_counts = df['cleaned_text'].str.split().str.len()\n",
        "\n",
        "            print(f\" Final dataset: {final_count} documents\")\n",
        "            print(f\" Average length: {text_lengths.mean():.1f} chars, {word_counts.mean():.1f} words\")\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Data loading failed: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "\n",
        "class EnhancedDomainClassifier:\n",
        "    def __init__(self):\n",
        "        # Comprehensive domain keywords with FIXED coverage\n",
        "        self.domain_keywords = {\n",
        "            'biology': [\n",
        "                'cell', 'gene', 'protein', 'dna', 'genetic', 'molecular', 'organism',\n",
        "                'evolution', 'genome', 'species', 'ecological', 'biodiversity',\n",
        "                'microbial', 'enzyme', 'metabolism', 'phylogenetic', 'transcription',\n",
        "                'rna', 'chromosome', 'mitochondria', 'apoptosis', 'sequence',\n",
        "                'mutation', 'expression', 'cellular', 'developmental', 'plant',\n",
        "                'animal', 'bacterial', 'viral', 'evolutionary', 'population',\n",
        "                'biological', 'physiology', 'genomic', 'proteomic', 'transcriptomic',\n",
        "                'microbiome', 'neuroscience', 'zoology', 'botany', 'ecology',\n",
        "                'soybean', 'fruit', 'circrnas', 'drosophila', 'nutrient', 'invasive',\n",
        "                'breast', 'strain', 'speech', 'brain', 'neural', 'jet', 'physics',\n",
        "                'collision', 'proton', 'oxygen', 'sex', 'temperature', 'thermal'\n",
        "            ],\n",
        "            'medicine': [\n",
        "                'patient', 'clinical', 'treatment', 'disease', 'medical', 'therapy',\n",
        "                'health', 'drug', 'vaccine', 'diagnosis', 'symptom', 'hospital',\n",
        "                'pharmaceutical', 'epidemiology', 'pathology', 'oncology', 'immunology',\n",
        "                'surgery', 'prognosis', 'biomarker', 'clinical trial', 'pharmacology',\n",
        "                'therapeutic', 'dosage', 'recovery', 'mortality', 'morbidity', 'cancer',\n",
        "                'tumor', 'infection', 'inflammatory', 'neurological', 'cardiology',\n",
        "                'pediatric', 'geriatric', 'psychiatry', 'radiology', 'anesthesia',\n",
        "                'public health', 'virology', 'bacteriology', 'dose', 'imaging',\n",
        "                'muscle', 'care', 'inflammation', 'mental', 'virus', 'sars', 'vector',\n",
        "                'mouse', 'dam', 'like', 'wash', 'londrina'\n",
        "            ],\n",
        "            'chemistry': [\n",
        "                'molecule', 'reaction', 'compound', 'chemical', 'synthesis',\n",
        "                'catalyst', 'polymer', 'organic', 'inorganic', 'spectroscopy',\n",
        "                'chromatography', 'crystallography', 'stoichiometry', 'kinetics',\n",
        "                'nmr', 'mass spectrometry', 'electrochemistry', 'photochemistry',\n",
        "                'reagent', 'solvent', 'yield', 'purification', 'characterization',\n",
        "                'crystal', 'bond', 'structure', 'atomic', 'molecular', 'analytical',\n",
        "                'physical chemistry', 'quantum chemistry', 'medicinal chemistry',\n",
        "                'biochemistry', 'polymer chemistry', 'materials chemistry',\n",
        "                'irrigation', 'management', 'soil', 'water'\n",
        "            ],\n",
        "            'environmental_science': [\n",
        "                'environmental', 'climate', 'ecosystem', 'sustainability', 'pollution',\n",
        "                'conservation', 'biodiversity', 'ecological', 'environmental impact',\n",
        "                'climate change', 'environmental management', 'sustainable development',\n",
        "                'habitat', 'wildlife', 'conservation', 'environmental policy', 'earth',\n",
        "                'geological', 'ocean', 'atmospheric', 'marine', 'terrestrial',\n",
        "                'agricultural', 'forestry', 'water resources', 'air quality', 'soil science',\n",
        "                'environmental engineering', 'conservation biology', 'environmental health',\n",
        "                'ice', 'university', 'usa', 'fish', 'population', 'ecology',\n",
        "                'geological survey', 'santa', 'survey', 'rupture'\n",
        "            ],\n",
        "            'computer_science': [\n",
        "                'algorithm', 'software', 'programming', 'machine learning',\n",
        "                'neural network', 'data', 'system', 'computational', 'artificial intelligence',\n",
        "                'database', 'network', 'optimization', 'cybersecurity', 'blockchain',\n",
        "                'deep learning', 'computer vision', 'natural language processing',\n",
        "                'computation', 'modeling', 'simulation', 'data analysis', 'big data',\n",
        "                'cloud computing', 'internet of things', 'robotics', 'automation'\n",
        "            ],\n",
        "            'physics': [\n",
        "                'quantum', 'particle', 'energy', 'field', 'mechanics', 'astrophysics',\n",
        "                'relativity', 'nuclear', 'optics', 'thermodynamics', 'electromagnetic',\n",
        "                'condensed matter', 'cosmology', 'entanglement', 'superconductivity',\n",
        "                'atomic', 'molecular', 'theoretical', 'experimental', 'quantum mechanics',\n",
        "                'statistical mechanics', 'fluid dynamics', 'plasma physics', 'optics',\n",
        "                'astronomy', 'cosmology', 'particle physics', 'solid state physics'\n",
        "            ],\n",
        "            'engineering': [\n",
        "                'design', 'system', 'manufacturing', 'structural', 'electrical',\n",
        "                'mechanical', 'control', 'sensor', 'robotics', 'automation',\n",
        "                'aerospace', 'civil', 'materials', 'nanotechnology', 'mechatronics',\n",
        "                'biomedical', 'chemical engineering', 'environmental engineering'\n",
        "            ],\n",
        "            'materials_science': [\n",
        "                'material', 'composite', 'polymer', 'ceramic', 'metal', 'alloy',\n",
        "                'nanomaterial', 'crystal', 'structure', 'properties', 'synthesis',\n",
        "                'fabrication', 'characterization', 'mechanical properties', 'thermal properties',\n",
        "                'electronic properties', 'optical properties', 'material design',\n",
        "                'heat', 'strength', 'stimulus', 'group'\n",
        "            ],\n",
        "            'psychology': [\n",
        "                'behavior', 'cognitive', 'psychological', 'mental', 'personality',\n",
        "                'emotion', 'memory', 'neural', 'brain', 'perception', 'learning',\n",
        "                'cognition', 'developmental', 'social psychology', 'clinical psychology',\n",
        "                'behavioral', 'psychiatric', 'neuroscience', 'cognitive science',\n",
        "                'speech', 'elife', 'neuroscience', 'university'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Domain relationships for ambiguous cases\n",
        "        self.domain_relationships = {\n",
        "            'ecology': ['biology', 'environmental_science'],\n",
        "            'neuroscience': ['biology', 'psychology', 'medicine'],\n",
        "            'biochemistry': ['biology', 'chemistry'],\n",
        "            'bioinformatics': ['biology', 'computer_science'],\n",
        "            'materials_science': ['chemistry', 'engineering', 'physics'],\n",
        "            'environmental_health': ['environmental_science', 'medicine']\n",
        "        }\n",
        "\n",
        "    def _enhanced_domain_scoring(self, topic_words, topic_text):\n",
        "        \"\"\"FIXED domain scoring - prevents division by zero\"\"\"\n",
        "        domain_scores = {}\n",
        "\n",
        "        for domain, keywords in self.domain_keywords.items():\n",
        "            score = 0.1  # CRITICAL FIX: Base score to prevent division by zero\n",
        "            matches = []\n",
        "\n",
        "            # Position-based scoring (words in top positions are more important)\n",
        "            for i, word in enumerate(topic_words[:8]):\n",
        "                if word in keywords:\n",
        "                    # Position bonus: higher score for words appearing earlier\n",
        "                    position_weight = max(0, (8 - i) / 8) * 2.0\n",
        "                    score += 1.0 + position_weight\n",
        "                    matches.append((word, i))\n",
        "\n",
        "            # Contextual presence scoring\n",
        "            for keyword in keywords:\n",
        "                if keyword in topic_text and keyword not in [m[0] for m in matches]:\n",
        "                    if ' ' in keyword:  # Multi-word terms\n",
        "                        if keyword in topic_text:\n",
        "                            score += 0.8\n",
        "                    else:  # Single words\n",
        "                        score += 0.3\n",
        "\n",
        "            # Multi-match bonus\n",
        "            if len(matches) >= 3:\n",
        "                score *= 1.3\n",
        "            elif len(matches) >= 2:\n",
        "                score *= 1.15\n",
        "\n",
        "            # Strong domain indicator bonus\n",
        "            strong_indicators = self._get_strong_domain_indicators(domain)\n",
        "            strong_matches = [word for word in topic_words[:4] if word in strong_indicators]\n",
        "            if strong_matches:\n",
        "                score *= 1.2\n",
        "\n",
        "            domain_scores[domain] = score\n",
        "\n",
        "        return domain_scores\n",
        "\n",
        "    def _get_strong_domain_indicators(self, domain):\n",
        "        \"\"\"Get strong indicator words for each domain\"\"\"\n",
        "        strong_indicators = {\n",
        "            'biology': ['gene', 'cell', 'dna', 'protein', 'genome', 'evolution'],\n",
        "            'medicine': ['patient', 'clinical', 'treatment', 'diagnosis', 'therapy', 'hospital'],\n",
        "            'chemistry': ['molecule', 'reaction', 'compound', 'synthesis', 'catalyst'],\n",
        "            'environmental_science': ['climate', 'ecosystem', 'pollution', 'conservation', 'habitat'],\n",
        "            'psychology': ['behavior', 'cognitive', 'mental', 'brain', 'memory'],\n",
        "            'physics': ['quantum', 'particle', 'energy', 'mechanics', 'relativity'],\n",
        "            'materials_science': ['material', 'composite', 'ceramic', 'polymer', 'alloy']\n",
        "        }\n",
        "        return strong_indicators.get(domain, [])\n",
        "\n",
        "    def _resolve_ambiguous_domains(self, domain_scores, topic_words):\n",
        "        \"\"\"Resolve ambiguous domain classifications\"\"\"\n",
        "        # Filter out zero scores\n",
        "        domain_scores = {k: v for k, v in domain_scores.items() if v > 0}\n",
        "\n",
        "        if not domain_scores:\n",
        "            return 'unknown'\n",
        "\n",
        "        sorted_domains = sorted(domain_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        if len(sorted_domains) < 2:\n",
        "            return sorted_domains[0][0]\n",
        "\n",
        "        best_domain, best_score = sorted_domains[0]\n",
        "        second_domain, second_score = sorted_domains[1]\n",
        "\n",
        "        # Clear winner (score difference > 50%)\n",
        "        if best_score > second_score * 1.5:\n",
        "            return best_domain\n",
        "\n",
        "        # Check for domain relationships\n",
        "        for relationship, related_domains in self.domain_relationships.items():\n",
        "            if best_domain in related_domains and second_domain in related_domains:\n",
        "                # Choose based on stronger indicators\n",
        "                best_indicators = sum(1 for word in topic_words[:4] if word in self._get_strong_domain_indicators(best_domain))\n",
        "                second_indicators = sum(1 for word in topic_words[:4] if word in self._get_strong_domain_indicators(second_domain))\n",
        "\n",
        "                if best_indicators > second_indicators:\n",
        "                    return best_domain\n",
        "                elif second_indicators > best_indicators:\n",
        "                    return second_domain\n",
        "\n",
        "        # Default to highest score\n",
        "        return best_domain\n",
        "\n",
        "    def classify_topic_domains(self, topic_model, topics):\n",
        "        \"\"\"FIXED domain classification with error handling\"\"\"\n",
        "        print(\" Performing domain classification...\")\n",
        "\n",
        "        topic_domain_mapping = {}\n",
        "        unique_topics = set(topics) - {-1}\n",
        "\n",
        "        for topic_id in unique_topics:\n",
        "            try:\n",
        "                topic_words_data = topic_model.get_topic(topic_id)\n",
        "                if not topic_words_data:\n",
        "                    print(f\"    No words for topic {topic_id}\")\n",
        "                    continue\n",
        "\n",
        "                # Get topic words\n",
        "                all_topic_words = [word for word, _ in topic_words_data[:12]]\n",
        "                if not all_topic_words:\n",
        "                    print(f\"    Empty topic words for topic {topic_id}\")\n",
        "                    continue\n",
        "\n",
        "                topic_text = ' '.join(all_topic_words).lower()\n",
        "\n",
        "                # Calculate enhanced domain scores\n",
        "                domain_scores = self._enhanced_domain_scoring(all_topic_words, topic_text)\n",
        "\n",
        "                if not domain_scores:\n",
        "                    print(f\"    No domain scores for topic {topic_id}\")\n",
        "                    continue\n",
        "\n",
        "                # Resolve ambiguous domains\n",
        "                best_domain = self._resolve_ambiguous_domains(domain_scores, all_topic_words)\n",
        "\n",
        "                if best_domain == 'unknown':\n",
        "                    print(f\"    Could not determine domain for topic {topic_id}\")\n",
        "                    continue\n",
        "\n",
        "                # Calculate confidence\n",
        "                total_score = sum(domain_scores.values())\n",
        "                confidence = domain_scores[best_domain] / max(total_score, 1)\n",
        "\n",
        "                # Adjust confidence based on score dominance\n",
        "                sorted_scores = sorted(domain_scores.values(), reverse=True)\n",
        "                if len(sorted_scores) > 1:\n",
        "                    score_ratio = sorted_scores[0] / sorted_scores[1]\n",
        "                    if score_ratio > 2.0:\n",
        "                        confidence = min(1.0, confidence * 1.3)\n",
        "                    elif score_ratio > 1.5:\n",
        "                        confidence = min(1.0, confidence * 1.15)\n",
        "\n",
        "                # Ensure JSON serializable types\n",
        "                topic_domain_mapping[int(topic_id)] = {\n",
        "                    'primary_domain': best_domain,\n",
        "                    'confidence': float(confidence),\n",
        "                    'all_domain_scores': {k: float(v) for k, v in domain_scores.items()},\n",
        "                    'topic_keywords': all_topic_words[:8]\n",
        "                }\n",
        "\n",
        "                # Display classification\n",
        "                confidence_level = \"HIGH\" if confidence > 0.7 else \"MEDIUM\" if confidence > 0.4 else \"LOW\"\n",
        "                top_keywords = ', '.join(all_topic_words[:4])\n",
        "                print(f\"   Topic {topic_id:2d} → {best_domain:20s} (conf: {confidence:.3f} [{confidence_level}]) - {top_keywords}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    Error classifying topic {topic_id}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        self._print_domain_analysis(topic_domain_mapping)\n",
        "        return topic_domain_mapping\n",
        "\n",
        "    def _print_domain_analysis(self, domain_mapping):\n",
        "        \"\"\"Print domain analysis\"\"\"\n",
        "        if not domain_mapping:\n",
        "            print(\" No topics to analyze\")\n",
        "            return\n",
        "\n",
        "        domain_counts = Counter()\n",
        "        confidence_sum = Counter()\n",
        "\n",
        "        for mapping in domain_mapping.values():\n",
        "            domain = mapping['primary_domain']\n",
        "            domain_counts[domain] += 1\n",
        "            confidence_sum[domain] += mapping['confidence']\n",
        "\n",
        "        print(f\"\\n DOMAIN DISTRIBUTION:\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        total_topics = len(domain_mapping)\n",
        "        for domain, count in domain_counts.most_common():\n",
        "            percentage = (count / total_topics) * 100\n",
        "            avg_confidence = confidence_sum[domain] / count\n",
        "            confidence_level = \"HIGH\" if avg_confidence > 0.7 else \"MEDIUM\" if avg_confidence > 0.5 else \"LOW\"\n",
        "            print(f\"   {domain:20s}: {count:2d} topics ({percentage:5.1f}%) [avg conf: {avg_confidence:.3f} - {confidence_level}]\")\n",
        "\n",
        "\n",
        "\n",
        "class TopicModeler:\n",
        "    def __init__(self):\n",
        "        self.topic_model = None\n",
        "        self.embeddings = None\n",
        "        self.topics = None\n",
        "        self.probabilities = None\n",
        "\n",
        "    def initialize_model(self):\n",
        "        \"\"\"Initialize BERTopic model with OPTIMIZED parameters\"\"\"\n",
        "        print(\" Initializing BERTopic model...\")\n",
        "\n",
        "        # UMAP for dimensionality reduction\n",
        "        umap_model = UMAP(**config.UMAP_PARAMS)\n",
        "\n",
        "        # HDBSCAN for clustering\n",
        "        hdbscan_model = HDBSCAN(**config.HDBSCAN_PARAMS)\n",
        "\n",
        "        # Vectorizer for text processing\n",
        "        vectorizer_model = CountVectorizer(\n",
        "            stop_words=\"english\",\n",
        "            ngram_range=config.BERTOPIC_SETTINGS['n_gram_range'],\n",
        "            min_df=2,\n",
        "            max_df=0.95,\n",
        "            max_features=8000\n",
        "        )\n",
        "\n",
        "        # Initialize BERTopic\n",
        "        self.topic_model = BERTopic(\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            vectorizer_model=vectorizer_model,\n",
        "            top_n_words=config.BERTOPIC_SETTINGS['top_n_words'],\n",
        "            min_topic_size=config.BERTOPIC_SETTINGS['min_topic_size'],\n",
        "            calculate_probabilities=config.BERTOPIC_SETTINGS['calculate_probabilities'],\n",
        "            verbose=config.BERTOPIC_SETTINGS['verbose'],\n",
        "            nr_topics=config.BERTOPIC_SETTINGS['nr_topics']\n",
        "        )\n",
        "\n",
        "        print(\" BERTopic model initialized\")\n",
        "\n",
        "    def fit_model(self, documents):\n",
        "        \"\"\"Fit topic model to documents with enhanced processing\"\"\"\n",
        "        print(\" Fitting topic model...\")\n",
        "\n",
        "        # Generate embeddings\n",
        "        model = SentenceTransformer(config.EMBEDDING_MODEL)\n",
        "        self.embeddings = model.encode(\n",
        "            documents,\n",
        "            batch_size=config.BATCH_SIZE,\n",
        "            show_progress_bar=True,\n",
        "            normalize_embeddings=True\n",
        "        )\n",
        "\n",
        "        # Initialize model\n",
        "        self.initialize_model()\n",
        "\n",
        "        # Fit model with error handling\n",
        "        try:\n",
        "            self.topics, self.probabilities = self.topic_model.fit_transform(\n",
        "                documents, self.embeddings\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\" Primary fitting failed: {e}\")\n",
        "            print(\" Using fallback model...\")\n",
        "            # Fallback with simpler parameters\n",
        "            self.topic_model = BERTopic(\n",
        "                min_topic_size=10,\n",
        "                verbose=False,\n",
        "                nr_topics=15\n",
        "            )\n",
        "            self.topics, self.probabilities = self.topic_model.fit_transform(\n",
        "                documents, self.embeddings\n",
        "            )\n",
        "\n",
        "        # Apply topic optimization if too few topics\n",
        "        self._optimize_topic_count(documents)\n",
        "\n",
        "        # Print results\n",
        "        self._print_results()\n",
        "\n",
        "        return self.topics, self.probabilities\n",
        "\n",
        "    def _optimize_topic_count(self, documents):\n",
        "        \"\"\"Optimize topic count if too few topics found\"\"\"\n",
        "        unique_topics = len(set(self.topics)) - (1 if -1 in self.topics else 0)\n",
        "        print(f\" Initial topics found: {unique_topics}\")\n",
        "\n",
        "        # If very few topics, try to reduce further\n",
        "        if unique_topics < 8 and hasattr(self.topic_model, 'reduce_topics'):\n",
        "            try:\n",
        "                print(\" Attempting to discover more topics...\")\n",
        "                target_topics = min(20, len(documents) // 25)  # Reasonable target\n",
        "                self.topics, self.probabilities = self.topic_model.reduce_topics(\n",
        "                    documents, self.topics, self.probabilities, nr_topics=target_topics\n",
        "                )\n",
        "                new_unique = len(set(self.topics)) - (1 if -1 in self.topics else 0)\n",
        "                print(f\" Topics after optimization: {new_unique}\")\n",
        "            except Exception as e:\n",
        "                print(f\" Topic optimization failed: {e}\")\n",
        "\n",
        "    def _print_results(self):\n",
        "        \"\"\"Print topic modeling results\"\"\"\n",
        "        unique_topics = len(set(self.topics)) - (1 if -1 in self.topics else 0)\n",
        "        outliers = np.sum(self.topics == -1)\n",
        "\n",
        "        # Calculate topic statistics\n",
        "        topic_counts = Counter(self.topics)\n",
        "        if -1 in topic_counts:\n",
        "            valid_topics = {k: v for k, v in topic_counts.items() if k != -1}\n",
        "        else:\n",
        "            valid_topics = topic_counts\n",
        "\n",
        "        if valid_topics:\n",
        "            sizes = list(valid_topics.values())\n",
        "            stats = {\n",
        "                'min_size': min(sizes),\n",
        "                'max_size': max(sizes),\n",
        "                'avg_size': np.mean(sizes),\n",
        "                'std_size': np.std(sizes)\n",
        "            }\n",
        "        else:\n",
        "            stats = {'min_size': 0, 'max_size': 0, 'avg_size': 0, 'std_size': 0}\n",
        "\n",
        "        print(f\"\\n TOPIC MODELING RESULTS:\")\n",
        "        print(f\"   • Topics found: {unique_topics}\")\n",
        "        print(f\"   • Outliers: {outliers} ({outliers/len(self.topics)*100:.1f}%)\")\n",
        "        print(f\"   • Topic size range: {stats['min_size']} - {stats['max_size']}\")\n",
        "        print(f\"   • Average topic size: {stats['avg_size']:.1f}\")\n",
        "\n",
        "        # Show topic details\n",
        "        if hasattr(self.topic_model, 'get_topic_info'):\n",
        "            try:\n",
        "                topic_info = self.topic_model.get_topic_info()\n",
        "                print(f\"\\n TOPIC DETAILS:\")\n",
        "                print(\"=\" * 70)\n",
        "\n",
        "                for _, row in topic_info.iterrows():\n",
        "                    topic_id = row['Topic']\n",
        "                    if topic_id != -1:\n",
        "                        topic_words = self.topic_model.get_topic(topic_id)\n",
        "                        if topic_words:\n",
        "                            words = [word for word, _ in topic_words[:6]]\n",
        "                            size_percentage = (row['Count'] / len(self.topics)) * 100\n",
        "                            print(f\"   Topic {topic_id:2d} ({row['Count']:3d} docs, {size_percentage:4.1f}%): {', '.join(words)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   • Could not extract topic details: {e}\")\n",
        "\n",
        "# =============================================================================\n",
        "# IMPROVED MODEL EVALUATOR\n",
        "# =============================================================================\n",
        "\n",
        "class ModelEvaluator:\n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "\n",
        "    def evaluate_model(self, topic_model, documents, topics, embeddings):\n",
        "        \"\"\"IMPROVED model evaluation with better metrics\"\"\"\n",
        "        print(\" Performing model evaluation...\")\n",
        "\n",
        "        evaluation = {}\n",
        "\n",
        "        # Basic statistics\n",
        "        unique_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
        "        outliers = np.sum(topics == -1)\n",
        "\n",
        "        evaluation['basic_stats'] = {\n",
        "            'total_documents': int(len(documents)),\n",
        "            'topics_found': int(unique_topics),\n",
        "            'outliers': int(outliers),\n",
        "            'outlier_percentage': float((outliers / len(topics)) * 100)\n",
        "        }\n",
        "\n",
        "        # Topic coherence - IMPROVED calculation\n",
        "        coherence = self._calculate_robust_coherence(topic_model, documents, topics)\n",
        "        evaluation['coherence_score'] = float(coherence)\n",
        "\n",
        "        # Topic quality metrics\n",
        "        topic_quality = self._analyze_topic_quality(topics)\n",
        "        evaluation['topic_quality'] = topic_quality\n",
        "\n",
        "        # Additional metrics\n",
        "        evaluation['additional_metrics'] = self._calculate_additional_metrics(topics, embeddings)\n",
        "\n",
        "        # Overall score - IMPROVED weighting\n",
        "        evaluation['overall_score'] = float(self._calculate_improved_overall_score(evaluation))\n",
        "\n",
        "        self.results = evaluation\n",
        "        self._print_evaluation_results()\n",
        "\n",
        "        return evaluation\n",
        "\n",
        "    def _calculate_robust_coherence(self, topic_model, documents, topics):\n",
        "        \"\"\"Robust coherence calculation with better error handling\"\"\"\n",
        "        try:\n",
        "            # Get topic words\n",
        "            topic_words = []\n",
        "            for topic in set(topics):\n",
        "                if topic != -1:\n",
        "                    words = topic_model.get_topic(topic)\n",
        "                    if words and len(words) >= 3:  # Only use topics with enough words\n",
        "                        topic_words.append([word for word, _ in words[:6]])\n",
        "\n",
        "            if len(topic_words) < 2:\n",
        "                return 0.0\n",
        "\n",
        "            # Tokenize documents\n",
        "            tokenized_docs = [doc.split() for doc in documents if doc and len(doc.split()) > 10]  # Longer docs only\n",
        "\n",
        "            if len(tokenized_docs) < 15:\n",
        "                return 0.0\n",
        "\n",
        "            # Calculate coherence with robust parameters\n",
        "            dictionary = corpora.Dictionary(tokenized_docs)\n",
        "            dictionary.filter_extremes(no_below=3, no_above=0.75)  # Stricter filtering\n",
        "            corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "            if len(corpus) < 10:\n",
        "                return 0.0\n",
        "\n",
        "            coherence_model = CoherenceModel(\n",
        "                topics=topic_words,\n",
        "                texts=tokenized_docs,\n",
        "                dictionary=dictionary,\n",
        "                coherence='c_v',\n",
        "                processes=1\n",
        "            )\n",
        "\n",
        "            coherence_score = coherence_model.get_coherence()\n",
        "            return max(0.0, float(coherence_score))  # Ensure non-negative\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Coherence calculation failed: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _analyze_topic_quality(self, topics):\n",
        "        \"\"\"Analyze topic quality metrics\"\"\"\n",
        "        topic_counts = Counter(topics)\n",
        "        if -1 in topic_counts:\n",
        "            valid_topics = {k: v for k, v in topic_counts.items() if k != -1}\n",
        "        else:\n",
        "            valid_topics = topic_counts\n",
        "\n",
        "        if not valid_topics:\n",
        "            return {\n",
        "                'balance_score': 0.0,\n",
        "                'size_variation': 0.0,\n",
        "                'avg_topic_size': 0.0,\n",
        "                'min_topic_size': 0.0,\n",
        "                'max_topic_size': 0.0,\n",
        "                'topic_diversity': 0.0\n",
        "            }\n",
        "\n",
        "        sizes = list(valid_topics.values())\n",
        "\n",
        "        # Calculate topic diversity (how evenly distributed topics are)\n",
        "        total_docs = sum(sizes)\n",
        "        if total_docs > 0:\n",
        "            proportions = [size / total_docs for size in sizes]\n",
        "            diversity = 1 - sum(p**2 for p in proportions)  # Gini-like diversity\n",
        "        else:\n",
        "            diversity = 0.0\n",
        "\n",
        "        return {\n",
        "            'balance_score': float(min(sizes) / max(sizes)) if max(sizes) > 0 else 0.0,\n",
        "            'size_variation': float(np.std(sizes) / np.mean(sizes)) if np.mean(sizes) > 0 else 0.0,\n",
        "            'avg_topic_size': float(np.mean(sizes)),\n",
        "            'min_topic_size': float(min(sizes)),\n",
        "            'max_topic_size': float(max(sizes)),\n",
        "            'topic_diversity': float(diversity)\n",
        "        }\n",
        "\n",
        "    def _calculate_additional_metrics(self, topics, embeddings):\n",
        "        \"\"\"Calculate additional evaluation metrics\"\"\"\n",
        "        try:\n",
        "            # Topic count score (penalize too few or too many topics)\n",
        "            unique_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
        "            if unique_topics <= 5:\n",
        "                topic_count_score = 0.2\n",
        "            elif unique_topics <= 10:\n",
        "                topic_count_score = 0.5\n",
        "            elif unique_topics <= 20:\n",
        "                topic_count_score = 0.8\n",
        "            elif unique_topics <= 30:\n",
        "                topic_count_score = 0.6\n",
        "            else:\n",
        "                topic_count_score = 0.3\n",
        "\n",
        "            return {\n",
        "                'topic_count_score': float(topic_count_score),\n",
        "                'unique_topics': int(unique_topics)\n",
        "            }\n",
        "        except:\n",
        "            return {\n",
        "                'topic_count_score': 0.0,\n",
        "                'unique_topics': 0\n",
        "            }\n",
        "\n",
        "    def _calculate_improved_overall_score(self, evaluation):\n",
        "        \"\"\"IMPROVED overall quality score with better weighting\"\"\"\n",
        "        basic = evaluation['basic_stats']\n",
        "        coherence = evaluation['coherence_score']\n",
        "        quality = evaluation['topic_quality']\n",
        "        additional = evaluation['additional_metrics']\n",
        "\n",
        "        # Improved weighted scoring\n",
        "        score = (\n",
        "            coherence * 0.35 +  # Coherence importance\n",
        "            (1 - basic['outlier_percentage'] / 100) * 0.25 +  # Low outliers good\n",
        "            quality['balance_score'] * 0.20 +  # Balanced topics good\n",
        "            quality['topic_diversity'] * 0.10 +  # Topic diversity\n",
        "            additional['topic_count_score'] * 0.10  # Appropriate topic count\n",
        "        )\n",
        "\n",
        "        return min(max(score, 0.0), 1.0)  # Ensure between 0 and 1\n",
        "\n",
        "    def _print_evaluation_results(self):\n",
        "        \"\"\"Print evaluation results\"\"\"\n",
        "        print(f\"\\n MODEL EVALUATION:\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        basic = self.results['basic_stats']\n",
        "        print(f\"   • Documents: {basic['total_documents']}\")\n",
        "        print(f\"   • Topics: {basic['topics_found']}\")\n",
        "        print(f\"   • Outliers: {basic['outliers']} ({basic['outlier_percentage']:.1f}%)\")\n",
        "\n",
        "        print(f\"   • Coherence Score: {self.results['coherence_score']:.3f}\")\n",
        "\n",
        "        quality = self.results['topic_quality']\n",
        "        print(f\"   • Balance Score: {quality['balance_score']:.3f}\")\n",
        "        print(f\"   • Topic Diversity: {quality['topic_diversity']:.3f}\")\n",
        "        print(f\"   • Avg Topic Size: {quality['avg_topic_size']:.1f}\")\n",
        "        print(f\"   • Size Range: {quality['min_topic_size']} - {quality['max_topic_size']}\")\n",
        "\n",
        "        additional = self.results['additional_metrics']\n",
        "        print(f\"   • Topic Count Score: {additional['topic_count_score']:.3f}\")\n",
        "\n",
        "        overall = self.results['overall_score']\n",
        "        assessment = \"EXCELLENT\" if overall > 0.7 else \"GOOD\" if overall > 0.5 else \"FAIR\" if overall > 0.3 else \"POOR\"\n",
        "        print(f\"   • Overall Score: {overall:.3f} - {assessment}\")\n",
        "\n",
        "\n",
        "\n",
        "class ResultsVisualizer:\n",
        "    def __init__(self, results_dir=\"domain_modeling_results\"):\n",
        "        self.results_dir = results_dir\n",
        "        self.setup_plot_style()\n",
        "\n",
        "    def setup_plot_style(self):\n",
        "        \"\"\"Setup professional plotting style\"\"\"\n",
        "        plt.style.use('default')\n",
        "        sns.set_palette(\"husl\")\n",
        "        self.colors = px.colors.qualitative.Set3\n",
        "\n",
        "    def load_results(self):\n",
        "        \"\"\"Load all saved results\"\"\"\n",
        "        print(\" Loading saved results for visualization...\")\n",
        "\n",
        "        try:\n",
        "            self.doc_assignments = pd.read_csv(f\"{self.results_dir}/document_assignments.csv\")\n",
        "            self.domain_mapping = pd.read_csv(f\"{self.results_dir}/domain_mapping.csv\")\n",
        "            self.topic_info = pd.read_csv(f\"{self.results_dir}/topic_info.csv\")\n",
        "\n",
        "            with open(f\"{self.results_dir}/evaluation.json\", 'r') as f:\n",
        "                self.evaluation = json.load(f)\n",
        "\n",
        "            print(\" All results loaded successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error loading results: {e}\")\n",
        "            return False\n",
        "\n",
        "    def create_comprehensive_visualizations(self):\n",
        "        \"\"\"Create all visualizations\"\"\"\n",
        "        if not self.load_results():\n",
        "            return\n",
        "\n",
        "        print(\"\\n CREATING COMPREHENSIVE VISUALIZATIONS...\")\n",
        "\n",
        "        # Create output directory\n",
        "        viz_dir = f\"{self.results_dir}/visualizations\"\n",
        "        os.makedirs(viz_dir, exist_ok=True)\n",
        "\n",
        "        # Create all visualizations\n",
        "        self.plot_domain_distribution(viz_dir)\n",
        "        self.plot_topic_size_distribution(viz_dir)\n",
        "        self.plot_confidence_analysis(viz_dir)\n",
        "        self.create_topic_wordclouds(viz_dir)\n",
        "        self.create_interactive_dashboard(viz_dir)\n",
        "        self.create_domain_analysis(viz_dir)\n",
        "\n",
        "        print(f\" All visualizations saved to: {viz_dir}\")\n",
        "\n",
        "    def plot_domain_distribution(self, output_dir):\n",
        "        \"\"\"Create domain distribution visualizations\"\"\"\n",
        "        print(\"    Creating domain distribution plots...\")\n",
        "\n",
        "        domain_counts = self.domain_mapping['primary_domain'].value_counts()\n",
        "\n",
        "        # Matplotlib version\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "        # Pie chart\n",
        "        colors = plt.cm.Set3(np.linspace(0, 1, len(domain_counts)))\n",
        "        wedges, texts, autotexts = ax1.pie(domain_counts.values, labels=domain_counts.index,\n",
        "                                          autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "        ax1.set_title('Domain Distribution - Topic Count', fontsize=14, fontweight='bold')\n",
        "\n",
        "        # Bar chart\n",
        "        y_pos = np.arange(len(domain_counts))\n",
        "        bars = ax2.barh(y_pos, domain_counts.values, color=colors)\n",
        "        ax2.set_yticks(y_pos)\n",
        "        ax2.set_yticklabels(domain_counts.index)\n",
        "        ax2.set_xlabel('Number of Topics')\n",
        "        ax2.set_title('Domain Distribution - Topic Count', fontsize=14, fontweight='bold')\n",
        "        ax2.bar_label(bars, padding=3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{output_dir}/domain_distribution.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_topic_size_distribution(self, output_dir):\n",
        "        \"\"\"Create topic size distribution visualizations\"\"\"\n",
        "        print(\"    Creating topic size distribution plots...\")\n",
        "\n",
        "        topic_sizes = self.topic_info[self.topic_info['Topic'] != -1]['Count']\n",
        "\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "        # Histogram\n",
        "        ax1.hist(topic_sizes, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        ax1.set_xlabel('Documents per Topic')\n",
        "        ax1.set_ylabel('Frequency')\n",
        "        ax1.set_title('Topic Size Distribution', fontweight='bold')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Box plot\n",
        "        ax2.boxplot(topic_sizes, vert=False)\n",
        "        ax2.set_xlabel('Number of Documents')\n",
        "        ax2.set_title('Topic Size Distribution (Box Plot)', fontweight='bold')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        # Top topics bar chart\n",
        "        top_topics = self.topic_info[self.topic_info['Topic'] != -1].nlargest(10, 'Count')\n",
        "        y_pos = np.arange(len(top_topics))\n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, len(top_topics)))\n",
        "\n",
        "        bars = ax3.barh(y_pos, top_topics['Count'], color=colors)\n",
        "        ax3.set_yticks(y_pos)\n",
        "        ax3.set_yticklabels([f\"Topic {t}\" for t in top_topics['Topic']])\n",
        "        ax3.set_xlabel('Number of Documents')\n",
        "        ax3.set_title('Top 10 Largest Topics', fontweight='bold')\n",
        "        ax3.bar_label(bars, padding=3)\n",
        "        ax3.invert_yaxis()\n",
        "\n",
        "        # Size vs Domain\n",
        "        domain_sizes = self.domain_mapping.merge(\n",
        "            self.topic_info[['Topic', 'Count']],\n",
        "            left_on='topic_id', right_on='Topic'\n",
        "        )\n",
        "        domain_avg_sizes = domain_sizes.groupby('primary_domain')['Count'].mean().sort_values()\n",
        "\n",
        "        y_pos = np.arange(len(domain_avg_sizes))\n",
        "        bars = ax4.barh(y_pos, domain_avg_sizes.values, color=plt.cm.Set3(np.linspace(0, 1, len(domain_avg_sizes))))\n",
        "        ax4.set_yticks(y_pos)\n",
        "        ax4.set_yticklabels(domain_avg_sizes.index)\n",
        "        ax4.set_xlabel('Average Documents per Topic')\n",
        "        ax4.set_title('Average Topic Size by Domain', fontweight='bold')\n",
        "        ax4.bar_label(bars, fmt='%.1f', padding=3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{output_dir}/topic_size_distribution.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_confidence_analysis(self, output_dir):\n",
        "        \"\"\"Create confidence analysis visualizations\"\"\"\n",
        "        print(\"    Creating confidence analysis plots...\")\n",
        "\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "        # Confidence distribution\n",
        "        ax1.hist(self.domain_mapping['confidence'], bins=15, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "        ax1.set_xlabel('Confidence Score')\n",
        "        ax1.set_ylabel('Frequency')\n",
        "        ax1.set_title('Distribution of Classification Confidence', fontweight='bold')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Confidence by domain\n",
        "        domain_conf = self.domain_mapping.groupby('primary_domain')['confidence'].mean().sort_values()\n",
        "        y_pos = np.arange(len(domain_conf))\n",
        "        colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(domain_conf)))\n",
        "\n",
        "        bars = ax2.barh(y_pos, domain_conf.values, color=colors)\n",
        "        ax2.set_yticks(y_pos)\n",
        "        ax2.set_yticklabels(domain_conf.index)\n",
        "        ax2.set_xlabel('Average Confidence Score')\n",
        "        ax2.set_title('Average Confidence by Domain', fontweight='bold')\n",
        "        ax2.set_xlim(0, 1)\n",
        "        ax2.bar_label(bars, fmt='%.3f', padding=3)\n",
        "\n",
        "        # Confidence categories\n",
        "        conf_categories = pd.cut(self.domain_mapping['confidence'],\n",
        "                               bins=[0, 0.5, 0.75, 1.0],\n",
        "                               labels=['Low (<0.5)', 'Medium (0.5-0.75)', 'High (>0.75)'])\n",
        "        conf_counts = conf_categories.value_counts()\n",
        "\n",
        "        ax3.pie(conf_counts.values, labels=conf_counts.index, autopct='%1.1f%%',\n",
        "               colors=['lightcoral', 'gold', 'lightgreen'])\n",
        "        ax3.set_title('Confidence Level Distribution', fontweight='bold')\n",
        "\n",
        "        # Domain-wise confidence distribution\n",
        "        domain_data = []\n",
        "        for domain in self.domain_mapping['primary_domain'].unique():\n",
        "            domain_conf = self.domain_mapping[self.domain_mapping['primary_domain'] == domain]['confidence']\n",
        "            domain_data.append(domain_conf.values)\n",
        "\n",
        "        ax4.boxplot(domain_data, labels=self.domain_mapping['primary_domain'].unique())\n",
        "        ax4.set_xticklabels(self.domain_mapping['primary_domain'].unique(), rotation=45)\n",
        "        ax4.set_ylabel('Confidence Score')\n",
        "        ax4.set_title('Confidence Distribution by Domain', fontweight='bold')\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{output_dir}/confidence_analysis.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def create_topic_wordclouds(self, output_dir):\n",
        "        \"\"\"Create word clouds for each domain\"\"\"\n",
        "        print(\"    Creating topic word clouds...\")\n",
        "\n",
        "        # Group topics by domain and create combined word lists\n",
        "        domain_words = {}\n",
        "\n",
        "        for _, row in self.domain_mapping.iterrows():\n",
        "            domain = row['primary_domain']\n",
        "            keywords = row['topic_keywords'].split(', ')\n",
        "\n",
        "            if domain not in domain_words:\n",
        "                domain_words[domain] = []\n",
        "\n",
        "            domain_words[domain].extend(keywords)\n",
        "\n",
        "        # Create word clouds for each domain\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for idx, (domain, words) in enumerate(list(domain_words.items())[:6]):\n",
        "            if idx < len(axes):\n",
        "                word_freq = Counter(words)\n",
        "                wordcloud = WordCloud(width=400, height=300,\n",
        "                                    background_color='white',\n",
        "                                    colormap='viridis').generate_from_frequencies(word_freq)\n",
        "\n",
        "                axes[idx].imshow(wordcloud, interpolation='bilinear')\n",
        "                axes[idx].set_title(f'{domain}\\n({len(words)} keywords)', fontweight='bold')\n",
        "                axes[idx].axis('off')\n",
        "\n",
        "        # Hide unused subplots\n",
        "        for idx in range(len(domain_words), len(axes)):\n",
        "            axes[idx].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{output_dir}/domain_wordclouds.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def create_interactive_dashboard(self, output_dir):\n",
        "        \"\"\"Create interactive Plotly dashboard\"\"\"\n",
        "        print(\"    Creating interactive dashboard...\")\n",
        "\n",
        "        # Prepare data\n",
        "        topic_data = self.topic_info[self.topic_info['Topic'] != -1].merge(\n",
        "            self.domain_mapping, left_on='Topic', right_on='topic_id'\n",
        "        )\n",
        "\n",
        "        # Create interactive scatter plot\n",
        "        fig = px.scatter(topic_data,\n",
        "                        x='Count',\n",
        "                        y='confidence',\n",
        "                        size='Count',\n",
        "                        color='primary_domain',\n",
        "                        hover_data=['Topic', 'topic_keywords'],\n",
        "                        title='Topic Analysis: Size vs Confidence by Domain',\n",
        "                        labels={'Count': 'Number of Documents', 'confidence': 'Classification Confidence'},\n",
        "                        size_max=60)\n",
        "\n",
        "        fig.update_layout(showlegend=True)\n",
        "        fig.write_html(f\"{output_dir}/interactive_topic_analysis.html\")\n",
        "\n",
        "    def create_domain_analysis(self, output_dir):\n",
        "        \"\"\"Create detailed domain-wise analysis\"\"\"\n",
        "        print(\"    Creating domain-wise analysis...\")\n",
        "\n",
        "        domain_data = self.domain_mapping.merge(\n",
        "            self.topic_info[['Topic', 'Count', 'Name']],\n",
        "            left_on='topic_id', right_on='Topic'\n",
        "        )\n",
        "\n",
        "        domains = domain_data['primary_domain'].unique()\n",
        "\n",
        "        for domain in domains:\n",
        "            domain_topics = domain_data[domain_data['primary_domain'] == domain]\n",
        "\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "            # Topic sizes for this domain\n",
        "            topics_sorted = domain_topics.nlargest(10, 'Count')\n",
        "            y_pos = np.arange(len(topics_sorted))\n",
        "\n",
        "            bars = ax1.barh(y_pos, topics_sorted['Count'],\n",
        "                           color=plt.cm.Blues(np.linspace(0.4, 0.8, len(topics_sorted))))\n",
        "            ax1.set_yticks(y_pos)\n",
        "            ax1.set_yticklabels([f\"Topic {t}\" for t in topics_sorted['Topic']])\n",
        "            ax1.set_xlabel('Number of Documents')\n",
        "            ax1.set_title(f'{domain} - Topic Sizes', fontweight='bold')\n",
        "            ax1.bar_label(bars, padding=3)\n",
        "            ax1.invert_yaxis()\n",
        "\n",
        "            # Confidence distribution for this domain\n",
        "            ax2.hist(domain_topics['confidence'], bins=10, alpha=0.7,\n",
        "                    color='lightgreen', edgecolor='black')\n",
        "            ax2.set_xlabel('Confidence Score')\n",
        "            ax2.set_ylabel('Number of Topics')\n",
        "            ax2.set_title(f'{domain} - Confidence Distribution', fontweight='bold')\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{output_dir}/domain_analysis_{domain.lower().replace(' ', '_')}.png\",\n",
        "                       dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "    def generate_comprehensive_report(self):\n",
        "        \"\"\"Generate comprehensive text report\"\"\"\n",
        "        print(\"\\n GENERATING COMPREHENSIVE REPORT...\")\n",
        "\n",
        "        report_lines = []\n",
        "\n",
        "        # Header\n",
        "        report_lines.append(\"=\"*80)\n",
        "        report_lines.append(\"SCIENTIFIC DOMAIN MODELING REPORT\")\n",
        "        report_lines.append(\"=\"*80)\n",
        "        report_lines.append(f\"Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        report_lines.append(f\"Total Documents: {len(self.doc_assignments)}\")\n",
        "        report_lines.append(f\"Total Topics: {len(self.domain_mapping)}\")\n",
        "        report_lines.append(\"\")\n",
        "\n",
        "        # Executive Summary\n",
        "        report_lines.append(\"EXECUTIVE SUMMARY\")\n",
        "        report_lines.append(\"-\"*40)\n",
        "        report_lines.append(f\"Overall Quality Score: {self.evaluation['overall_score']:.3f}\")\n",
        "        report_lines.append(f\"Topic Coherence: {self.evaluation['coherence_score']:.3f}\")\n",
        "        report_lines.append(f\"Outlier Percentage: {self.evaluation['basic_stats']['outlier_percentage']:.1f}%\")\n",
        "        report_lines.append(\"\")\n",
        "\n",
        "        # Domain Distribution\n",
        "        report_lines.append(\"DOMAIN DISTRIBUTION\")\n",
        "        report_lines.append(\"-\"*40)\n",
        "        domain_counts = self.domain_mapping['primary_domain'].value_counts()\n",
        "        for domain, count in domain_counts.items():\n",
        "            percentage = (count / len(self.domain_mapping)) * 100\n",
        "            report_lines.append(f\"{domain:25} {count:2d} topics ({percentage:5.1f}%)\")\n",
        "        report_lines.append(\"\")\n",
        "\n",
        "        # Save report\n",
        "        report_path = f\"{self.results_dir}/comprehensive_report.txt\"\n",
        "        with open(report_path, 'w') as f:\n",
        "            f.write('\\n'.join(report_lines))\n",
        "\n",
        "        print(f\" Comprehensive report saved to: {report_path}\")\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN PIPELINE\n",
        "# =============================================================================\n",
        "\n",
        "class DomainModelingPipeline:\n",
        "    def __init__(self):\n",
        "        self.data_processor = DataProcessor()\n",
        "        self.topic_modeler = TopicModeler()\n",
        "        self.domain_classifier = EnhancedDomainClassifier()\n",
        "        self.evaluator = ModelEvaluator()\n",
        "        self.visualizer = ResultsVisualizer()\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        \"\"\"Run complete domain modeling pipeline\"\"\"\n",
        "        print(\" STARTING DOMAIN MODELING PIPELINE\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        try:\n",
        "            # Step 1: Data processing\n",
        "            print(\"\\n STEP 1: DATA PROCESSING\")\n",
        "            df = self.data_processor.load_and_process_data()\n",
        "            documents = df['cleaned_text'].tolist()\n",
        "            print(f\" Processing {len(documents)} documents\")\n",
        "\n",
        "            # Step 2: Topic modeling\n",
        "            print(\"\\n STEP 2: TOPIC MODELING\")\n",
        "            topics, probabilities = self.topic_modeler.fit_model(documents)\n",
        "\n",
        "            # Step 3: Domain classification\n",
        "            print(\"\\n STEP 3: DOMAIN CLASSIFICATION\")\n",
        "            domain_mapping = self.domain_classifier.classify_topic_domains(\n",
        "                self.topic_modeler.topic_model, topics\n",
        "            )\n",
        "\n",
        "            # Step 4: Evaluation\n",
        "            print(\"\\n STEP 4: MODEL EVALUATION\")\n",
        "            evaluation = self.evaluator.evaluate_model(\n",
        "                self.topic_modeler.topic_model, documents, topics, self.topic_modeler.embeddings\n",
        "            )\n",
        "\n",
        "            # Step 5: Save results\n",
        "            print(\"\\n STEP 5: SAVING RESULTS\")\n",
        "            self._save_results(df, topics, domain_mapping, evaluation)\n",
        "\n",
        "            # Step 6: Create visualizations and reports\n",
        "            print(\"\\n STEP 6: CREATING VISUALIZATIONS & REPORTS\")\n",
        "            self.visualizer.create_comprehensive_visualizations()\n",
        "            self.visualizer.generate_comprehensive_report()\n",
        "\n",
        "            print(\"\\n PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "\n",
        "            # Final summary\n",
        "            self._print_final_summary(evaluation, domain_mapping)\n",
        "\n",
        "            return {\n",
        "                'topics': topics,\n",
        "                'domain_mapping': domain_mapping,\n",
        "                'evaluation': evaluation,\n",
        "                'model': self.topic_modeler.topic_model,\n",
        "                'documents': documents\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Pipeline execution failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def _print_final_summary(self, evaluation, domain_mapping):\n",
        "        \"\"\"Print final summary\"\"\"\n",
        "        print(f\"\\n FINAL SUMMARY:\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        topics_found = evaluation['basic_stats']['topics_found']\n",
        "        overall_score = evaluation['overall_score']\n",
        "\n",
        "        # Extract unique domains correctly\n",
        "        unique_domains = set(mapping['primary_domain'] for mapping in domain_mapping.values())\n",
        "        domains_found = len(unique_domains)\n",
        "\n",
        "        print(f\"   • Topics Discovered: {topics_found}\")\n",
        "        print(f\"   • Domains Identified: {domains_found}\")\n",
        "        print(f\"   • Overall Quality: {overall_score:.3f}\")\n",
        "\n",
        "        # Domain breakdown\n",
        "        domain_counts = Counter(mapping['primary_domain'] for mapping in domain_mapping.values())\n",
        "        print(f\"   • Domain Distribution:\")\n",
        "        for domain, count in domain_counts.most_common():\n",
        "            percentage = (count / topics_found) * 100\n",
        "            print(f\"      - {domain}: {count} topics ({percentage:.1f}%)\")\n",
        "\n",
        "    def _save_results(self, df, topics, domain_mapping, evaluation):\n",
        "        \"\"\"Save all results to files\"\"\"\n",
        "        try:\n",
        "            # Create document assignments dataframe\n",
        "            results_df = df.copy()\n",
        "            results_df['topic'] = topics\n",
        "            results_df['domain'] = results_df['topic'].map(\n",
        "                {k: v['primary_domain'] for k, v in domain_mapping.items()}\n",
        "            )\n",
        "            results_df['confidence'] = results_df['topic'].map(\n",
        "                {k: v['confidence'] for k, v in domain_mapping.items()}\n",
        "            )\n",
        "\n",
        "            # Save document assignments\n",
        "            results_df.to_csv(f\"{config.OUTPUT_DIR}/document_assignments.csv\", index=False)\n",
        "            print(f\" Document assignments saved\")\n",
        "\n",
        "            # Save topic model\n",
        "            self.topic_modeler.topic_model.save(f\"{config.OUTPUT_DIR}/topic_model\")\n",
        "            print(f\" Topic model saved\")\n",
        "\n",
        "            # Save domain mapping\n",
        "            domain_df = pd.DataFrame([\n",
        "                {\n",
        "                    'topic_id': topic_id,\n",
        "                    'primary_domain': info['primary_domain'],\n",
        "                    'confidence': info['confidence'],\n",
        "                    'topic_keywords': ', '.join(info['topic_keywords'])\n",
        "                }\n",
        "                for topic_id, info in domain_mapping.items()\n",
        "            ])\n",
        "            domain_df.to_csv(f\"{config.OUTPUT_DIR}/domain_mapping.csv\", index=False)\n",
        "            print(f\" Domain mapping saved\")\n",
        "\n",
        "            # Save evaluation with proper serialization\n",
        "            with open(f\"{config.OUTPUT_DIR}/evaluation.json\", 'w') as f:\n",
        "                json.dump(evaluation, f, indent=2, default=self._json_serializer)\n",
        "            print(f\" Evaluation results saved\")\n",
        "\n",
        "            # Save topic information\n",
        "            if hasattr(self.topic_modeler.topic_model, 'get_topic_info'):\n",
        "                topic_info = self.topic_modeler.topic_model.get_topic_info()\n",
        "                topic_info.to_csv(f\"{config.OUTPUT_DIR}/topic_info.csv\", index=False)\n",
        "                print(f\" Topic information saved\")\n",
        "\n",
        "            print(f\" All results saved to: {config.OUTPUT_DIR}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error saving results: {e}\")\n",
        "\n",
        "    def _json_serializer(self, obj):\n",
        "        \"\"\"JSON serializer for objects not serializable by default json code\"\"\"\n",
        "        if isinstance(obj, (np.integer, np.int32, np.int64)):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, (np.ndarray,)):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, (np.bool_)):\n",
        "            return bool(obj)\n",
        "        raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
        "\n",
        "# =============================================================================\n",
        "# EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configure logging\n",
        "    logging.getLogger(\"bertopic\").setLevel(logging.WARNING)\n",
        "    logging.getLogger(\"umap\").setLevel(logging.WARNING)\n",
        "    logging.getLogger(\"hdbscan\").setLevel(logging.WARNING)\n",
        "\n",
        "    print(\" OPTIMIZED DOMAIN MODELING PIPELINE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"   • Embedding Model: {config.EMBEDDING_MODEL}\")\n",
        "    print(f\"   • Output Directory: {config.OUTPUT_DIR}\")\n",
        "    print(f\"   • Features: Optimized Topic Modeling + Fixed Domain Classification + Visualization\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Execute pipeline\n",
        "    pipeline = DomainModelingPipeline()\n",
        "    results = pipeline.run_pipeline()\n",
        "\n",
        "    if results is not None:\n",
        "        print(f\"\\n PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(f\" Check '{config.OUTPUT_DIR}' folder for all results and visualizations\")\n",
        "    else:\n",
        "        print(\"\\n Pipeline execution failed\")"
      ],
      "metadata": {
        "id": "ZLUbOAm_D0SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime, timezone\n",
        "import torch\n",
        "from bertopic import BERTopic\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import silhouette_score\n",
        "import pickle\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud\n",
        "from gensim.models import LdaModel\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import time\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All imports completed successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "\n",
        "\n",
        "class EnhancedDomainConfig:\n",
        "    # Paths\n",
        "    PROCESSED_TEXT_CSV = \"updated.csv\"\n",
        "    OUTPUT_DIR = \"domain_modeling_results\"\n",
        "    COMPARISON_DIR = \"model_comparison\"\n",
        "\n",
        "    # Embedding models\n",
        "    EMBEDDING_MODELS = {\n",
        "        'primary': \"sentence-transformers/all-mpnet-base-v2\",\n",
        "        'backup': \"sentence-transformers/all-MiniLM-L12-v2\"\n",
        "    }\n",
        "\n",
        "    # Optimized UMAP parameters\n",
        "    UMAP_PARAMS = {\n",
        "        'n_neighbors': 10,\n",
        "        'n_components': 5,\n",
        "        'min_dist': 0.05,\n",
        "        'metric': 'cosine',\n",
        "        'random_state': 42,\n",
        "        'low_memory': False\n",
        "    }\n",
        "\n",
        "    # Optimized HDBSCAN parameters\n",
        "    HDBSCAN_PARAMS = {\n",
        "        'min_cluster_size': 10,\n",
        "        'min_samples': 3,\n",
        "        'cluster_selection_epsilon': 0.03,\n",
        "        'metric': 'euclidean',\n",
        "        'cluster_selection_method': 'eom',\n",
        "        'prediction_data': True\n",
        "    }\n",
        "\n",
        "    # BERTopic settings\n",
        "    BERTOPIC_SETTINGS = {\n",
        "        'top_n_words': 12,\n",
        "        'n_gram_range': (1, 2),\n",
        "        'min_topic_size': 10,\n",
        "        'calculate_probabilities': True,\n",
        "        'verbose': False,\n",
        "        'nr_topics': 20\n",
        "    }\n",
        "\n",
        "    # Enhanced LDA parameters\n",
        "    LDA_PARAMS = {\n",
        "        'n_components': 20,\n",
        "        'random_state': 42,\n",
        "        'learning_method': 'online',\n",
        "        'max_iter': 25,\n",
        "        'batch_size': 128,\n",
        "        'evaluate_every': 5\n",
        "    }\n",
        "\n",
        "    # Processing settings\n",
        "    MIN_DOC_LENGTH = 50\n",
        "    MAX_DOC_LENGTH = 2500\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    # Evaluation settings\n",
        "    TOPIC_EVALUATION_SAMPLE_SIZE = 3  # Number of docs per topic to sample\n",
        "\n",
        "    def __init__(self):\n",
        "        os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
        "        os.makedirs(self.COMPARISON_DIR, exist_ok=True)\n",
        "\n",
        "config = EnhancedDomainConfig()\n",
        "print(f\"Enhanced Configuration initialized\")\n",
        "\n",
        "\n",
        "class EnhancedDataProcessor:\n",
        "    def __init__(self):\n",
        "        self.scientific_stop_words = set([\n",
        "            'paper', 'study', 'research', 'result', 'method', 'approach',\n",
        "            'show', 'demonstrate', 'present', 'investigate', 'analyze',\n",
        "            'discuss', 'conclude', 'suggest', 'indicate', 'figure', 'table',\n",
        "            'author', 'journal', 'publication', 'reference', 'citation',\n",
        "            'section', 'abstract', 'introduction', 'background', 'conclusion',\n",
        "            'however', 'therefore', 'moreover', 'furthermore', 'additionally'\n",
        "        ])\n",
        "\n",
        "        self.technical_terms = {\n",
        "            'biology': ['gene', 'protein', 'cell', 'dna', 'rna', 'genome', 'organism'],\n",
        "            'medicine': ['patient', 'clinical', 'treatment', 'diagnosis', 'therapy', 'drug'],\n",
        "            'chemistry': ['molecule', 'compound', 'reaction', 'chemical', 'synthesis'],\n",
        "            'physics': ['quantum', 'particle', 'energy', 'field', 'mechanics'],\n",
        "            'computer_science': ['algorithm', 'neural', 'network', 'data', 'system'],\n",
        "            'engineering': ['design', 'system', 'manufacturing', 'structural'],\n",
        "            'materials_science': ['material', 'composite', 'polymer', 'ceramic'],\n",
        "            'psychology': ['behavior', 'cognitive', 'mental', 'brain', 'memory']\n",
        "        }\n",
        "\n",
        "    def enhanced_clean_text(self, text):\n",
        "        \"\"\"Enhanced text cleaning with technical term preservation\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        text = text.lower().strip()\n",
        "\n",
        "        if len(text) < config.MIN_DOC_LENGTH:\n",
        "            return \"\"\n",
        "        if len(text) > config.MAX_DOC_LENGTH:\n",
        "            text = text[:config.MAX_DOC_LENGTH]\n",
        "\n",
        "        # Remove technical patterns\n",
        "        patterns = [\n",
        "            (r'http\\S+|www\\S+|https\\S+', ' '),\n",
        "            (r'doi:\\s*\\S+', ' '),\n",
        "            (r'arXiv:\\s*\\d+\\.\\d+', ' '),\n",
        "            (r'©\\s*\\d{4}', ' '),\n",
        "            (r'fig\\.?\\s*\\d+', ' '),\n",
        "            (r'table\\s*\\d+', ' '),\n",
        "            (r'equation\\s*\\d+', ' '),\n",
        "            (r'\\s+', ' ')\n",
        "        ]\n",
        "\n",
        "        for pattern, replacement in patterns:\n",
        "            text = re.sub(pattern, replacement, text)\n",
        "\n",
        "        # Tokenize with preservation of technical terms\n",
        "        words = text.split()\n",
        "        cleaned_words = []\n",
        "\n",
        "        for word in words:\n",
        "            if self._should_preserve_word(word):\n",
        "                cleaned_words.append(word)\n",
        "\n",
        "        cleaned_text = ' '.join(cleaned_words).strip()\n",
        "\n",
        "        return cleaned_text if len(cleaned_text) >= config.MIN_DOC_LENGTH else \"\"\n",
        "\n",
        "    def _should_preserve_word(self, word):\n",
        "        \"\"\"Determine if word should be preserved\"\"\"\n",
        "        # Check if it's a technical term\n",
        "        for domain, terms in self.technical_terms.items():\n",
        "            if word in terms:\n",
        "                return True\n",
        "\n",
        "        # Basic filtering\n",
        "        if (len(word) <= 2 or len(word) >= 30 or\n",
        "            word in self.scientific_stop_words or\n",
        "            word.isdigit()):\n",
        "            return False\n",
        "\n",
        "        # Keep reasonable words\n",
        "        return len(word) > 3 and any(c.isalpha() for c in word)\n",
        "\n",
        "    def load_and_process_data(self):\n",
        "        \"\"\"Load and process data with enhanced cleaning\"\"\"\n",
        "        try:\n",
        "            print(\" Loading and processing data with enhanced cleaning...\")\n",
        "\n",
        "            df = pd.read_csv(config.PROCESSED_TEXT_CSV)\n",
        "            print(f\" Loaded {len(df)} documents\")\n",
        "\n",
        "            df['cleaned_text'] = df['processed_text'].apply(self.enhanced_clean_text)\n",
        "\n",
        "            initial_count = len(df)\n",
        "            df = df[df['cleaned_text'].str.len() > config.MIN_DOC_LENGTH].copy()\n",
        "            final_count = len(df)\n",
        "\n",
        "            if initial_count != final_count:\n",
        "                removed = initial_count - final_count\n",
        "                print(f\" Removed {removed} documents after cleaning ({removed/initial_count*100:.1f}%)\")\n",
        "\n",
        "            # Text statistics\n",
        "            word_counts = df['cleaned_text'].str.split().str.len()\n",
        "            print(f\" Final dataset: {final_count} documents\")\n",
        "            print(f\" Average document length: {word_counts.mean():.1f} words\")\n",
        "            print(f\" Total words: {word_counts.sum():,}\")\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Data loading failed: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "class FixedLDAModel:\n",
        "    \"\"\"Fixed LDA model with proper topic extraction\"\"\"\n",
        "    def __init__(self):\n",
        "        self.lda_model = None\n",
        "        self.vectorizer = None\n",
        "        self.dictionary = None\n",
        "        self.corpus = None\n",
        "        self.feature_names = None\n",
        "        self.perplexity_score = None\n",
        "        self.coherence_score = None\n",
        "\n",
        "    def fit(self, documents):\n",
        "        \"\"\"Train enhanced LDA model\"\"\"\n",
        "        print(\" Training Fixed LDA model...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Enhanced vectorizer\n",
        "        self.vectorizer = CountVectorizer(\n",
        "            max_df=0.9,\n",
        "            min_df=2,\n",
        "            stop_words='english',\n",
        "            ngram_range=(1, 2),\n",
        "            max_features=15000,\n",
        "            lowercase=True,\n",
        "            strip_accents='unicode'\n",
        "        )\n",
        "\n",
        "        X = self.vectorizer.fit_transform(documents)\n",
        "        self.feature_names = self.vectorizer.get_feature_names_out()\n",
        "        print(f\"    Vocabulary size: {X.shape[1]:,} features\")\n",
        "\n",
        "        # Train LDA with early stopping\n",
        "        self.lda_model = LatentDirichletAllocation(**config.LDA_PARAMS)\n",
        "        self.lda_model.fit(X)\n",
        "\n",
        "        # Calculate perplexity\n",
        "        self.perplexity_score = self.lda_model.perplexity(X)\n",
        "\n",
        "        # Prepare for coherence calculation\n",
        "        tokenized_docs = [doc.split() for doc in documents]\n",
        "        self.dictionary = corpora.Dictionary(tokenized_docs)\n",
        "        self.dictionary.filter_extremes(no_below=3, no_above=0.85)\n",
        "        self.corpus = [self.dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "        # Train gensim LDA for coherence\n",
        "        self.gensim_lda = LdaModel(\n",
        "            corpus=self.corpus,\n",
        "            id2word=self.dictionary,\n",
        "            num_topics=config.LDA_PARAMS['n_components'],\n",
        "            random_state=42,\n",
        "            update_every=1,\n",
        "            chunksize=100,\n",
        "            passes=10,\n",
        "            alpha='auto',\n",
        "            per_word_topics=True\n",
        "        )\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\" LDA training completed in {end_time - start_time:.2f}s\")\n",
        "\n",
        "    def get_topic_words(self, topic_id, n_words=10):\n",
        "        \"\"\"Extract actual topic words from LDA\"\"\"\n",
        "        if self.lda_model is None:\n",
        "            return []\n",
        "\n",
        "        # Get topic weights\n",
        "        topic_weights = self.lda_model.components_[topic_id]\n",
        "        # Get indices of top words\n",
        "        top_indices = topic_weights.argsort()[:-n_words-1:-1]\n",
        "        # Map indices to actual words\n",
        "        topic_words = [self.feature_names[i] for i in top_indices]\n",
        "\n",
        "        return topic_words\n",
        "\n",
        "    def get_topics_dict(self, n_words=10):\n",
        "        \"\"\"Get all topics as dictionary\"\"\"\n",
        "        topics = {}\n",
        "        for i in range(config.LDA_PARAMS['n_components']):\n",
        "            topics[i] = self.get_topic_words(i, n_words)\n",
        "        return topics\n",
        "\n",
        "    def predict_topics(self, documents):\n",
        "        \"\"\"Predict topic distributions\"\"\"\n",
        "        X = self.vectorizer.transform(documents)\n",
        "        topic_distributions = self.lda_model.transform(X)\n",
        "        return topic_distributions.argmax(axis=1)\n",
        "\n",
        "    def evaluate(self, documents):\n",
        "        \"\"\"Comprehensive LDA evaluation\"\"\"\n",
        "        print(\" Evaluating Fixed LDA model...\")\n",
        "\n",
        "        # Coherence score\n",
        "        tokenized_docs = [doc.split() for doc in documents]\n",
        "\n",
        "        coherence_model = CoherenceModel(\n",
        "            model=self.gensim_lda,\n",
        "            texts=tokenized_docs,\n",
        "            dictionary=self.dictionary,\n",
        "            coherence='c_v'\n",
        "        )\n",
        "        coherence = coherence_model.get_coherence()\n",
        "        self.coherence_score = coherence\n",
        "\n",
        "        # Topic quality metrics\n",
        "        topics = self.get_topics_dict(n_words=10)\n",
        "        all_words = []\n",
        "        for topic_words in topics.values():\n",
        "            all_words.extend(topic_words[:5])\n",
        "\n",
        "        unique_words = len(set(all_words))\n",
        "        total_words = len(all_words)\n",
        "        diversity = unique_words / total_words if total_words > 0 else 0\n",
        "\n",
        "        # Topic distinctiveness\n",
        "        topic_matrices = self.lda_model.components_\n",
        "        distinctiveness = self._calculate_topic_distinctiveness(topic_matrices)\n",
        "\n",
        "        evaluation = {\n",
        "            'coherence_score': float(coherence),\n",
        "            'perplexity_score': float(self.perplexity_score),\n",
        "            'topic_diversity': float(diversity),\n",
        "            'topic_distinctiveness': float(distinctiveness),\n",
        "            'num_topics': config.LDA_PARAMS['n_components'],\n",
        "            'model_type': 'LDA_FIXED'\n",
        "        }\n",
        "\n",
        "        print(f\"   • Coherence: {coherence:.3f}\")\n",
        "        print(f\"   • Perplexity: {self.perplexity_score:.2f}\")\n",
        "        print(f\"   • Topic Diversity: {diversity:.3f}\")\n",
        "        print(f\"   • Topic Distinctiveness: {distinctiveness:.3f}\")\n",
        "\n",
        "        return evaluation\n",
        "\n",
        "    def _calculate_topic_distinctiveness(self, topic_matrices):\n",
        "        \"\"\"Calculate how distinct topics are from each other\"\"\"\n",
        "        n_topics = topic_matrices.shape[0]\n",
        "        similarities = []\n",
        "\n",
        "        for i in range(n_topics):\n",
        "            for j in range(i+1, n_topics):\n",
        "                # Cosine similarity between topics\n",
        "                cos_sim = np.dot(topic_matrices[i], topic_matrices[j]) / (\n",
        "                    np.linalg.norm(topic_matrices[i]) * np.linalg.norm(topic_matrices[j])\n",
        "                )\n",
        "                similarities.append(cos_sim)\n",
        "\n",
        "        if similarities:\n",
        "            avg_similarity = np.mean(similarities)\n",
        "            return 1 - avg_similarity  # Higher is better (more distinct)\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "class EnhancedTopicModeler:\n",
        "    \"\"\"Enhanced BERTopic modeler\"\"\"\n",
        "    def __init__(self):\n",
        "        self.topic_model = None\n",
        "        self.embeddings = None\n",
        "        self.topics = None\n",
        "        self.probabilities = None\n",
        "\n",
        "    def initialize_enhanced_model(self):\n",
        "        \"\"\"Initialize enhanced BERTopic model\"\"\"\n",
        "        print(\" Initializing enhanced BERTopic model...\")\n",
        "\n",
        "        try:\n",
        "            umap_model = UMAP(**config.UMAP_PARAMS)\n",
        "            hdbscan_model = HDBSCAN(**config.HDBSCAN_PARAMS)\n",
        "\n",
        "            vectorizer_model = CountVectorizer(\n",
        "                stop_words=\"english\",\n",
        "                ngram_range=config.BERTOPIC_SETTINGS['n_gram_range'],\n",
        "                min_df=2,\n",
        "                max_df=0.9,\n",
        "                max_features=15000,\n",
        "                lowercase=True,\n",
        "                strip_accents='unicode'\n",
        "            )\n",
        "\n",
        "            self.topic_model = BERTopic(\n",
        "                umap_model=umap_model,\n",
        "                hdbscan_model=hdbscan_model,\n",
        "                vectorizer_model=vectorizer_model,\n",
        "                top_n_words=config.BERTOPIC_SETTINGS['top_n_words'],\n",
        "                min_topic_size=config.BERTOPIC_SETTINGS['min_topic_size'],\n",
        "                calculate_probabilities=config.BERTOPIC_SETTINGS['calculate_probabilities'],\n",
        "                verbose=config.BERTOPIC_SETTINGS['verbose'],\n",
        "                nr_topics=config.BERTOPIC_SETTINGS['nr_topics']\n",
        "            )\n",
        "\n",
        "            print(\" Enhanced BERTopic model initialized successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Enhanced initialization failed: {e}\")\n",
        "            self._initialize_fallback_model()\n",
        "\n",
        "    def _initialize_fallback_model(self):\n",
        "        \"\"\"Initialize fallback model\"\"\"\n",
        "        print(\" Initializing fallback model...\")\n",
        "        self.topic_model = BERTopic(\n",
        "            min_topic_size=15,\n",
        "            verbose=False,\n",
        "            calculate_probabilities=True\n",
        "        )\n",
        "\n",
        "    def fit_enhanced_model(self, documents):\n",
        "        \"\"\"Fit enhanced topic model\"\"\"\n",
        "        print(\" Fitting enhanced topic model...\")\n",
        "\n",
        "        # Generate embeddings with primary model\n",
        "        try:\n",
        "            model = SentenceTransformer(config.EMBEDDING_MODELS['primary'])\n",
        "            self.embeddings = model.encode(\n",
        "                documents,\n",
        "                batch_size=config.BATCH_SIZE,\n",
        "                show_progress_bar=True,\n",
        "                normalize_embeddings=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\" Primary embedding failed, using backup: {e}\")\n",
        "            model = SentenceTransformer(config.EMBEDDING_MODELS['backup'])\n",
        "            self.embeddings = model.encode(\n",
        "                documents,\n",
        "                batch_size=config.BATCH_SIZE,\n",
        "                show_progress_bar=True,\n",
        "                normalize_embeddings=True\n",
        "            )\n",
        "\n",
        "        self.initialize_enhanced_model()\n",
        "\n",
        "        try:\n",
        "            self.topics, self.probabilities = self.topic_model.fit_transform(\n",
        "                documents, self.embeddings\n",
        "            )\n",
        "            print(\" Enhanced model fitting completed successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\" Enhanced fitting failed: {e}\")\n",
        "            self._fit_fallback_model(documents)\n",
        "\n",
        "        self._optimize_topics(documents)\n",
        "        self._print_enhanced_results()\n",
        "\n",
        "        return self.topics, self.probabilities\n",
        "\n",
        "    def _fit_fallback_model(self, documents):\n",
        "        \"\"\"Fit fallback model\"\"\"\n",
        "        print(\" Fitting fallback model...\")\n",
        "        self.topic_model = BERTopic(\n",
        "            min_topic_size=20,\n",
        "            verbose=False,\n",
        "            calculate_probabilities=False\n",
        "        )\n",
        "        self.topics, self.probabilities = self.topic_model.fit_transform(documents)\n",
        "\n",
        "    def _optimize_topics(self, documents):\n",
        "        \"\"\"Optimize topics based on quality metrics\"\"\"\n",
        "        unique_topics = len(set(self.topics)) - (1 if -1 in self.topics else 0)\n",
        "        print(f\" Found {unique_topics} initial topics\")\n",
        "\n",
        "        if unique_topics < 6 and hasattr(self.topic_model, 'reduce_topics'):\n",
        "            try:\n",
        "                target_topics = max(8, min(25, len(documents) // 30))\n",
        "                print(f\" Optimizing to {target_topics} topics...\")\n",
        "                self.topics, self.probabilities = self.topic_model.reduce_topics(\n",
        "                    documents, self.topics, self.probabilities, nr_topics=target_topics\n",
        "                )\n",
        "                print(f\" Optimized to {len(set(self.topics)) - (1 if -1 in self.topics else 0)} topics\")\n",
        "            except Exception as e:\n",
        "                print(f\" Topic optimization failed: {e}\")\n",
        "\n",
        "    def _print_enhanced_results(self):\n",
        "        \"\"\"Print enhanced topic modeling results\"\"\"\n",
        "        unique_topics = len(set(self.topics)) - (1 if -1 in self.topics else 0)\n",
        "        outliers = np.sum(self.topics == -1)\n",
        "        outlier_percentage = (outliers / len(self.topics)) * 100\n",
        "\n",
        "        topic_counts = Counter(self.topics)\n",
        "        valid_topics = {k: v for k, v in topic_counts.items() if k != -1}\n",
        "\n",
        "        if valid_topics:\n",
        "            sizes = list(valid_topics.values())\n",
        "            stats = {\n",
        "                'min_size': min(sizes),\n",
        "                'max_size': max(sizes),\n",
        "                'avg_size': np.mean(sizes),\n",
        "                'std_size': np.std(sizes),\n",
        "                'median_size': np.median(sizes)\n",
        "            }\n",
        "        else:\n",
        "            stats = {'min_size': 0, 'max_size': 0, 'avg_size': 0, 'std_size': 0, 'median_size': 0}\n",
        "\n",
        "        print(f\"\\n ENHANCED TOPIC MODELING RESULTS:\")\n",
        "        print(f\"   • Topics discovered: {unique_topics}\")\n",
        "        print(f\"   • Outliers: {outliers} ({outlier_percentage:.1f}%)\")\n",
        "        print(f\"   • Topic size range: {stats['min_size']} - {stats['max_size']}\")\n",
        "        print(f\"   • Average topic size: {stats['avg_size']:.1f}\")\n",
        "        print(f\"   • Median topic size: {stats['median_size']:.1f}\")\n",
        "\n",
        "        self._assess_topic_quality(valid_topics)\n",
        "        self._display_sample_topics()\n",
        "\n",
        "    def _assess_topic_quality(self, valid_topics):\n",
        "        \"\"\"Assess overall topic quality\"\"\"\n",
        "        if not valid_topics:\n",
        "            return\n",
        "\n",
        "        sizes = list(valid_topics.values())\n",
        "        balance_ratio = min(sizes) / max(sizes) if max(sizes) > 0 else 0\n",
        "        cv = np.std(sizes) / np.mean(sizes) if np.mean(sizes) > 0 else 0\n",
        "\n",
        "        print(f\"   • Topic balance ratio: {balance_ratio:.3f}\")\n",
        "        print(f\"   • Size coefficient of variation: {cv:.3f}\")\n",
        "\n",
        "        if balance_ratio > 0.15 and cv < 1.0:\n",
        "            print(\"   •  Excellent topic distribution\")\n",
        "        elif balance_ratio > 0.05:\n",
        "            print(\"   •  Good topic distribution\")\n",
        "        else:\n",
        "            print(\"   •  Poor topic distribution - consider parameter adjustment\")\n",
        "\n",
        "    def _display_sample_topics(self):\n",
        "        \"\"\"Display sample topics for quality inspection\"\"\"\n",
        "        if hasattr(self.topic_model, 'get_topic_info'):\n",
        "            try:\n",
        "                topic_info = self.topic_model.get_topic_info()\n",
        "                valid_topics = topic_info[topic_info['Topic'] != -1]\n",
        "\n",
        "                print(f\"\\n TOPIC OVERVIEW (showing {min(8, len(valid_topics))} sample topics):\")\n",
        "                print(\"=\" * 80)\n",
        "\n",
        "                for _, row in valid_topics.head(8).iterrows():\n",
        "                    topic_id = row['Topic']\n",
        "                    topic_words = self.topic_model.get_topic(topic_id)\n",
        "                    if topic_words:\n",
        "                        words = [word for word, _ in topic_words[:6]]\n",
        "                        size_percentage = (row['Count'] / len(self.topics)) * 100\n",
        "                        print(f\"   Topic {topic_id:2d} ({row['Count']:3d} docs, {size_percentage:4.1f}%): {', '.join(words)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   • Could not extract topic details: {e}\")\n",
        "\n",
        "\n",
        "class EnhancedDomainClassifier:\n",
        "    \"\"\"Enhanced domain classifier with improved keyword matching\"\"\"\n",
        "    def __init__(self):\n",
        "        # Comprehensive domain keywords with FIXED coverage\n",
        "        self.domain_keywords = {\n",
        "            'biology': [\n",
        "                'cell', 'gene', 'protein', 'dna', 'genetic', 'molecular', 'organism',\n",
        "                'evolution', 'genome', 'species', 'ecological', 'biodiversity',\n",
        "                'microbial', 'enzyme', 'metabolism', 'phylogenetic', 'transcription',\n",
        "                'rna', 'chromosome', 'mitochondria', 'apoptosis', 'sequence',\n",
        "                'mutation', 'expression', 'cellular', 'developmental', 'plant',\n",
        "                'animal', 'bacterial', 'viral', 'evolutionary', 'population',\n",
        "                'biological', 'physiology', 'genomic', 'proteomic', 'transcriptomic',\n",
        "                'microbiome', 'neuroscience', 'zoology', 'botany', 'ecology',\n",
        "                'soybean', 'fruit', 'circrnas', 'drosophila', 'nutrient', 'invasive',\n",
        "                'breast', 'strain', 'speech', 'brain', 'neural', 'jet', 'physics',\n",
        "                'collision', 'proton', 'oxygen', 'sex', 'temperature', 'thermal'\n",
        "            ],\n",
        "            'medicine': [\n",
        "                'patient', 'clinical', 'treatment', 'disease', 'medical', 'therapy',\n",
        "                'health', 'drug', 'vaccine', 'diagnosis', 'symptom', 'hospital',\n",
        "                'pharmaceutical', 'epidemiology', 'pathology', 'oncology', 'immunology',\n",
        "                'surgery', 'prognosis', 'biomarker', 'clinical trial', 'pharmacology',\n",
        "                'therapeutic', 'dosage', 'recovery', 'mortality', 'morbidity', 'cancer',\n",
        "                'tumor', 'infection', 'inflammatory', 'neurological', 'cardiology',\n",
        "                'pediatric', 'geriatric', 'psychiatry', 'radiology', 'anesthesia',\n",
        "                'public health', 'virology', 'bacteriology', 'dose', 'imaging',\n",
        "                'muscle', 'care', 'inflammation', 'mental', 'virus', 'sars', 'vector',\n",
        "                'mouse', 'dam', 'like', 'wash', 'londrina'\n",
        "            ],\n",
        "            'chemistry': [\n",
        "                'molecule', 'reaction', 'compound', 'chemical', 'synthesis',\n",
        "                'catalyst', 'polymer', 'organic', 'inorganic', 'spectroscopy',\n",
        "                'chromatography', 'crystallography', 'stoichiometry', 'kinetics',\n",
        "                'nmr', 'mass spectrometry', 'electrochemistry', 'photochemistry',\n",
        "                'reagent', 'solvent', 'yield', 'purification', 'characterization',\n",
        "                'crystal', 'bond', 'structure', 'atomic', 'molecular', 'analytical',\n",
        "                'physical chemistry', 'quantum chemistry', 'medicinal chemistry',\n",
        "                'biochemistry', 'polymer chemistry', 'materials chemistry',\n",
        "                'irrigation', 'management', 'soil', 'water'\n",
        "            ],\n",
        "            'environmental_science': [\n",
        "                'environmental', 'climate', 'ecosystem', 'sustainability', 'pollution',\n",
        "                'conservation', 'biodiversity', 'ecological', 'environmental impact',\n",
        "                'climate change', 'environmental management', 'sustainable development',\n",
        "                'habitat', 'wildlife', 'conservation', 'environmental policy', 'earth',\n",
        "                'geological', 'ocean', 'atmospheric', 'marine', 'terrestrial',\n",
        "                'agricultural', 'forestry', 'water resources', 'air quality', 'soil science',\n",
        "                'environmental engineering', 'conservation biology', 'environmental health',\n",
        "                'ice', 'university', 'usa', 'fish', 'population', 'ecology',\n",
        "                'geological survey', 'santa', 'survey', 'rupture'\n",
        "            ],\n",
        "            'computer_science': [\n",
        "                'algorithm', 'software', 'programming', 'machine learning',\n",
        "                'neural network', 'data', 'system', 'computational', 'artificial intelligence',\n",
        "                'database', 'network', 'optimization', 'cybersecurity', 'blockchain',\n",
        "                'deep learning', 'computer vision', 'natural language processing',\n",
        "                'computation', 'modeling', 'simulation', 'data analysis', 'big data',\n",
        "                'cloud computing', 'internet of things', 'robotics', 'automation'\n",
        "            ],\n",
        "            'physics': [\n",
        "                'quantum', 'particle', 'energy', 'field', 'mechanics', 'astrophysics',\n",
        "                'relativity', 'nuclear', 'optics', 'thermodynamics', 'electromagnetic',\n",
        "                'condensed matter', 'cosmology', 'entanglement', 'superconductivity',\n",
        "                'atomic', 'molecular', 'theoretical', 'experimental', 'quantum mechanics',\n",
        "                'statistical mechanics', 'fluid dynamics', 'plasma physics', 'optics',\n",
        "                'astronomy', 'cosmology', 'particle physics', 'solid state physics'\n",
        "            ],\n",
        "            'engineering': [\n",
        "                'design', 'system', 'manufacturing', 'structural', 'electrical',\n",
        "                'mechanical', 'control', 'sensor', 'robotics', 'automation',\n",
        "                'aerospace', 'civil', 'materials', 'nanotechnology', 'mechatronics',\n",
        "                'biomedical', 'chemical engineering', 'environmental engineering'\n",
        "            ],\n",
        "            'materials_science': [\n",
        "                'material', 'composite', 'polymer', 'ceramic', 'metal', 'alloy',\n",
        "                'nanomaterial', 'crystal', 'structure', 'properties', 'synthesis',\n",
        "                'fabrication', 'characterization', 'mechanical properties', 'thermal properties',\n",
        "                'electronic properties', 'optical properties', 'material design',\n",
        "                'heat', 'strength', 'stimulus', 'group'\n",
        "            ],\n",
        "            'psychology': [\n",
        "                'behavior', 'cognitive', 'psychological', 'mental', 'personality',\n",
        "                'emotion', 'memory', 'neural', 'brain', 'perception', 'learning',\n",
        "                'cognition', 'developmental', 'social psychology', 'clinical psychology',\n",
        "                'behavioral', 'psychiatric', 'neuroscience', 'cognitive science',\n",
        "                'speech', 'elife', 'neuroscience', 'university'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Domain relationships for ambiguous cases\n",
        "        self.domain_relationships = {\n",
        "            'ecology': ['biology', 'environmental_science'],\n",
        "            'neuroscience': ['biology', 'psychology', 'medicine'],\n",
        "            'biochemistry': ['biology', 'chemistry'],\n",
        "            'bioinformatics': ['biology', 'computer_science'],\n",
        "            'materials_science': ['chemistry', 'engineering', 'physics'],\n",
        "            'environmental_health': ['environmental_science', 'medicine']\n",
        "        }\n",
        "\n",
        "        # Strong domain indicators\n",
        "        self.strong_indicators = {\n",
        "            'biology': ['gene', 'cell', 'dna', 'protein', 'genome', 'evolution'],\n",
        "            'medicine': ['patient', 'clinical', 'treatment', 'diagnosis', 'therapy', 'hospital'],\n",
        "            'chemistry': ['molecule', 'reaction', 'compound', 'synthesis', 'catalyst'],\n",
        "            'environmental_science': ['climate', 'ecosystem', 'pollution', 'conservation', 'habitat'],\n",
        "            'psychology': ['behavior', 'cognitive', 'mental', 'brain', 'memory'],\n",
        "            'physics': ['quantum', 'particle', 'energy', 'mechanics', 'relativity'],\n",
        "            'materials_science': ['material', 'composite', 'ceramic', 'polymer', 'alloy']\n",
        "        }\n",
        "\n",
        "    def classify_topic_domains(self, topic_model, topics, model_type=\"bertopic\",\n",
        "                              topic_samples=None):\n",
        "        \"\"\"Enhanced domain classification with sample document context\"\"\"\n",
        "        print(f\" Performing enhanced domain classification for {model_type}...\")\n",
        "\n",
        "        topic_domain_mapping = {}\n",
        "        unique_topics = set(topics) - {-1} if model_type == \"bertopic\" else set(topics)\n",
        "\n",
        "        with tqdm(total=len(unique_topics), desc=f\"Classifying {model_type} topics\") as pbar:\n",
        "            for topic_id in unique_topics:\n",
        "                try:\n",
        "                    # Get topic words based on model type\n",
        "                    if model_type == \"bertopic\":\n",
        "                        topic_words_data = topic_model.get_topic(topic_id)\n",
        "                        if not topic_words_data:\n",
        "                            pbar.update(1)\n",
        "                            continue\n",
        "                        all_topic_words = [word for word, _ in topic_words_data[:12]]\n",
        "                    else:  # LDA - use fixed method\n",
        "                        if hasattr(topic_model, 'get_topic_words'):\n",
        "                            all_topic_words = topic_model.get_topic_words(topic_id)\n",
        "                        else:\n",
        "                            pbar.update(1)\n",
        "                            continue\n",
        "\n",
        "                    if not all_topic_words:\n",
        "                        pbar.update(1)\n",
        "                        continue\n",
        "\n",
        "                    # Create topic text with optional sample context\n",
        "                    topic_text = ' '.join(all_topic_words).lower()\n",
        "                    if topic_samples and topic_id in topic_samples:\n",
        "                        sample_text = ' '.join(topic_samples[topic_id])\n",
        "                        topic_text += ' ' + sample_text[:500].lower()  # Add sample context\n",
        "\n",
        "                    # Calculate enhanced domain scores\n",
        "                    domain_scores = self._calculate_enhanced_domain_scores(all_topic_words, topic_text)\n",
        "\n",
        "                    if not domain_scores:\n",
        "                        pbar.update(1)\n",
        "                        continue\n",
        "\n",
        "                    # Resolve ambiguous domains with context\n",
        "                    best_domain = self._resolve_enhanced_domains(domain_scores, all_topic_words, topic_text)\n",
        "\n",
        "                    if best_domain == 'unknown':\n",
        "                        pbar.update(1)\n",
        "                        continue\n",
        "\n",
        "                    # Calculate confidence with context\n",
        "                    confidence = self._calculate_confidence_with_context(domain_scores, best_domain, topic_text)\n",
        "\n",
        "                    # Store result\n",
        "                    topic_domain_mapping[int(topic_id)] = {\n",
        "                        'primary_domain': best_domain,\n",
        "                        'confidence': float(confidence),\n",
        "                        'all_domain_scores': {k: float(v) for k, v in domain_scores.items()},\n",
        "                        'topic_keywords': all_topic_words[:8],\n",
        "                        'model_type': model_type,\n",
        "                        'classification_method': 'enhanced_keyword'\n",
        "                    }\n",
        "\n",
        "                    # Display classification\n",
        "                    confidence_level = \"HIGH\" if confidence > 0.7 else \"MEDIUM\" if confidence > 0.4 else \"LOW\"\n",
        "                    top_keywords = ', '.join(all_topic_words[:4])\n",
        "                    print(f\"   {model_type.upper()} Topic {topic_id:2d} → {best_domain:20s} \"\n",
        "                          f\"(conf: {confidence:.3f} [{confidence_level}]) - {top_keywords}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    Error classifying topic {topic_id}: {str(e)}\")\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "        self._print_enhanced_domain_analysis(topic_domain_mapping, model_type)\n",
        "        return topic_domain_mapping\n",
        "\n",
        "    def _calculate_enhanced_domain_scores(self, topic_words, topic_text):\n",
        "        \"\"\"Calculate enhanced domain scores with context\"\"\"\n",
        "        domain_scores = {}\n",
        "\n",
        "        for domain, keywords in self.domain_keywords.items():\n",
        "            score = 0.1  # Base score\n",
        "            matches = []\n",
        "\n",
        "            # Position-based scoring\n",
        "            for i, word in enumerate(topic_words[:10]):\n",
        "                if word in keywords:\n",
        "                    position_weight = max(0, (10 - i) / 10) * 2.0\n",
        "                    score += 1.0 + position_weight\n",
        "                    matches.append((word, i))\n",
        "\n",
        "            # Contextual presence scoring\n",
        "            for keyword in keywords:\n",
        "                if keyword in topic_text and keyword not in [m[0] for m in matches]:\n",
        "                    if ' ' in keyword:  # Multi-word terms\n",
        "                        if keyword in topic_text:\n",
        "                            score += 0.8\n",
        "                    else:  # Single words\n",
        "                        score += 0.3\n",
        "\n",
        "            # Multi-match bonus\n",
        "            if len(matches) >= 3:\n",
        "                score *= 1.3\n",
        "            elif len(matches) >= 2:\n",
        "                score *= 1.15\n",
        "\n",
        "            # Strong domain indicator bonus\n",
        "            strong_matches = [word for word in topic_words[:4] if word in self.strong_indicators.get(domain, [])]\n",
        "            if strong_matches:\n",
        "                score *= 1.2\n",
        "\n",
        "            # Context density bonus (more keywords in context = higher score)\n",
        "            context_keywords = sum(1 for keyword in keywords if keyword in topic_text)\n",
        "            if context_keywords > 5:\n",
        "                score *= 1.1\n",
        "\n",
        "            domain_scores[domain] = score\n",
        "\n",
        "        return domain_scores\n",
        "\n",
        "    def _resolve_enhanced_domains(self, domain_scores, topic_words, topic_text):\n",
        "        \"\"\"Resolve ambiguous domains with enhanced logic\"\"\"\n",
        "        domain_scores = {k: v for k, v in domain_scores.items() if v > 0}\n",
        "\n",
        "        if not domain_scores:\n",
        "            return 'unknown'\n",
        "\n",
        "        sorted_domains = sorted(domain_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        if len(sorted_domains) < 2:\n",
        "            return sorted_domains[0][0]\n",
        "\n",
        "        best_domain, best_score = sorted_domains[0]\n",
        "        second_domain, second_score = sorted_domains[1]\n",
        "\n",
        "        # Clear winner (score difference > 50%)\n",
        "        if best_score > second_score * 1.5:\n",
        "            return best_domain\n",
        "\n",
        "        # Check for domain relationships\n",
        "        for relationship, related_domains in self.domain_relationships.items():\n",
        "            if best_domain in related_domains and second_domain in related_domains:\n",
        "                # Choose based on stronger indicators and context\n",
        "                best_indicators = sum(1 for word in topic_words[:4]\n",
        "                                    if word in self.strong_indicators.get(best_domain, []))\n",
        "                second_indicators = sum(1 for word in topic_words[:4]\n",
        "                                      if word in self.strong_indicators.get(second_domain, []))\n",
        "\n",
        "                if best_indicators > second_indicators * 1.5:\n",
        "                    return best_domain\n",
        "                elif second_indicators > best_indicators * 1.5:\n",
        "                    return second_domain\n",
        "\n",
        "        # Check context for additional clues\n",
        "        context_winner = self._check_context_for_domain(topic_text)\n",
        "        if context_winner and context_winner in [best_domain, second_domain]:\n",
        "            return context_winner\n",
        "\n",
        "        # Default to highest score\n",
        "        return best_domain\n",
        "\n",
        "    def _check_context_for_domain(self, topic_text):\n",
        "        \"\"\"Check context for domain clues\"\"\"\n",
        "        domain_context_indicators = {\n",
        "            'biology': ['cell', 'organism', 'species', 'evolutionary'],\n",
        "            'medicine': ['clinical', 'patient', 'treatment', 'diagnosis'],\n",
        "            'chemistry': ['compound', 'reaction', 'synthesis', 'chemical'],\n",
        "            'physics': ['quantum', 'particle', 'energy', 'field'],\n",
        "            'computer_science': ['algorithm', 'data', 'network', 'computational'],\n",
        "            'engineering': ['design', 'system', 'structural', 'mechanical']\n",
        "        }\n",
        "\n",
        "        scores = {}\n",
        "        for domain, indicators in domain_context_indicators.items():\n",
        "            score = sum(1 for indicator in indicators if indicator in topic_text)\n",
        "            if score > 0:\n",
        "                scores[domain] = score\n",
        "\n",
        "        if scores:\n",
        "            return max(scores.items(), key=lambda x: x[1])[0]\n",
        "        return None\n",
        "\n",
        "    def _calculate_confidence_with_context(self, domain_scores, best_domain, topic_text):\n",
        "        \"\"\"Calculate confidence with context consideration\"\"\"\n",
        "        if best_domain not in domain_scores:\n",
        "            return 0.0\n",
        "\n",
        "        best_score = domain_scores[best_domain]\n",
        "        total_score = sum(domain_scores.values())\n",
        "\n",
        "        if total_score == 0:\n",
        "            return 0.0\n",
        "\n",
        "        base_confidence = best_score / total_score\n",
        "\n",
        "        # Adjust based on score dominance\n",
        "        sorted_scores = sorted(domain_scores.values(), reverse=True)\n",
        "        if len(sorted_scores) > 1:\n",
        "            dominance_ratio = sorted_scores[0] / sorted_scores[1]\n",
        "            if dominance_ratio > 2.0:\n",
        "                base_confidence = min(1.0, base_confidence * 1.3)\n",
        "            elif dominance_ratio > 1.5:\n",
        "                base_confidence = min(1.0, base_confidence * 1.15)\n",
        "\n",
        "        # Adjust based on strong indicators in context\n",
        "        strong_indicator_count = sum(1 for indicator in self.strong_indicators.get(best_domain, [])\n",
        "                                   if indicator in topic_text)\n",
        "        if strong_indicator_count >= 2:\n",
        "            base_confidence = min(1.0, base_confidence * 1.1)\n",
        "\n",
        "        return min(1.0, base_confidence)\n",
        "\n",
        "    def _print_enhanced_domain_analysis(self, domain_mapping, model_type):\n",
        "        \"\"\"Print enhanced domain analysis\"\"\"\n",
        "        if not domain_mapping:\n",
        "            print(f\" No topics to analyze for {model_type}\")\n",
        "            return\n",
        "\n",
        "        domain_counts = Counter()\n",
        "        confidence_sum = Counter()\n",
        "\n",
        "        for mapping in domain_mapping.values():\n",
        "            domain = mapping['primary_domain']\n",
        "            domain_counts[domain] += 1\n",
        "            confidence_sum[domain] += mapping['confidence']\n",
        "\n",
        "        print(f\"\\n {model_type.upper()} ENHANCED DOMAIN DISTRIBUTION:\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        total_topics = len(domain_mapping)\n",
        "        for domain, count in domain_counts.most_common():\n",
        "            percentage = (count / total_topics) * 100\n",
        "            avg_confidence = confidence_sum[domain] / count\n",
        "            confidence_level = \"HIGH\" if avg_confidence > 0.7 else \"MEDIUM\" if avg_confidence > 0.5 else \"LOW\"\n",
        "            print(f\"   {domain:22s}: {count:2d} topics ({percentage:5.1f}%) \"\n",
        "                  f\"[conf: {avg_confidence:.3f} - {confidence_level}]\")\n",
        "\n",
        "\n",
        "class EnhancedModelEvaluator:\n",
        "    \"\"\"Enhanced model evaluator with comprehensive metrics\"\"\"\n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "\n",
        "    def evaluate_enhanced_model(self, topic_model, documents, topics,\n",
        "                               embeddings=None, model_type=\"bertopic\",\n",
        "                               domain_mapping=None):\n",
        "        \"\"\"Enhanced model evaluation with comprehensive metrics\"\"\"\n",
        "        print(f\" Performing enhanced {model_type} model evaluation...\")\n",
        "\n",
        "        evaluation = {}\n",
        "\n",
        "        # Basic statistics\n",
        "        if model_type == \"bertopic\":\n",
        "            unique_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
        "            outliers = np.sum(topics == -1)\n",
        "        else:  # LDA\n",
        "            unique_topics = len(set(topics))\n",
        "            outliers = 0\n",
        "\n",
        "        evaluation['basic_stats'] = {\n",
        "            'total_documents': int(len(documents)),\n",
        "            'topics_found': int(unique_topics),\n",
        "            'outliers': int(outliers),\n",
        "            'outlier_percentage': float((outliers / len(topics)) * 100) if len(topics) > 0 else 0.0\n",
        "        }\n",
        "\n",
        "        # Enhanced coherence calculation\n",
        "        coherence = self._calculate_enhanced_coherence(topic_model, documents, topics, model_type)\n",
        "        evaluation['coherence_score'] = float(coherence)\n",
        "\n",
        "        # Topic quality metrics\n",
        "        topic_quality = self._analyze_enhanced_topic_quality(topics, model_type)\n",
        "        evaluation['topic_quality'] = topic_quality\n",
        "\n",
        "        # Domain metrics if available\n",
        "        if domain_mapping:\n",
        "            domain_metrics = self._calculate_domain_metrics(domain_mapping)\n",
        "            evaluation['domain_metrics'] = domain_metrics\n",
        "\n",
        "        # Additional metrics\n",
        "        additional_metrics = self._calculate_additional_metrics(topics, embeddings, model_type)\n",
        "        evaluation['additional_metrics'] = additional_metrics\n",
        "\n",
        "        # Overall score with enhanced weighting\n",
        "        evaluation['overall_score'] = float(self._calculate_enhanced_overall_score(evaluation, model_type))\n",
        "        evaluation['model_type'] = model_type\n",
        "\n",
        "        self.results = evaluation\n",
        "        self._print_enhanced_evaluation_results(model_type)\n",
        "\n",
        "        return evaluation\n",
        "\n",
        "    def _calculate_enhanced_coherence(self, topic_model, documents, topics, model_type):\n",
        "        \"\"\"Enhanced coherence calculation\"\"\"\n",
        "        try:\n",
        "            topic_words = []\n",
        "\n",
        "            if model_type == \"bertopic\":\n",
        "                for topic in set(topics):\n",
        "                    if topic != -1:\n",
        "                        words = topic_model.get_topic(topic)\n",
        "                        if words and len(words) >= 3:\n",
        "                            topic_words.append([word for word, _ in words[:8]])\n",
        "            else:  # LDA\n",
        "                if hasattr(topic_model, 'get_topic_words'):\n",
        "                    for topic_id in set(topics):\n",
        "                        words = topic_model.get_topic_words(topic_id)\n",
        "                        if words and len(words) >= 3:\n",
        "                            topic_words.append(words[:8])\n",
        "\n",
        "            if len(topic_words) < 2:\n",
        "                return 0.0\n",
        "\n",
        "            # Tokenize documents\n",
        "            tokenized_docs = [doc.split() for doc in documents if doc and len(doc.split()) > 10]\n",
        "\n",
        "            if len(tokenized_docs) < 15:\n",
        "                return 0.0\n",
        "\n",
        "            # Calculate coherence\n",
        "            dictionary = corpora.Dictionary(tokenized_docs)\n",
        "            dictionary.filter_extremes(no_below=3, no_above=0.75)\n",
        "            corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "            if len(corpus) < 10:\n",
        "                return 0.0\n",
        "\n",
        "            coherence_model = CoherenceModel(\n",
        "                topics=topic_words,\n",
        "                texts=tokenized_docs,\n",
        "                dictionary=dictionary,\n",
        "                coherence='c_v',\n",
        "                processes=1\n",
        "            )\n",
        "\n",
        "            coherence_score = coherence_model.get_coherence()\n",
        "            return max(0.0, float(coherence_score))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Enhanced coherence calculation failed for {model_type}: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _analyze_enhanced_topic_quality(self, topics, model_type):\n",
        "        \"\"\"Analyze enhanced topic quality metrics\"\"\"\n",
        "        topic_counts = Counter(topics)\n",
        "\n",
        "        if model_type == \"bertopic\" and -1 in topic_counts:\n",
        "            valid_topics = {k: v for k, v in topic_counts.items() if k != -1}\n",
        "        else:\n",
        "            valid_topics = topic_counts\n",
        "\n",
        "        if not valid_topics:\n",
        "            return {\n",
        "                'balance_score': 0.0,\n",
        "                'size_variation': 0.0,\n",
        "                'avg_topic_size': 0.0,\n",
        "                'min_topic_size': 0.0,\n",
        "                'max_topic_size': 0.0,\n",
        "                'topic_diversity': 0.0,\n",
        "                'topic_distribution_score': 0.0\n",
        "            }\n",
        "\n",
        "        sizes = list(valid_topics.values())\n",
        "\n",
        "        # Calculate topic diversity\n",
        "        total_docs = sum(sizes)\n",
        "        if total_docs > 0:\n",
        "            proportions = [size / total_docs for size in sizes]\n",
        "            diversity = 1 - sum(p**2 for p in proportions)\n",
        "        else:\n",
        "            diversity = 0.0\n",
        "\n",
        "        # Calculate distribution score (penalize extreme sizes)\n",
        "        avg_size = np.mean(sizes)\n",
        "        if avg_size > 0:\n",
        "            size_variation = np.std(sizes) / avg_size\n",
        "            distribution_score = 1 / (1 + size_variation)\n",
        "        else:\n",
        "            distribution_score = 0.0\n",
        "\n",
        "        return {\n",
        "            'balance_score': float(min(sizes) / max(sizes)) if max(sizes) > 0 else 0.0,\n",
        "            'size_variation': float(np.std(sizes) / np.mean(sizes)) if np.mean(sizes) > 0 else 0.0,\n",
        "            'avg_topic_size': float(np.mean(sizes)),\n",
        "            'min_topic_size': float(min(sizes)),\n",
        "            'max_topic_size': float(max(sizes)),\n",
        "            'topic_diversity': float(diversity),\n",
        "            'topic_distribution_score': float(distribution_score)\n",
        "        }\n",
        "\n",
        "    def _calculate_domain_metrics(self, domain_mapping):\n",
        "        \"\"\"Calculate domain-specific metrics\"\"\"\n",
        "        if not domain_mapping:\n",
        "            return {}\n",
        "\n",
        "        domains = [mapping['primary_domain'] for mapping in domain_mapping.values()]\n",
        "        confidences = [mapping['confidence'] for mapping in domain_mapping.values()]\n",
        "\n",
        "        domain_counts = Counter(domains)\n",
        "\n",
        "        return {\n",
        "            'unique_domains': len(domain_counts),\n",
        "            'domain_entropy': self._calculate_entropy(domains),\n",
        "            'avg_confidence': float(np.mean(confidences)) if confidences else 0.0,\n",
        "            'domain_distribution': {k: int(v) for k, v in domain_counts.items()}\n",
        "        }\n",
        "\n",
        "    def _calculate_entropy(self, items):\n",
        "        \"\"\"Calculate entropy of a distribution\"\"\"\n",
        "        if not items:\n",
        "            return 0.0\n",
        "\n",
        "        counts = Counter(items)\n",
        "        total = len(items)\n",
        "        entropy = 0.0\n",
        "\n",
        "        for count in counts.values():\n",
        "            p = count / total\n",
        "            entropy -= p * np.log2(p)\n",
        "\n",
        "        return entropy / np.log2(len(counts)) if len(counts) > 1 else 0.0\n",
        "\n",
        "    def _calculate_additional_metrics(self, topics, embeddings, model_type):\n",
        "        \"\"\"Calculate additional evaluation metrics\"\"\"\n",
        "        try:\n",
        "            if model_type == \"bertopic\":\n",
        "                unique_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
        "            else:\n",
        "                unique_topics = len(set(topics))\n",
        "\n",
        "            # Topic count score (penalize too few or too many topics)\n",
        "            ideal_topics = 20  # Target number\n",
        "            topic_count_score = 1 / (1 + abs(unique_topics - ideal_topics) / ideal_topics)\n",
        "\n",
        "            return {\n",
        "                'topic_count_score': float(topic_count_score),\n",
        "                'unique_topics': int(unique_topics),\n",
        "                'ideal_topics_deviation': float(abs(unique_topics - ideal_topics) / ideal_topics)\n",
        "            }\n",
        "        except:\n",
        "            return {\n",
        "                'topic_count_score': 0.0,\n",
        "                'unique_topics': 0,\n",
        "                'ideal_topics_deviation': 1.0\n",
        "            }\n",
        "\n",
        "    def _calculate_enhanced_overall_score(self, evaluation, model_type):\n",
        "        \"\"\"Calculate enhanced overall quality score\"\"\"\n",
        "        basic = evaluation['basic_stats']\n",
        "        coherence = evaluation['coherence_score']\n",
        "        quality = evaluation['topic_quality']\n",
        "        additional = evaluation['additional_metrics']\n",
        "\n",
        "        if model_type == \"bertopic\":\n",
        "            # BERTopic scoring\n",
        "            score = (\n",
        "                coherence * 0.35 +\n",
        "                (1 - basic['outlier_percentage'] / 100) * 0.25 +\n",
        "                quality['balance_score'] * 0.15 +\n",
        "                quality['topic_distribution_score'] * 0.15 +\n",
        "                additional['topic_count_score'] * 0.10\n",
        "            )\n",
        "        else:\n",
        "            # LDA scoring\n",
        "            score = (\n",
        "                coherence * 0.40 +\n",
        "                quality['balance_score'] * 0.20 +\n",
        "                quality['topic_distribution_score'] * 0.20 +\n",
        "                additional['topic_count_score'] * 0.20\n",
        "            )\n",
        "\n",
        "        # Add domain metrics if available\n",
        "        if 'domain_metrics' in evaluation:\n",
        "            domain = evaluation['domain_metrics']\n",
        "            domain_score = (domain.get('avg_confidence', 0) +\n",
        "                          (1 - domain.get('domain_entropy', 0))) / 2\n",
        "            score = score * 0.8 + domain_score * 0.2\n",
        "\n",
        "        return min(max(score, 0.0), 1.0)\n",
        "\n",
        "    def _print_enhanced_evaluation_results(self, model_type):\n",
        "        \"\"\"Print enhanced evaluation results\"\"\"\n",
        "        print(f\"\\n {model_type.upper()} ENHANCED MODEL EVALUATION:\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        basic = self.results['basic_stats']\n",
        "        print(f\"   • Documents: {basic['total_documents']}\")\n",
        "        print(f\"   • Topics: {basic['topics_found']}\")\n",
        "        if model_type == \"bertopic\":\n",
        "            print(f\"   • Outliers: {basic['outliers']} ({basic['outlier_percentage']:.1f}%)\")\n",
        "\n",
        "        print(f\"   • Coherence Score: {self.results['coherence_score']:.3f}\")\n",
        "\n",
        "        quality = self.results['topic_quality']\n",
        "        print(f\"   • Balance Score: {quality['balance_score']:.3f}\")\n",
        "        print(f\"   • Topic Diversity: {quality['topic_diversity']:.3f}\")\n",
        "        print(f\"   • Distribution Score: {quality['topic_distribution_score']:.3f}\")\n",
        "        print(f\"   • Avg Topic Size: {quality['avg_topic_size']:.1f}\")\n",
        "        print(f\"   • Size Range: {quality['min_topic_size']} - {quality['max_topic_size']}\")\n",
        "\n",
        "        additional = self.results['additional_metrics']\n",
        "        print(f\"   • Topic Count Score: {additional['topic_count_score']:.3f}\")\n",
        "        print(f\"   • Topics from Ideal: {additional['ideal_topics_deviation']:.1%}\")\n",
        "\n",
        "        # Domain metrics if available\n",
        "        if 'domain_metrics' in self.results:\n",
        "            domain = self.results['domain_metrics']\n",
        "            print(f\"   • Unique Domains: {domain['unique_domains']}\")\n",
        "            print(f\"   • Avg Domain Confidence: {domain.get('avg_confidence', 0):.3f}\")\n",
        "            print(f\"   • Domain Entropy: {domain.get('domain_entropy', 0):.3f}\")\n",
        "\n",
        "        overall = self.results['overall_score']\n",
        "        assessment = \"EXCELLENT\" if overall > 0.7 else \"GOOD\" if overall > 0.5 else \"FAIR\" if overall > 0.3 else \"POOR\"\n",
        "        print(f\"   • Overall Score: {overall:.3f} - {assessment}\")\n",
        "\n",
        "\n",
        "class EnhancedModelComparator:\n",
        "    \"\"\"Enhanced model comparator with comprehensive metrics\"\"\"\n",
        "    def __init__(self):\n",
        "        self.comparison_results = {}\n",
        "\n",
        "    def compare_models_enhanced(self, bertopic_results, lda_results, documents):\n",
        "        \"\"\"Enhanced model comparison\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\" ENHANCED MODEL COMPARISON\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Extract topic samples for context\n",
        "        bertopic_samples = self._extract_topic_samples(\n",
        "            bertopic_results['topics'], documents, bertopic_results['domain_mapping']\n",
        "        )\n",
        "        lda_samples = self._extract_topic_samples(\n",
        "            lda_results['topics'], documents, lda_results['domain_mapping']\n",
        "        )\n",
        "\n",
        "        # Build enhanced comparison\n",
        "        comparison = {\n",
        "            'bertopic': self._enhance_model_results(\n",
        "                bertopic_results, \"bertopic\", bertopic_samples\n",
        "            ),\n",
        "            'lda': self._enhance_model_results(\n",
        "                lda_results, \"lda\", lda_samples\n",
        "            ),\n",
        "            'comparison_timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        # Print enhanced comparison\n",
        "        self._print_enhanced_comparison(comparison)\n",
        "\n",
        "        # Save comparison\n",
        "        self.comparison_results = comparison\n",
        "        self._save_enhanced_comparison()\n",
        "\n",
        "        return comparison\n",
        "\n",
        "    def _extract_topic_samples(self, topics, documents, domain_mapping):\n",
        "        \"\"\"Extract sample documents for each topic\"\"\"\n",
        "        samples = {}\n",
        "\n",
        "        for topic_id in set(topics):\n",
        "            if topic_id == -1 or topic_id not in domain_mapping:\n",
        "                continue\n",
        "\n",
        "            # Get indices of documents in this topic\n",
        "            doc_indices = [i for i, t in enumerate(topics) if t == topic_id]\n",
        "\n",
        "            # Sample documents\n",
        "            sample_size = min(config.TOPIC_EVALUATION_SAMPLE_SIZE, len(doc_indices))\n",
        "            if sample_size > 0:\n",
        "                sample_indices = np.random.choice(doc_indices, size=sample_size, replace=False)\n",
        "                samples[topic_id] = [documents[i] for i in sample_indices]\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def _enhance_model_results(self, model_results, model_type, topic_samples):\n",
        "        \"\"\"Enhance model results with additional metrics\"\"\"\n",
        "        enhanced = {\n",
        "            'evaluation': model_results['evaluation'],\n",
        "            'domain_mapping': model_results['domain_mapping'],\n",
        "            'topics_found': len(model_results['domain_mapping']),\n",
        "            'domains_covered': len(set(\n",
        "                m['primary_domain'] for m in model_results['domain_mapping'].values()\n",
        "            )),\n",
        "            'avg_confidence': np.mean([\n",
        "                m['confidence'] for m in model_results['domain_mapping'].values()\n",
        "            ]),\n",
        "            'model_type': model_type,\n",
        "            'topic_samples_available': len(topic_samples)\n",
        "        }\n",
        "\n",
        "        # Add domain-specific metrics\n",
        "        if model_results['domain_mapping']:\n",
        "            domains = [m['primary_domain'] for m in model_results['domain_mapping'].values()]\n",
        "            domain_counts = Counter(domains)\n",
        "            enhanced['domain_distribution'] = dict(domain_counts.most_common())\n",
        "\n",
        "            # Calculate domain purity (how concentrated domains are)\n",
        "            if len(domain_counts) > 1:\n",
        "                total = sum(domain_counts.values())\n",
        "                max_domain = max(domain_counts.values())\n",
        "                enhanced['domain_purity'] = max_domain / total\n",
        "            else:\n",
        "                enhanced['domain_purity'] = 1.0\n",
        "\n",
        "        return enhanced\n",
        "\n",
        "    def _print_enhanced_comparison(self, comparison):\n",
        "        \"\"\"Print enhanced comparison table\"\"\"\n",
        "\n",
        "        bert = comparison['bertopic']\n",
        "        lda = comparison['lda']\n",
        "\n",
        "        print(\"\\n ENHANCED PERFORMANCE COMPARISON:\")\n",
        "        print(\"-\" * 100)\n",
        "        print(f\"{'Metric':<30} {'BERTopic':<15} {'LDA':<15} {'Winner':<10} {'Notes':<30}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        metrics = [\n",
        "            ('Coherence Score',\n",
        "             bert['evaluation']['coherence_score'],\n",
        "             lda['evaluation']['coherence_score'],\n",
        "             'Higher is better'),\n",
        "\n",
        "            ('Overall Quality Score',\n",
        "             bert['evaluation']['overall_score'],\n",
        "             lda['evaluation']['overall_score'],\n",
        "             'Model evaluation score'),\n",
        "\n",
        "            ('Topics Found',\n",
        "             bert['topics_found'],\n",
        "             lda['topics_found'],\n",
        "             'Closer to ideal (20) is better'),\n",
        "\n",
        "            ('Domains Covered',\n",
        "             bert['domains_covered'],\n",
        "             lda['domains_covered'],\n",
        "             'Domain diversity'),\n",
        "\n",
        "            ('Avg Confidence',\n",
        "             bert['avg_confidence'],\n",
        "             lda['avg_confidence'],\n",
        "             'Classification confidence'),\n",
        "\n",
        "            ('Topic Diversity',\n",
        "             bert['evaluation']['topic_quality']['topic_diversity'],\n",
        "             lda['evaluation']['topic_quality']['topic_diversity'],\n",
        "             'Higher = more balanced'),\n",
        "\n",
        "            ('Distribution Score',\n",
        "             bert['evaluation']['topic_quality']['topic_distribution_score'],\n",
        "             lda['evaluation']['topic_quality']['topic_distribution_score'],\n",
        "             'Topic size distribution'),\n",
        "        ]\n",
        "\n",
        "        # Add domain metrics if available\n",
        "        if 'domain_purity' in bert:\n",
        "            metrics.extend([\n",
        "                ('Domain Purity',\n",
        "                 bert['domain_purity'],\n",
        "                 lda['domain_purity'],\n",
        "                 'Higher = more concentrated domains'),\n",
        "            ])\n",
        "\n",
        "        for metric_name, bert_value, lda_value, notes in metrics:\n",
        "            # Format values\n",
        "            if isinstance(bert_value, float):\n",
        "                bert_display = f\"{bert_value:.3f}\"\n",
        "                lda_display = f\"{lda_value:.3f}\"\n",
        "            else:\n",
        "                bert_display = str(bert_value)\n",
        "                lda_display = str(lda_value)\n",
        "\n",
        "            # Determine winner\n",
        "            if isinstance(bert_value, (int, float)) and isinstance(lda_value, (int, float)):\n",
        "                # For most metrics, higher is better\n",
        "                if metric_name in ['Coherence Score', 'Overall Quality Score',\n",
        "                                 'Avg Confidence', 'Topic Diversity', 'Distribution Score',\n",
        "                                 'Domain Purity']:\n",
        "                    winner = \"BERTopic\" if bert_value > lda_value else \"LDA\" if lda_value > bert_value else \"Tie\"\n",
        "                elif metric_name == 'Topics Found':\n",
        "                    # Closer to 20 is better\n",
        "                    bert_dev = abs(bert_value - 20)\n",
        "                    lda_dev = abs(lda_value - 20)\n",
        "                    winner = \"BERTopic\" if bert_dev < lda_dev else \"LDA\" if lda_dev < bert_dev else \"Tie\"\n",
        "                elif metric_name == 'Domains Covered':\n",
        "                    # Balance is important - neither too few nor too many\n",
        "                    bert_score = 1 / (1 + abs(bert_value - 6))  # Target 6 domains\n",
        "                    lda_score = 1 / (1 + abs(lda_value - 6))\n",
        "                    winner = \"BERTopic\" if bert_score > lda_score else \"LDA\" if lda_score > bert_score else \"Tie\"\n",
        "                else:\n",
        "                    winner = \"N/A\"\n",
        "            else:\n",
        "                winner = \"N/A\"\n",
        "\n",
        "            print(f\"{metric_name:<30} {bert_display:<15} {lda_display:<15} {winner:<10} {notes:<30}\")\n",
        "\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # Overall assessment\n",
        "        bert_quality = bert['evaluation']['overall_score']\n",
        "        lda_quality = lda['evaluation']['overall_score']\n",
        "\n",
        "        if bert_quality > lda_quality * 1.1:\n",
        "            print(f\"\\n OVERALL ASSESSMENT: BERTopic significantly better\")\n",
        "            print(f\"   • BERTopic advantage: +{(bert_quality/lda_quality - 1)*100:.1f}%\")\n",
        "            print(f\"   • Recommendation: Use BERTopic for this dataset\")\n",
        "        elif lda_quality > bert_quality * 1.1:\n",
        "            print(f\"\\n OVERALL ASSESSMENT: LDA significantly better\")\n",
        "            print(f\"   • LDA advantage: +{(lda_quality/bert_quality - 1)*100:.1f}%\")\n",
        "            print(f\"   • Recommendation: Use LDA for this dataset\")\n",
        "        else:\n",
        "            print(f\"\\n OVERALL ASSESSMENT: Comparable performance\")\n",
        "            print(f\"   • BERTopic: {bert_quality:.3f}, LDA: {lda_quality:.3f}\")\n",
        "            print(f\"   • Recommendation: Consider computational requirements\")\n",
        "\n",
        "        # Domain distribution comparison\n",
        "        print(f\"\\n DOMAIN DISTRIBUTION COMPARISON:\")\n",
        "        bert_domains = bert.get('domain_distribution', {})\n",
        "        lda_domains = lda.get('domain_distribution', {})\n",
        "\n",
        "        all_domains = set(bert_domains.keys()) | set(lda_domains.keys())\n",
        "        if all_domains:\n",
        "            print(f\"{'Domain':<20} {'BERTopic':<10} {'LDA':<10}\")\n",
        "            print(\"-\" * 45)\n",
        "            for domain in sorted(all_domains):\n",
        "                bert_count = bert_domains.get(domain, 0)\n",
        "                lda_count = lda_domains.get(domain, 0)\n",
        "                print(f\"{domain:<20} {bert_count:<10} {lda_count:<10}\")\n",
        "\n",
        "    def _save_enhanced_comparison(self):\n",
        "        \"\"\"Save enhanced comparison results\"\"\"\n",
        "        comparison_dir = config.COMPARISON_DIR\n",
        "\n",
        "        # Save comparison data\n",
        "        with open(f\"{comparison_dir}/enhanced_comparison.json\", 'w') as f:\n",
        "            json.dump(self.comparison_results, f, indent=2, default=str)\n",
        "\n",
        "        # Create enhanced visualizations\n",
        "        self._create_enhanced_visualizations()\n",
        "\n",
        "        print(f\" Enhanced comparison results saved to: {comparison_dir}\")\n",
        "\n",
        "    def _create_enhanced_visualizations(self):\n",
        "        \"\"\"Create enhanced comparison visualizations\"\"\"\n",
        "        bert = self.comparison_results['bertopic']\n",
        "        lda = self.comparison_results['lda']\n",
        "\n",
        "        # Prepare data for visualization\n",
        "        metrics = ['Coherence', 'Overall', 'Topics*', 'Domains', 'Confidence', 'Diversity', 'Distribution']\n",
        "\n",
        "        bert_scores = [\n",
        "            bert['evaluation']['coherence_score'],\n",
        "            bert['evaluation']['overall_score'],\n",
        "            bert['topics_found'] / 20,  # Normalized\n",
        "            min(bert['domains_covered'] / 8, 1.0),  # Normalized\n",
        "            bert['avg_confidence'],\n",
        "            bert['evaluation']['topic_quality']['topic_diversity'],\n",
        "            bert['evaluation']['topic_quality']['topic_distribution_score']\n",
        "        ]\n",
        "\n",
        "        lda_scores = [\n",
        "            lda['evaluation']['coherence_score'],\n",
        "            lda['evaluation']['overall_score'],\n",
        "            lda['topics_found'] / 20,\n",
        "            min(lda['domains_covered'] / 8, 1.0),\n",
        "            lda['avg_confidence'],\n",
        "            lda['evaluation']['topic_quality']['topic_diversity'],\n",
        "            lda['evaluation']['topic_quality']['topic_distribution_score']\n",
        "        ]\n",
        "\n",
        "        # Create radar chart\n",
        "        fig = go.Figure()\n",
        "\n",
        "        fig.add_trace(go.Scatterpolar(\n",
        "            r=bert_scores,\n",
        "            theta=metrics,\n",
        "            fill='toself',\n",
        "            name='BERTopic',\n",
        "            line_color='blue',\n",
        "            opacity=0.8\n",
        "        ))\n",
        "\n",
        "        fig.add_trace(go.Scatterpolar(\n",
        "            r=lda_scores,\n",
        "            theta=metrics,\n",
        "            fill='toself',\n",
        "            name='LDA',\n",
        "            line_color='red',\n",
        "            opacity=0.8\n",
        "        ))\n",
        "\n",
        "        fig.update_layout(\n",
        "            polar=dict(\n",
        "                radialaxis=dict(\n",
        "                    visible=True,\n",
        "                    range=[0, 1]\n",
        "                )),\n",
        "            showlegend=True,\n",
        "            title=\"Enhanced Model Comparison: BERTopic vs LDA\",\n",
        "            title_font_size=16,\n",
        "            legend=dict(\n",
        "                yanchor=\"top\",\n",
        "                y=0.99,\n",
        "                xanchor=\"left\",\n",
        "                x=0.01\n",
        "            )\n",
        "        )\n",
        "\n",
        "        fig.write_html(f\"{config.COMPARISON_DIR}/enhanced_comparison_radar.html\")\n",
        "\n",
        "        # Create comprehensive bar chart\n",
        "        fig, ax = plt.subplots(figsize=(14, 8))\n",
        "        x = np.arange(len(metrics))\n",
        "        width = 0.35\n",
        "\n",
        "        bars1 = ax.bar(x - width/2, bert_scores, width, label='BERTopic',\n",
        "                      color='blue', alpha=0.7, edgecolor='black')\n",
        "        bars2 = ax.bar(x + width/2, lda_scores, width, label='LDA',\n",
        "                      color='red', alpha=0.7, edgecolor='black')\n",
        "\n",
        "        ax.set_xlabel('Evaluation Metrics', fontsize=12)\n",
        "        ax.set_ylabel('Normalized Scores', fontsize=12)\n",
        "        ax.set_title('Enhanced Model Comparison: BERTopic vs LDA', fontsize=14, fontweight='bold')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bars in [bars1, bars2]:\n",
        "            for bar in bars:\n",
        "                height = bar.get_height()\n",
        "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                       f'{height:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{config.COMPARISON_DIR}/enhanced_comparison_bar.png\",\n",
        "                   dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(\" Enhanced visualizations created\")\n",
        "\n",
        "\n",
        "class EnhancedDomainModelingPipeline:\n",
        "    \"\"\"Enhanced pipeline without LLM\"\"\"\n",
        "    def __init__(self):\n",
        "        self.data_processor = EnhancedDataProcessor()\n",
        "        self.topic_modeler = EnhancedTopicModeler()\n",
        "        self.fixed_lda = FixedLDAModel()\n",
        "        self.domain_classifier = EnhancedDomainClassifier()\n",
        "        self.evaluator = EnhancedModelEvaluator()\n",
        "        self.comparator = EnhancedModelComparator()\n",
        "\n",
        "    def run_enhanced_pipeline(self):\n",
        "        \"\"\"Run enhanced pipeline without LLM\"\"\"\n",
        "        print(\" STARTING ENHANCED DOMAIN MODELING PIPELINE\")\n",
        "        print(\"=\" * 70)\n",
        "        print(\" Features:\")\n",
        "        print(\"   • Enhanced data processing\")\n",
        "        print(\"   • Fixed LDA implementation with proper topic extraction\")\n",
        "        print(\"   • Enhanced domain classification with context\")\n",
        "        print(\"   • Comprehensive model comparison\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        try:\n",
        "            # Step 1: Enhanced Data Processing\n",
        "            print(\"\\n STEP 1: ENHANCED DATA PROCESSING\")\n",
        "            df = self.data_processor.load_and_process_data()\n",
        "            documents = df['cleaned_text'].tolist()\n",
        "            print(f\" Processing {len(documents)} documents\")\n",
        "\n",
        "            # Step 2: Enhanced BERTopic Modeling\n",
        "            print(\"\\n STEP 2: ENHANCED BERTopic MODELING\")\n",
        "            bertopic_topics, bertopic_probabilities = self.topic_modeler.fit_enhanced_model(documents)\n",
        "\n",
        "            # Step 3: Fixed LDA Modeling\n",
        "            print(\"\\n STEP 3: FIXED LDA MODELING\")\n",
        "            self.fixed_lda.fit(documents)\n",
        "            lda_topics = self.fixed_lda.predict_topics(documents)\n",
        "            lda_evaluation = self.fixed_lda.evaluate(documents)\n",
        "\n",
        "            # Step 4: Enhanced Domain Classification\n",
        "            print(\"\\n STEP 4: ENHANCED DOMAIN CLASSIFICATION\")\n",
        "\n",
        "            # Extract topic samples for context\n",
        "            bertopic_samples = self._extract_topic_samples(bertopic_topics, documents)\n",
        "            lda_samples = self._extract_topic_samples(lda_topics, documents)\n",
        "\n",
        "            print(\"   Classifying BERTopic topics with context...\")\n",
        "            bertopic_domain_mapping = self.domain_classifier.classify_topic_domains(\n",
        "                self.topic_modeler.topic_model, bertopic_topics, \"bertopic\", bertopic_samples\n",
        "            )\n",
        "\n",
        "            print(\"\\n   Classifying LDA topics with context...\")\n",
        "            lda_domain_mapping = self.domain_classifier.classify_topic_domains(\n",
        "                self.fixed_lda, lda_topics, \"lda\", lda_samples\n",
        "            )\n",
        "\n",
        "            # Step 5: Enhanced Evaluation\n",
        "            print(\"\\n STEP 5: ENHANCED MODEL EVALUATION\")\n",
        "\n",
        "            bertopic_evaluation = self.evaluator.evaluate_enhanced_model(\n",
        "                self.topic_modeler.topic_model, documents, bertopic_topics,\n",
        "                self.topic_modeler.embeddings, \"bertopic\", bertopic_domain_mapping\n",
        "            )\n",
        "\n",
        "            lda_evaluation_enhanced = self.evaluator.evaluate_enhanced_model(\n",
        "                self.fixed_lda, documents, lda_topics, None, \"lda\", lda_domain_mapping\n",
        "            )\n",
        "\n",
        "            # Step 6: Enhanced Model Comparison\n",
        "            print(\"\\n STEP 6: ENHANCED MODEL COMPARISON\")\n",
        "\n",
        "            bertopic_results = {\n",
        "                'topics': bertopic_topics,\n",
        "                'domain_mapping': bertopic_domain_mapping,\n",
        "                'evaluation': bertopic_evaluation,\n",
        "                'model': self.topic_modeler.topic_model\n",
        "            }\n",
        "\n",
        "            lda_results = {\n",
        "                'topics': lda_topics,\n",
        "                'domain_mapping': lda_domain_mapping,\n",
        "                'evaluation': lda_evaluation_enhanced,\n",
        "                'topic_model': self.fixed_lda\n",
        "            }\n",
        "\n",
        "            comparison_results = self.comparator.compare_models_enhanced(\n",
        "                bertopic_results, lda_results, documents\n",
        "            )\n",
        "\n",
        "            # Step 7: Save Enhanced Results\n",
        "            print(\"\\n STEP 7: SAVING ENHANCED RESULTS\")\n",
        "            self._save_enhanced_results(df, bertopic_results, lda_results, comparison_results)\n",
        "\n",
        "            # Step 8: Final Summary\n",
        "            print(\"\\n STEP 8: FINAL ENHANCED SUMMARY\")\n",
        "            self._print_enhanced_summary(bertopic_results, lda_results, comparison_results)\n",
        "\n",
        "            print(\"\\n ENHANCED PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "\n",
        "            return {\n",
        "                'bertopic': bertopic_results,\n",
        "                'lda': lda_results,\n",
        "                'comparison': comparison_results,\n",
        "                'documents': documents\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Enhanced pipeline execution failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def _extract_topic_samples(self, topics, documents):\n",
        "        \"\"\"Extract sample documents for each topic\"\"\"\n",
        "        samples = {}\n",
        "\n",
        "        for topic_id in set(topics):\n",
        "            if topic_id == -1:\n",
        "                continue\n",
        "\n",
        "            # Get indices of documents in this topic\n",
        "            doc_indices = [i for i, t in enumerate(topics) if t == topic_id]\n",
        "\n",
        "            # Sample documents\n",
        "            sample_size = min(config.TOPIC_EVALUATION_SAMPLE_SIZE, len(doc_indices))\n",
        "            if sample_size > 0:\n",
        "                sample_indices = np.random.choice(doc_indices, size=sample_size, replace=False)\n",
        "                samples[topic_id] = [documents[i] for i in sample_indices]\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def _save_enhanced_results(self, df, bertopic_results, lda_results, comparison_results):\n",
        "        \"\"\"Save enhanced results\"\"\"\n",
        "        try:\n",
        "            # Save BERTopic results\n",
        "            bert_df = df.copy()\n",
        "            bert_df['topic'] = bertopic_results['topics']\n",
        "            bert_df['domain'] = bert_df['topic'].map(\n",
        "                {k: v['primary_domain'] for k, v in bertopic_results['domain_mapping'].items()}\n",
        "            )\n",
        "            bert_df['confidence'] = bert_df['topic'].map(\n",
        "                {k: v['confidence'] for k, v in bertopic_results['domain_mapping'].items()}\n",
        "            )\n",
        "            bert_df['classification_method'] = bert_df['topic'].map(\n",
        "                {k: v.get('classification_method', 'enhanced_keyword') for k, v in bertopic_results['domain_mapping'].items()}\n",
        "            )\n",
        "            bert_df.to_csv(f\"{config.OUTPUT_DIR}/enhanced_bertopic_assignments.csv\", index=False)\n",
        "\n",
        "            # Save LDA results\n",
        "            lda_df = df.copy()\n",
        "            lda_df['topic'] = lda_results['topics']\n",
        "            lda_df['domain'] = lda_df['topic'].map(\n",
        "                {k: v['primary_domain'] for k, v in lda_results['domain_mapping'].items()}\n",
        "            )\n",
        "            lda_df['confidence'] = lda_df['topic'].map(\n",
        "                {k: v['confidence'] for k, v in lda_results['domain_mapping'].items()}\n",
        "            )\n",
        "            lda_df['classification_method'] = lda_df['topic'].map(\n",
        "                {k: v.get('classification_method', 'enhanced_keyword') for k, v in lda_results['domain_mapping'].items()}\n",
        "            )\n",
        "            lda_df.to_csv(f\"{config.COMPARISON_DIR}/enhanced_lda_assignments.csv\", index=False)\n",
        "\n",
        "            # Save model evaluations\n",
        "            with open(f\"{config.OUTPUT_DIR}/enhanced_evaluations.json\", 'w') as f:\n",
        "                json.dump({\n",
        "                    'bertopic': bertopic_results['evaluation'],\n",
        "                    'lda': lda_results['evaluation'],\n",
        "                    'comparison': comparison_results\n",
        "                }, f, indent=2, default=str)\n",
        "\n",
        "            # Save topic models\n",
        "            self.topic_modeler.topic_model.save(f\"{config.OUTPUT_DIR}/enhanced_topic_model\")\n",
        "\n",
        "            # Save LDA topic words\n",
        "            lda_topics_dict = self.fixed_lda.get_topics_dict(n_words=15)\n",
        "            with open(f\"{config.COMPARISON_DIR}/enhanced_lda_topics.json\", 'w') as f:\n",
        "                json.dump(lda_topics_dict, f, indent=2)\n",
        "\n",
        "            print(f\" Enhanced results saved successfully\")\n",
        "            print(f\" BERTopic results: {config.OUTPUT_DIR}\")\n",
        "            print(f\" LDA results: {config.COMPARISON_DIR}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error saving enhanced results: {e}\")\n",
        "\n",
        "    def _print_enhanced_summary(self, bertopic_results, lda_results, comparison_results):\n",
        "        \"\"\"Print enhanced summary\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\" ENHANCED PIPELINE SUMMARY\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # BERTopic summary\n",
        "        bert_domains = set(\n",
        "            m['primary_domain'] for m in bertopic_results['domain_mapping'].values()\n",
        "        )\n",
        "        bert_avg_conf = np.mean([\n",
        "            m['confidence'] for m in bertopic_results['domain_mapping'].values()\n",
        "        ])\n",
        "\n",
        "        print(f\"\\n BERTopic Enhanced Results:\")\n",
        "        print(f\"   • Topics classified: {len(bertopic_results['domain_mapping'])}\")\n",
        "        print(f\"   • Domains identified: {len(bert_domains)}\")\n",
        "        print(f\"   • Domains: {', '.join(sorted(bert_domains))}\")\n",
        "        print(f\"   • Average confidence: {bert_avg_conf:.3f}\")\n",
        "        print(f\"   • Coherence score: {bertopic_results['evaluation']['coherence_score']:.3f}\")\n",
        "        print(f\"   • Overall quality: {bertopic_results['evaluation']['overall_score']:.3f}\")\n",
        "\n",
        "        # LDA summary\n",
        "        lda_domains = set(\n",
        "            m['primary_domain'] for m in lda_results['domain_mapping'].values()\n",
        "        )\n",
        "        lda_avg_conf = np.mean([\n",
        "            m['confidence'] for m in lda_results['domain_mapping'].values()\n",
        "        ])\n",
        "\n",
        "        print(f\"\\n LDA Enhanced Results:\")\n",
        "        print(f\"   • Topics classified: {len(lda_results['domain_mapping'])}\")\n",
        "        print(f\"   • Domains identified: {len(lda_domains)}\")\n",
        "        print(f\"   • Domains: {', '.join(sorted(lda_domains))}\")\n",
        "        print(f\"   • Average confidence: {lda_avg_conf:.3f}\")\n",
        "        print(f\"   • Coherence score: {lda_results['evaluation']['coherence_score']:.3f}\")\n",
        "        print(f\"   • Perplexity: {self.fixed_lda.perplexity_score:.2f}\")\n",
        "        print(f\"   • Overall quality: {lda_results['evaluation']['overall_score']:.3f}\")\n",
        "\n",
        "        # Performance comparison\n",
        "        print(f\"\\n PERFORMANCE COMPARISON:\")\n",
        "        bert_quality = bertopic_results['evaluation']['overall_score']\n",
        "        lda_quality = lda_results['evaluation']['overall_score']\n",
        "\n",
        "        if bert_quality > lda_quality:\n",
        "            advantage = (bert_quality / lda_quality - 1) * 100\n",
        "            print(f\"   • BERTopic is {advantage:.1f}% better than LDA\")\n",
        "            print(f\"   • Recommendation: Use BERTopic for this dataset\")\n",
        "        else:\n",
        "            advantage = (lda_quality / bert_quality - 1) * 100\n",
        "            print(f\"   • LDA is {advantage:.1f}% better than BERTopic\")\n",
        "            print(f\"   • Recommendation: Use LDA for this dataset\")\n",
        "\n",
        "        print(f\"\\n Next steps:\")\n",
        "        print(f\"   1. Check {config.OUTPUT_DIR}/ for detailed BERTopic results\")\n",
        "        print(f\"   2. Check {config.COMPARISON_DIR}/ for LDA results and comparison\")\n",
        "        print(f\"   3. Review the enhanced comparison report for insights\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configure logging\n",
        "    logging.getLogger(\"bertopic\").setLevel(logging.WARNING)\n",
        "    logging.getLogger(\"umap\").setLevel(logging.WARNING)\n",
        "    logging.getLogger(\"hdbscan\").setLevel(logging.WARNING)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" ENHANCED DOMAIN MODELING PIPELINE WITHOUT LLM\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"   • Embedding Models: {list(config.EMBEDDING_MODELS.values())}\")\n",
        "    print(f\"   • Enhanced LDA with proper topic extraction\")\n",
        "    print(f\"   • Enhanced domain classification with context\")\n",
        "    print(f\"   • Comprehensive comparison framework\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Execute enhanced pipeline\n",
        "    pipeline = EnhancedDomainModelingPipeline()\n",
        "    results = pipeline.run_enhanced_pipeline()\n",
        "\n",
        "    if results is not None:\n",
        "        print(f\"\\n\" + \"=\"*70)\n",
        "        print(\" ENHANCED PIPELINE EXECUTION COMPLETE\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\" Check the following directories for results:\")\n",
        "        print(f\"   1. {config.OUTPUT_DIR}/ - BERTopic results and visualizations\")\n",
        "        print(f\"   2. {config.COMPARISON_DIR}/ - LDA results and enhanced comparison\")\n",
        "        print(f\"   3. Review the enhanced comparison report for model selection guidance\")\n",
        "    else:\n",
        "        print(\"\\n Enhanced pipeline execution failed\")"
      ],
      "metadata": {
        "id": "-29XByj7yw2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class EnhancedVisualizationGenerator:\n",
        "    \"\"\"Enhanced visualization generator for topic modeling results\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.color_palette = plt.cm.Set3\n",
        "        self.domain_colors = {\n",
        "            'biology': '#2E86AB',\n",
        "            'medicine': '#A23B72',\n",
        "            'chemistry': '#F18F01',\n",
        "            'environmental_science': '#73AB84',\n",
        "            'computer_science': '#6D597A',\n",
        "            'physics': '#EF476F',\n",
        "            'engineering': '#118AB2',\n",
        "            'materials_science': '#06D6A0',\n",
        "            'psychology': '#FFD166',\n",
        "            'unknown': '#999999'\n",
        "        }\n",
        "\n",
        "    def generate_all_visualizations(self, results_df, topic_model, domain_mapping,\n",
        "                                  model_type=\"bertopic\", output_dir=config.OUTPUT_DIR):\n",
        "        \"\"\"Generate all visualizations for a model\"\"\"\n",
        "        print(f\"\\n Generating enhanced visualizations for {model_type}...\")\n",
        "\n",
        "        # Create visualization directory\n",
        "        vis_dir = f\"{output_dir}/visualizations/{model_type}\"\n",
        "        os.makedirs(vis_dir, exist_ok=True)\n",
        "\n",
        "        # 1. Topic Distribution\n",
        "        self._plot_topic_distribution(results_df, model_type, vis_dir)\n",
        "\n",
        "        # 2. Domain Distribution\n",
        "        self._plot_domain_distribution(results_df, model_type, vis_dir)\n",
        "\n",
        "        # 3. Topic Words Heatmap\n",
        "        if model_type == \"bertopic\":\n",
        "            self._plot_bertopic_heatmap(topic_model, results_df, vis_dir)\n",
        "        else:\n",
        "            self._plot_lda_topic_words(domain_mapping, vis_dir)\n",
        "\n",
        "        # 4. Topic Similarity Matrix\n",
        "        if model_type == \"bertopic\":\n",
        "            self._plot_topic_similarity(topic_model, vis_dir)\n",
        "\n",
        "        # 5. Document Embedding Visualization\n",
        "        if hasattr(topic_model, 'embeddings_') and model_type == \"bertopic\":\n",
        "            self._plot_document_embeddings(topic_model, results_df, vis_dir)\n",
        "\n",
        "        # 6. Topic Evolution/Over Time\n",
        "        if 'date' in results_df.columns:\n",
        "            self._plot_topic_evolution(results_df, vis_dir)\n",
        "\n",
        "        # 7. Interactive Visualizations\n",
        "        self._create_interactive_visualizations(results_df, domain_mapping, model_type, vis_dir)\n",
        "\n",
        "        # 8. Word Clouds\n",
        "        self._create_wordclouds(domain_mapping, model_type, vis_dir)\n",
        "\n",
        "        # 9. Confidence Distribution\n",
        "        self._plot_confidence_distribution(results_df, model_type, vis_dir)\n",
        "\n",
        "        print(f\" Visualizations saved to: {vis_dir}\")\n",
        "\n",
        "    def _plot_topic_distribution(self, results_df, model_type, vis_dir):\n",
        "        \"\"\"Plot topic distribution\"\"\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "        # Topic count distribution\n",
        "        topic_counts = results_df['topic'].value_counts()\n",
        "\n",
        "        if model_type == \"bertopic\":\n",
        "            # Separate outliers\n",
        "            if -1 in topic_counts.index:\n",
        "                outliers = topic_counts[-1]\n",
        "                topic_counts = topic_counts.drop(-1)\n",
        "                axes[0].bar(['Outliers'], [outliers], color='gray', alpha=0.7)\n",
        "\n",
        "        # Plot topic sizes\n",
        "        topics_sorted = topic_counts.sort_values(ascending=True)\n",
        "        colors = [self.color_palette(i/len(topics_sorted)) for i in range(len(topics_sorted))]\n",
        "\n",
        "        axes[0].barh(range(len(topics_sorted)), topics_sorted.values, color=colors, edgecolor='black')\n",
        "        axes[0].set_yticks(range(len(topics_sorted)))\n",
        "        axes[0].set_yticklabels([f\"Topic {int(idx)}\" for idx in topics_sorted.index])\n",
        "        axes[0].set_xlabel('Number of Documents')\n",
        "        axes[0].set_title(f'{model_type.upper()} Topic Distribution')\n",
        "        axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "        # Add document counts on bars\n",
        "        for i, v in enumerate(topics_sorted.values):\n",
        "            axes[0].text(v + max(topics_sorted.values)*0.01, i, str(v),\n",
        "                        va='center', fontsize=9)\n",
        "\n",
        "        # Topic size distribution histogram\n",
        "        sizes = topics_sorted.values\n",
        "        axes[1].hist(sizes, bins=min(20, len(sizes)), color='skyblue',\n",
        "                    edgecolor='black', alpha=0.7)\n",
        "        axes[1].axvline(np.mean(sizes), color='red', linestyle='--',\n",
        "                       label=f'Mean: {np.mean(sizes):.1f}')\n",
        "        axes[1].axvline(np.median(sizes), color='green', linestyle='--',\n",
        "                       label=f'Median: {np.median(sizes):.1f}')\n",
        "        axes[1].set_xlabel('Topic Size (Number of Documents)')\n",
        "        axes[1].set_ylabel('Frequency')\n",
        "        axes[1].set_title('Topic Size Distribution')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{vis_dir}/topic_distribution.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_domain_distribution(self, results_df, model_type, vis_dir):\n",
        "        \"\"\"Plot domain distribution\"\"\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "        # Domain count\n",
        "        domain_counts = results_df['domain'].value_counts()\n",
        "\n",
        "        # Prepare colors\n",
        "        domain_colors = [self.domain_colors.get(domain, '#999999')\n",
        "                        for domain in domain_counts.index]\n",
        "\n",
        "        # Pie chart\n",
        "        axes[0].pie(domain_counts.values, labels=domain_counts.index,\n",
        "                   colors=domain_colors, autopct='%1.1f%%', startangle=90)\n",
        "        axes[0].axis('equal')\n",
        "        axes[0].set_title(f'{model_type.upper()} Domain Distribution (Pie)')\n",
        "\n",
        "        # Bar chart\n",
        "        y_pos = np.arange(len(domain_counts))\n",
        "        axes[1].barh(y_pos, domain_counts.values, color=domain_colors, edgecolor='black')\n",
        "        axes[1].set_yticks(y_pos)\n",
        "        axes[1].set_yticklabels(domain_counts.index)\n",
        "        axes[1].set_xlabel('Number of Documents')\n",
        "        axes[1].set_title(f'{model_type.upper()} Domain Distribution (Bar)')\n",
        "        axes[1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "        # Add counts on bars\n",
        "        for i, v in enumerate(domain_counts.values):\n",
        "            axes[1].text(v + max(domain_counts.values)*0.01, i, str(v),\n",
        "                        va='center', fontsize=9)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{vis_dir}/domain_distribution.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Create detailed domain-topic matrix\n",
        "        if 'topic' in results_df.columns:\n",
        "            self._plot_domain_topic_matrix(results_df, model_type, vis_dir)\n",
        "\n",
        "    def _plot_domain_topic_matrix(self, results_df, model_type, vis_dir):\n",
        "        \"\"\"Plot domain-topic matrix\"\"\"\n",
        "        # Create cross-tabulation\n",
        "        cross_tab = pd.crosstab(results_df['domain'], results_df['topic'])\n",
        "\n",
        "        # Remove outliers if present\n",
        "        if -1 in cross_tab.columns:\n",
        "            cross_tab = cross_tab.drop(columns=[-1])\n",
        "\n",
        "        plt.figure(figsize=(max(10, cross_tab.shape[1]*0.8),\n",
        "                          max(6, cross_tab.shape[0]*0.6)))\n",
        "\n",
        "        sns.heatmap(cross_tab, annot=True, fmt='d', cmap='YlOrRd',\n",
        "                   cbar_kws={'label': 'Number of Documents'})\n",
        "        plt.title(f'{model_type.upper()} Domain-Topic Matrix')\n",
        "        plt.xlabel('Topic ID')\n",
        "        plt.ylabel('Domain')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{vis_dir}/domain_topic_matrix.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_bertopic_heatmap(self, topic_model, results_df, vis_dir):\n",
        "        \"\"\"Plot BERTopic heatmap\"\"\"\n",
        "        try:\n",
        "            # Get topic info\n",
        "            topic_info = topic_model.get_topic_info()\n",
        "            valid_topics = topic_info[topic_info['Topic'] != -1]\n",
        "\n",
        "            if len(valid_topics) < 2:\n",
        "                return\n",
        "\n",
        "            # Create similarity matrix\n",
        "            topics = valid_topics['Topic'].tolist()\n",
        "            topic_words = {}\n",
        "\n",
        "            for topic in topics:\n",
        "                words = topic_model.get_topic(topic)\n",
        "                if words:\n",
        "                    topic_words[topic] = [word for word, _ in words[:5]]\n",
        "\n",
        "            # Create figure\n",
        "            fig, ax = plt.subplots(figsize=(max(10, len(topics)), max(8, len(topics))))\n",
        "\n",
        "            # Create grid for displaying topic words\n",
        "            ax.set_xlim(0, len(topics))\n",
        "            ax.set_ylim(0, len(topics))\n",
        "            ax.invert_yaxis()\n",
        "\n",
        "            # Add topic labels\n",
        "            for i, topic in enumerate(topics):\n",
        "                ax.text(i + 0.5, -0.5, f\"Topic {topic}\",\n",
        "                       ha='center', va='top', rotation=45, fontsize=9)\n",
        "                ax.text(-0.5, i + 0.5, f\"Topic {topic}\",\n",
        "                       ha='right', va='center', fontsize=9)\n",
        "\n",
        "                # Add topic words in diagonal\n",
        "                if topic in topic_words:\n",
        "                    words_str = \"\\n\".join(topic_words[topic][:3])\n",
        "                    ax.text(i + 0.5, i + 0.5, words_str,\n",
        "                           ha='center', va='center', fontsize=8,\n",
        "                           bbox=dict(boxstyle=\"round,pad=0.3\",\n",
        "                                   facecolor='lightblue', alpha=0.7))\n",
        "\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "            ax.set_title('BERTopic Topics with Top Words')\n",
        "            ax.grid(False)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'{vis_dir}/topic_words_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Could not create BERTopic heatmap: {e}\")\n",
        "\n",
        "    def _plot_lda_topic_words(self, domain_mapping, vis_dir):\n",
        "        \"\"\"Plot LDA topic words\"\"\"\n",
        "        if not domain_mapping:\n",
        "            return\n",
        "\n",
        "        # Group topics by domain\n",
        "        domain_topics = {}\n",
        "        for topic_id, mapping in domain_mapping.items():\n",
        "            domain = mapping['primary_domain']\n",
        "            if domain not in domain_topics:\n",
        "                domain_topics[domain] = []\n",
        "            domain_topics[domain].append((topic_id, mapping['topic_keywords']))\n",
        "\n",
        "        # Create visualization\n",
        "        fig, axes = plt.subplots(len(domain_topics), 1,\n",
        "                               figsize=(12, 4 * len(domain_topics)))\n",
        "\n",
        "        if len(domain_topics) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for idx, (domain, topics) in enumerate(domain_topics.items()):\n",
        "            ax = axes[idx] if len(domain_topics) > 1 else axes\n",
        "\n",
        "            # Prepare data\n",
        "            topic_ids = [t[0] for t in topics]\n",
        "            topic_words = [', '.join(t[1][:4]) for t in topics]\n",
        "\n",
        "            # Create bar chart\n",
        "            y_pos = np.arange(len(topic_ids))\n",
        "            colors = [self.domain_colors.get(domain, '#999999')] * len(topic_ids)\n",
        "\n",
        "            bars = ax.barh(y_pos, [1] * len(topic_ids), color=colors, edgecolor='black')\n",
        "            ax.set_yticks(y_pos)\n",
        "            ax.set_yticklabels([f\"Topic {tid}\" for tid in topic_ids])\n",
        "            ax.set_xlim(0, 1.2)\n",
        "            ax.set_xlabel('')\n",
        "            ax.set_title(f'Domain: {domain} - Topics {len(topic_ids)}')\n",
        "            ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "            # Add topic words as text\n",
        "            for i, (bar, words) in enumerate(zip(bars, topic_words)):\n",
        "                width = bar.get_width()\n",
        "                ax.text(width + 0.02, bar.get_y() + bar.get_height()/2,\n",
        "                       words, va='center', fontsize=9)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{vis_dir}/lda_topic_words_by_domain.png',\n",
        "                   dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_topic_similarity(self, topic_model, vis_dir):\n",
        "        \"\"\"Plot topic similarity matrix\"\"\"\n",
        "        try:\n",
        "            # Get similarity matrix\n",
        "            similarity_matrix = topic_model.calculate_topic_similarity()\n",
        "\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            sns.heatmap(similarity_matrix, cmap='coolwarm', center=0,\n",
        "                       square=True, cbar_kws={'label': 'Similarity'})\n",
        "            plt.title('Topic Similarity Matrix')\n",
        "            plt.xlabel('Topic ID')\n",
        "            plt.ylabel('Topic ID')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'{vis_dir}/topic_similarity_matrix.png',\n",
        "                       dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Could not create similarity matrix: {e}\")\n",
        "\n",
        "    def _plot_document_embeddings(self, topic_model, results_df, vis_dir):\n",
        "        \"\"\"Plot document embeddings in 2D\"\"\"\n",
        "        try:\n",
        "            if not hasattr(topic_model, 'embeddings_') or topic_model.embeddings_ is None:\n",
        "                return\n",
        "\n",
        "            # Reduce to 2D using UMAP\n",
        "            umap_model = UMAP(n_components=2, random_state=42)\n",
        "            embeddings_2d = umap_model.fit_transform(topic_model.embeddings_)\n",
        "\n",
        "            # Create scatter plot\n",
        "            plt.figure(figsize=(12, 10))\n",
        "\n",
        "            # Color by domain if available\n",
        "            if 'domain' in results_df.columns:\n",
        "                unique_domains = results_df['domain'].unique()\n",
        "                color_map = {domain: self.domain_colors.get(domain, '#999999')\n",
        "                           for domain in unique_domains}\n",
        "\n",
        "                for domain in unique_domains:\n",
        "                    mask = results_df['domain'] == domain\n",
        "                    plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
        "                              color=color_map[domain], label=domain,\n",
        "                              alpha=0.6, s=20)\n",
        "\n",
        "                plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            else:\n",
        "                # Color by topic\n",
        "                plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
        "                          c=results_df['topic'], cmap='tab20',\n",
        "                          alpha=0.6, s=20)\n",
        "                plt.colorbar(label='Topic ID')\n",
        "\n",
        "            plt.title('Document Embeddings (2D UMAP projection)')\n",
        "            plt.xlabel('UMAP 1')\n",
        "            plt.ylabel('UMAP 2')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'{vis_dir}/document_embeddings.png',\n",
        "                       dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Could not create embeddings plot: {e}\")\n",
        "\n",
        "    def _plot_topic_evolution(self, results_df, vis_dir):\n",
        "        \"\"\"Plot topic evolution over time\"\"\"\n",
        "        try:\n",
        "            if 'date' not in results_df.columns:\n",
        "                return\n",
        "\n",
        "            # Convert date column\n",
        "            results_df['date'] = pd.to_datetime(results_df['date'], errors='coerce')\n",
        "            results_df = results_df.dropna(subset=['date'])\n",
        "\n",
        "            # Group by month and topic\n",
        "            results_df['year_month'] = results_df['date'].dt.to_period('M')\n",
        "\n",
        "            # Create pivot table\n",
        "            pivot = results_df.pivot_table(\n",
        "                index='year_month',\n",
        "                columns='topic',\n",
        "                values='cleaned_text',\n",
        "                aggfunc='count',\n",
        "                fill_value=0\n",
        "            )\n",
        "\n",
        "            # Remove outliers column if present\n",
        "            if -1 in pivot.columns:\n",
        "                pivot = pivot.drop(columns=[-1])\n",
        "\n",
        "            # Plot\n",
        "            plt.figure(figsize=(14, 8))\n",
        "            pivot.plot(kind='area', alpha=0.7, stacked=True, cmap='tab20', ax=plt.gca())\n",
        "            plt.title('Topic Evolution Over Time')\n",
        "            plt.xlabel('Time')\n",
        "            plt.ylabel('Number of Documents')\n",
        "            plt.legend(title='Topic ID', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'{vis_dir}/topic_evolution.png',\n",
        "                       dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Could not create topic evolution plot: {e}\")\n",
        "\n",
        "    def _create_interactive_visualizations(self, results_df, domain_mapping, model_type, vis_dir):\n",
        "        \"\"\"Create interactive visualizations using Plotly\"\"\"\n",
        "\n",
        "        # 1. Interactive topic-distribution bar chart\n",
        "        topic_counts = results_df['topic'].value_counts().reset_index()\n",
        "        topic_counts.columns = ['topic', 'count']\n",
        "\n",
        "        fig = px.bar(topic_counts, x='topic', y='count',\n",
        "                    title=f'{model_type.upper()} Topic Distribution',\n",
        "                    labels={'topic': 'Topic ID', 'count': 'Number of Documents'},\n",
        "                    color='count', color_continuous_scale='viridis')\n",
        "\n",
        "        fig.write_html(f'{vis_dir}/interactive_topic_distribution.html')\n",
        "\n",
        "        # 2. Interactive domain distribution\n",
        "        if 'domain' in results_df.columns:\n",
        "            domain_counts = results_df['domain'].value_counts().reset_index()\n",
        "            domain_counts.columns = ['domain', 'count']\n",
        "\n",
        "            fig = px.pie(domain_counts, values='count', names='domain',\n",
        "                        title=f'{model_type.upper()} Domain Distribution',\n",
        "                        color_discrete_map=self.domain_colors)\n",
        "\n",
        "            fig.write_html(f'{vis_dir}/interactive_domain_distribution.html')\n",
        "\n",
        "            # 3. Interactive scatter plot if embeddings available\n",
        "            if 'x' in results_df.columns and 'y' in results_df.columns:\n",
        "                fig = px.scatter(results_df, x='x', y='y', color='domain',\n",
        "                               hover_data=['topic', 'confidence'],\n",
        "                               title=f'{model_type.upper()} Document Clusters',\n",
        "                               color_discrete_map=self.domain_colors)\n",
        "\n",
        "                fig.write_html(f'{vis_dir}/interactive_document_clusters.html')\n",
        "\n",
        "    def _create_wordclouds(self, domain_mapping, model_type, vis_dir):\n",
        "        \"\"\"Create word clouds for each domain\"\"\"\n",
        "        try:\n",
        "            # Group topic keywords by domain\n",
        "            domain_keywords = {}\n",
        "\n",
        "            for topic_id, mapping in domain_mapping.items():\n",
        "                domain = mapping['primary_domain']\n",
        "                keywords = mapping.get('topic_keywords', [])\n",
        "\n",
        "                if domain not in domain_keywords:\n",
        "                    domain_keywords[domain] = []\n",
        "\n",
        "                domain_keywords[domain].extend(keywords)\n",
        "\n",
        "            # Create word cloud for each domain\n",
        "            for domain, keywords in domain_keywords.items():\n",
        "                if not keywords:\n",
        "                    continue\n",
        "\n",
        "                # Create frequency dictionary\n",
        "                freq_dict = Counter(keywords)\n",
        "\n",
        "                # Generate word cloud\n",
        "                wordcloud = WordCloud(\n",
        "                    width=800,\n",
        "                    height=400,\n",
        "                    background_color='white',\n",
        "                    colormap='tab20c',\n",
        "                    max_words=50\n",
        "                ).generate_from_frequencies(freq_dict)\n",
        "\n",
        "                # Plot\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                plt.imshow(wordcloud, interpolation='bilinear')\n",
        "                plt.title(f'{model_type.upper()} - Domain: {domain}', fontsize=16)\n",
        "                plt.axis('off')\n",
        "                plt.tight_layout()\n",
        "\n",
        "                # Save\n",
        "                safe_domain = domain.replace(' ', '_').lower()\n",
        "                plt.savefig(f'{vis_dir}/wordcloud_{safe_domain}.png',\n",
        "                           dpi=300, bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Could not create word clouds: {e}\")\n",
        "\n",
        "    def _plot_confidence_distribution(self, results_df, model_type, vis_dir):\n",
        "        \"\"\"Plot confidence score distribution\"\"\"\n",
        "        if 'confidence' not in results_df.columns:\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "        # Histogram of confidence scores\n",
        "        axes[0].hist(results_df['confidence'].dropna(), bins=20,\n",
        "                    color='lightcoral', edgecolor='black', alpha=0.7)\n",
        "        axes[0].axvline(results_df['confidence'].mean(), color='red',\n",
        "                       linestyle='--', label=f'Mean: {results_df[\"confidence\"].mean():.3f}')\n",
        "        axes[0].axvline(results_df['confidence'].median(), color='blue',\n",
        "                       linestyle='--', label=f'Median: {results_df[\"confidence\"].median():.3f}')\n",
        "        axes[0].set_xlabel('Confidence Score')\n",
        "        axes[0].set_ylabel('Frequency')\n",
        "        axes[0].set_title(f'{model_type.upper()} Confidence Distribution')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Box plot by domain\n",
        "        if 'domain' in results_df.columns:\n",
        "            data_to_plot = []\n",
        "            domains = []\n",
        "\n",
        "            for domain in results_df['domain'].unique():\n",
        "                domain_data = results_df[results_df['domain'] == domain]['confidence']\n",
        "                if len(domain_data) > 0:\n",
        "                    data_to_plot.append(domain_data)\n",
        "                    domains.append(domain)\n",
        "\n",
        "            if data_to_plot:\n",
        "                axes[1].boxplot(data_to_plot, labels=domains)\n",
        "                axes[1].set_xticklabels(domains, rotation=45, ha='right')\n",
        "                axes[1].set_ylabel('Confidence Score')\n",
        "                axes[1].set_title('Confidence by Domain')\n",
        "                axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{vis_dir}/confidence_distribution.png',\n",
        "                   dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def create_comparison_dashboard(self, bertopic_results, lda_results,\n",
        "                                  bertopic_df, lda_df, output_dir=config.COMPARISON_DIR):\n",
        "        \"\"\"Create comparison dashboard for both models\"\"\"\n",
        "        print(\"\\n Creating comparison dashboard...\")\n",
        "\n",
        "        vis_dir = f\"{output_dir}/comparison_visualizations\"\n",
        "        os.makedirs(vis_dir, exist_ok=True)\n",
        "\n",
        "        # 1. Side-by-side comparison\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "        # Topic count comparison\n",
        "        bert_topic_counts = bertopic_df['topic'].value_counts()\n",
        "        lda_topic_counts = lda_df['topic'].value_counts()\n",
        "\n",
        "        # Remove outliers\n",
        "        if -1 in bert_topic_counts.index:\n",
        "            bert_topic_counts = bert_topic_counts.drop(-1)\n",
        "\n",
        "        axes[0, 0].bar(['BERTopic', 'LDA'],\n",
        "                      [len(bert_topic_counts), len(lda_topic_counts)],\n",
        "                      color=['blue', 'red'], alpha=0.7)\n",
        "        axes[0, 0].set_ylabel('Number of Topics')\n",
        "        axes[0, 0].set_title('Topic Count Comparison')\n",
        "        axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "        # Add value labels\n",
        "        for i, v in enumerate([len(bert_topic_counts), len(lda_topic_counts)]):\n",
        "            axes[0, 0].text(i, v + 0.1, str(v), ha='center', va='bottom')\n",
        "\n",
        "        # Domain count comparison\n",
        "        bert_domain_counts = bertopic_df['domain'].nunique()\n",
        "        lda_domain_counts = lda_df['domain'].nunique()\n",
        "\n",
        "        axes[0, 1].bar(['BERTopic', 'LDA'],\n",
        "                      [bert_domain_counts, lda_domain_counts],\n",
        "                      color=['blue', 'red'], alpha=0.7)\n",
        "        axes[0, 1].set_ylabel('Number of Domains')\n",
        "        axes[0, 1].set_title('Domain Count Comparison')\n",
        "        axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "        # Add value labels\n",
        "        for i, v in enumerate([bert_domain_counts, lda_domain_counts]):\n",
        "            axes[0, 1].text(i, v + 0.1, str(v), ha='center', va='bottom')\n",
        "\n",
        "        # Average confidence comparison\n",
        "        bert_avg_conf = bertopic_df['confidence'].mean()\n",
        "        lda_avg_conf = lda_df['confidence'].mean()\n",
        "\n",
        "        axes[0, 2].bar(['BERTopic', 'LDA'],\n",
        "                      [bert_avg_conf, lda_avg_conf],\n",
        "                      color=['blue', 'red'], alpha=0.7)\n",
        "        axes[0, 2].set_ylabel('Average Confidence')\n",
        "        axes[0, 2].set_title('Confidence Comparison')\n",
        "        axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "        # Add value labels\n",
        "        for i, v in enumerate([bert_avg_conf, lda_avg_conf]):\n",
        "            axes[0, 2].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        # Topic size distribution comparison\n",
        "        bert_sizes = bert_topic_counts.values\n",
        "        lda_sizes = lda_topic_counts.values\n",
        "\n",
        "        axes[1, 0].hist([bert_sizes, lda_sizes], bins=15,\n",
        "                       label=['BERTopic', 'LDA'],\n",
        "                       color=['blue', 'red'], alpha=0.7)\n",
        "        axes[1, 0].set_xlabel('Topic Size')\n",
        "        axes[1, 0].set_ylabel('Frequency')\n",
        "        axes[1, 0].set_title('Topic Size Distribution Comparison')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Domain overlap analysis\n",
        "        bert_domains = set(bertopic_df['domain'].unique())\n",
        "        lda_domains = set(lda_df['domain'].unique())\n",
        "\n",
        "        overlap = bert_domains.intersection(lda_domains)\n",
        "        bert_only = bert_domains - lda_domains\n",
        "        lda_only = lda_domains - bert_domains\n",
        "\n",
        "        # Create Venn diagram-like visualization\n",
        "        axes[1, 1].text(0.3, 0.7, f\"BERTopic only:\\n{len(bert_only)}\",\n",
        "                       ha='center', va='center', fontsize=10,\n",
        "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='blue', alpha=0.3))\n",
        "        axes[1, 1].text(0.7, 0.7, f\"LDA only:\\n{len(lda_only)}\",\n",
        "                       ha='center', va='center', fontsize=10,\n",
        "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='red', alpha=0.3))\n",
        "        axes[1, 1].text(0.5, 0.3, f\"Overlap:\\n{len(overlap)}\",\n",
        "                       ha='center', va='center', fontsize=10,\n",
        "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='purple', alpha=0.3))\n",
        "        axes[1, 1].set_xlim(0, 1)\n",
        "        axes[1, 1].set_ylim(0, 1)\n",
        "        axes[1, 1].set_title('Domain Overlap Analysis')\n",
        "        axes[1, 1].axis('off')\n",
        "\n",
        "        # Performance metrics comparison\n",
        "        metrics = ['Coherence', 'Topic\\nDiversity', 'Distribution\\nScore']\n",
        "        bert_scores = [\n",
        "            bertopic_results['evaluation']['coherence_score'],\n",
        "            bertopic_results['evaluation']['topic_quality']['topic_diversity'],\n",
        "            bertopic_results['evaluation']['topic_quality']['topic_distribution_score']\n",
        "        ]\n",
        "        lda_scores = [\n",
        "            lda_results['evaluation']['coherence_score'],\n",
        "            lda_results['evaluation']['topic_quality']['topic_diversity'],\n",
        "            lda_results['evaluation']['topic_quality']['topic_distribution_score']\n",
        "        ]\n",
        "\n",
        "        x = np.arange(len(metrics))\n",
        "        width = 0.35\n",
        "\n",
        "        axes[1, 2].bar(x - width/2, bert_scores, width, label='BERTopic',\n",
        "                      color='blue', alpha=0.7)\n",
        "        axes[1, 2].bar(x + width/2, lda_scores, width, label='LDA',\n",
        "                      color='red', alpha=0.7)\n",
        "        axes[1, 2].set_ylabel('Score')\n",
        "        axes[1, 2].set_title('Quality Metrics Comparison')\n",
        "        axes[1, 2].set_xticks(x)\n",
        "        axes[1, 2].set_xticklabels(metrics)\n",
        "        axes[1, 2].legend()\n",
        "        axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{vis_dir}/model_comparison_dashboard.png',\n",
        "                   dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # 2. Interactive comparison dashboard\n",
        "        self._create_interactive_comparison_dashboard(\n",
        "            bertopic_results, lda_results, bertopic_df, lda_df, vis_dir\n",
        "        )\n",
        "\n",
        "        print(f\" Comparison dashboard saved to: {vis_dir}\")\n",
        "\n",
        "    def _create_interactive_comparison_dashboard(self, bertopic_results, lda_results,\n",
        "                                                bertopic_df, lda_df, vis_dir):\n",
        "        \"\"\"Create interactive comparison dashboard\"\"\"\n",
        "\n",
        "        # Create DataFrame for metrics comparison\n",
        "        metrics_data = {\n",
        "            'Metric': ['Topics Found', 'Domains Covered', 'Avg Confidence',\n",
        "                      'Coherence Score', 'Topic Diversity', 'Distribution Score',\n",
        "                      'Outliers (%)' if -1 in bertopic_df['topic'].values else 'N/A'],\n",
        "            'BERTopic': [\n",
        "                len(bertopic_df['topic'].unique()) - (1 if -1 in bertopic_df['topic'].values else 0),\n",
        "                bertopic_df['domain'].nunique(),\n",
        "                bertopic_df['confidence'].mean(),\n",
        "                bertopic_results['evaluation']['coherence_score'],\n",
        "                bertopic_results['evaluation']['topic_quality']['topic_diversity'],\n",
        "                bertopic_results['evaluation']['topic_quality']['topic_distribution_score'],\n",
        "                (bertopic_df['topic'] == -1).mean() * 100 if -1 in bertopic_df['topic'].values else 0\n",
        "            ],\n",
        "            'LDA': [\n",
        "                len(lda_df['topic'].unique()),\n",
        "                lda_df['domain'].nunique(),\n",
        "                lda_df['confidence'].mean(),\n",
        "                lda_results['evaluation']['coherence_score'],\n",
        "                lda_results['evaluation']['topic_quality']['topic_diversity'],\n",
        "                lda_results['evaluation']['topic_quality']['topic_distribution_score'],\n",
        "                0  # LDA has no outliers\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        df_metrics = pd.DataFrame(metrics_data)\n",
        "\n",
        "        # Create interactive bar chart\n",
        "        fig = go.Figure()\n",
        "\n",
        "        fig.add_trace(go.Bar(\n",
        "            x=df_metrics['Metric'],\n",
        "            y=df_metrics['BERTopic'],\n",
        "            name='BERTopic',\n",
        "            marker_color='blue',\n",
        "            text=df_metrics['BERTopic'].round(3),\n",
        "            textposition='auto',\n",
        "        ))\n",
        "\n",
        "        fig.add_trace(go.Bar(\n",
        "            x=df_metrics['Metric'],\n",
        "            y=df_metrics['LDA'],\n",
        "            name='LDA',\n",
        "            marker_color='red',\n",
        "            text=df_metrics['LDA'].round(3),\n",
        "            textposition='auto',\n",
        "        ))\n",
        "\n",
        "        fig.update_layout(\n",
        "            title='Model Comparison Dashboard',\n",
        "            xaxis_tickangle=-45,\n",
        "            barmode='group',\n",
        "            yaxis_title='Score/Value',\n",
        "            height=600,\n",
        "            showlegend=True\n",
        "        )\n",
        "\n",
        "        fig.write_html(f'{vis_dir}/interactive_comparison_dashboard.html')\n",
        "\n",
        "        # Create radar chart comparison\n",
        "        fig = go.Figure()\n",
        "\n",
        "        metrics_for_radar = ['Coherence Score', 'Topic Diversity', 'Distribution Score',\n",
        "                           'Avg Confidence', 'Domain Coverage']\n",
        "\n",
        "        bert_values = [\n",
        "            bertopic_results['evaluation']['coherence_score'],\n",
        "            bertopic_results['evaluation']['topic_quality']['topic_diversity'],\n",
        "            bertopic_results['evaluation']['topic_quality']['topic_distribution_score'],\n",
        "            bertopic_df['confidence'].mean(),\n",
        "            min(bertopic_df['domain'].nunique() / 8, 1.0)  # Normalized\n",
        "        ]\n",
        "\n",
        "        lda_values = [\n",
        "            lda_results['evaluation']['coherence_score'],\n",
        "            lda_results['evaluation']['topic_quality']['topic_diversity'],\n",
        "            lda_results['evaluation']['topic_quality']['topic_distribution_score'],\n",
        "            lda_df['confidence'].mean(),\n",
        "            min(lda_df['domain'].nunique() / 8, 1.0)  # Normalized\n",
        "        ]\n",
        "\n",
        "        fig.add_trace(go.Scatterpolar(\n",
        "            r=bert_values,\n",
        "            theta=metrics_for_radar,\n",
        "            fill='toself',\n",
        "            name='BERTopic',\n",
        "            line_color='blue'\n",
        "        ))\n",
        "\n",
        "        fig.add_trace(go.Scatterpolar(\n",
        "            r=lda_values,\n",
        "            theta=metrics_for_radar,\n",
        "            fill='toself',\n",
        "            name='LDA',\n",
        "            line_color='red'\n",
        "        ))\n",
        "\n",
        "        fig.update_layout(\n",
        "            polar=dict(\n",
        "                radialaxis=dict(\n",
        "                    visible=True,\n",
        "                    range=[0, 1]\n",
        "                )),\n",
        "            showlegend=True,\n",
        "            title='Quality Metrics Radar Comparison'\n",
        "        )\n",
        "\n",
        "        fig.write_html(f'{vis_dir}/quality_metrics_radar.html')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Add this method to your EnhancedDomainModelingPipeline class:\n",
        "\n",
        "def generate_all_visualizations_in_pipeline(self):\n",
        "    \"\"\"Generate all visualizations in the pipeline\"\"\"\n",
        "    print(\"\\n STEP 9: GENERATING ENHANCED VISUALIZATIONS\")\n",
        "\n",
        "    # Initialize visualization generator\n",
        "    vis_generator = EnhancedVisualizationGenerator()\n",
        "\n",
        "    # Load results\n",
        "    bert_df = pd.read_csv(f\"{config.OUTPUT_DIR}/enhanced_bertopic_assignments.csv\")\n",
        "    lda_df = pd.read_csv(f\"{config.COMPARISON_DIR}/enhanced_lda_assignments.csv\")\n",
        "\n",
        "    # Load evaluations\n",
        "    with open(f\"{config.OUTPUT_DIR}/enhanced_evaluations.json\", 'r') as f:\n",
        "        evaluations = json.load(f)\n",
        "\n",
        "    # Load domain mappings\n",
        "    bert_domain_mapping = {}\n",
        "    lda_domain_mapping = {}\n",
        "\n",
        "    # Reconstruct domain mappings from saved data\n",
        "    for _, row in bert_df.iterrows():\n",
        "        if row['topic'] not in bert_domain_mapping:\n",
        "            bert_domain_mapping[row['topic']] = {\n",
        "                'primary_domain': row['domain'],\n",
        "                'confidence': row['confidence'],\n",
        "                'topic_keywords': []  # Would need to load from topic model\n",
        "            }\n",
        "\n",
        "    for _, row in lda_df.iterrows():\n",
        "        if row['topic'] not in lda_domain_mapping:\n",
        "            lda_domain_mapping[row['topic']] = {\n",
        "                'primary_domain': row['domain'],\n",
        "                'confidence': row['confidence'],\n",
        "                'topic_keywords': []  # Would need to load from LDA topics\n",
        "            }\n",
        "\n",
        "    # Generate BERTopic visualizations\n",
        "    print(\" Generating BERTopic visualizations...\")\n",
        "    vis_generator.generate_all_visualizations(\n",
        "        bert_df,\n",
        "        self.topic_modeler.topic_model,\n",
        "        bert_domain_mapping,\n",
        "        model_type=\"bertopic\",\n",
        "        output_dir=config.OUTPUT_DIR\n",
        "    )\n",
        "\n",
        "    # Generate LDA visualizations\n",
        "    print(\" Generating LDA visualizations...\")\n",
        "    vis_generator.generate_all_visualizations(\n",
        "        lda_df,\n",
        "        self.fixed_lda,\n",
        "        lda_domain_mapping,\n",
        "        model_type=\"lda\",\n",
        "        output_dir=config.COMPARISON_DIR\n",
        "    )\n",
        "\n",
        "    # Generate comparison dashboard\n",
        "    print(\" Generating comparison dashboard...\")\n",
        "    vis_generator.create_comparison_dashboard(\n",
        "        bertopic_results={\n",
        "            'evaluation': evaluations['bertopic'],\n",
        "            'domain_mapping': bert_domain_mapping\n",
        "        },\n",
        "        lda_results={\n",
        "            'evaluation': evaluations['lda'],\n",
        "            'domain_mapping': lda_domain_mapping\n",
        "        },\n",
        "        bertopic_df=bert_df,\n",
        "        lda_df=lda_df,\n",
        "        output_dir=config.COMPARISON_DIR\n",
        "    )\n",
        "\n",
        "    print(\" Enhanced visualizations generated successfully!\")\n",
        "\n",
        "\n",
        "\n",
        "# Update the __main__ section to include visualizations:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configure logging\n",
        "    logging.getLogger(\"bertopic\").setLevel(logging.WARNING)\n",
        "    logging.getLogger(\"umap\").setLevel(logging.WARNING)\n",
        "    logging.getLogger(\"hdbscan\").setLevel(logging.WARNING)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" ENHANCED DOMAIN MODELING PIPELINE WITHOUT LLM\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"   • Embedding Models: {list(config.EMBEDDING_MODELS.values())}\")\n",
        "    print(f\"   • Enhanced LDA with proper topic extraction\")\n",
        "    print(f\"   • Enhanced domain classification with context\")\n",
        "    print(f\"   • Comprehensive comparison framework\")\n",
        "    print(f\"   • Enhanced visualizations and dashboards\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Execute enhanced pipeline\n",
        "    pipeline = EnhancedDomainModelingPipeline()\n",
        "    results = pipeline.run_enhanced_pipeline()\n",
        "\n",
        "    # Add visualization step\n",
        "    if results is not None:\n",
        "        # Generate visualizations\n",
        "        try:\n",
        "            # Create visualization generator\n",
        "            vis_generator = EnhancedVisualizationGenerator()\n",
        "\n",
        "            # Generate visualizations for both models\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\" GENERATING ENHANCED VISUALIZATIONS\")\n",
        "            print(\"=\"*70)\n",
        "\n",
        "            # Prepare dataframes\n",
        "            bert_df = pd.DataFrame({\n",
        "                'topic': results['bertopic']['topics'],\n",
        "                'domain': [results['bertopic']['domain_mapping'].get(t, {}).get('primary_domain', 'unknown')\n",
        "                          for t in results['bertopic']['topics']],\n",
        "                'confidence': [results['bertopic']['domain_mapping'].get(t, {}).get('confidence', 0.0)\n",
        "                             for t in results['bertopic']['topics']],\n",
        "                'cleaned_text': results['documents']\n",
        "            })\n",
        "\n",
        "            lda_df = pd.DataFrame({\n",
        "                'topic': results['lda']['topics'],\n",
        "                'domain': [results['lda']['domain_mapping'].get(t, {}).get('primary_domain', 'unknown')\n",
        "                          for t in results['lda']['topics']],\n",
        "                'confidence': [results['lda']['domain_mapping'].get(t, {}).get('confidence', 0.0)\n",
        "                             for t in results['lda']['topics']],\n",
        "                'cleaned_text': results['documents']\n",
        "            })\n",
        "\n",
        "            # Generate BERTopic visualizations\n",
        "            print(\" Generating BERTopic visualizations...\")\n",
        "            vis_generator.generate_all_visualizations(\n",
        "                bert_df,\n",
        "                results['bertopic']['model'],\n",
        "                results['bertopic']['domain_mapping'],\n",
        "                model_type=\"bertopic\",\n",
        "                output_dir=config.OUTPUT_DIR\n",
        "            )\n",
        "\n",
        "            # Generate LDA visualizations\n",
        "            print(\" Generating LDA visualizations...\")\n",
        "            vis_generator.generate_all_visualizations(\n",
        "                lda_df,\n",
        "                results['lda']['topic_model'],\n",
        "                results['lda']['domain_mapping'],\n",
        "                model_type=\"lda\",\n",
        "                output_dir=config.COMPARISON_DIR\n",
        "            )\n",
        "\n",
        "            # Generate comparison dashboard\n",
        "            print(\" Generating comparison dashboard...\")\n",
        "            vis_generator.create_comparison_dashboard(\n",
        "                results['bertopic'],\n",
        "                results['lda'],\n",
        "                bert_df,\n",
        "                lda_df,\n",
        "                config.COMPARISON_DIR\n",
        "            )\n",
        "\n",
        "            print(\"\\n Enhanced visualizations generated successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Visualization generation failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        print(f\"\\n\" + \"=\"*70)\n",
        "        print(\" ENHANCED PIPELINE EXECUTION COMPLETE\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\" Check the following directories for results:\")\n",
        "        print(f\"   1. {config.OUTPUT_DIR}/ - BERTopic results and visualizations\")\n",
        "        print(f\"   2. {config.COMPARISON_DIR}/ - LDA results and enhanced comparison\")\n",
        "        print(f\"   3. Review the enhanced comparison report for model selection guidance\")\n",
        "        print(f\"\\n Visualization directories:\")\n",
        "        print(f\"   • {config.OUTPUT_DIR}/visualizations/bertopic/ - BERTopic visualizations\")\n",
        "        print(f\"   • {config.COMPARISON_DIR}/visualizations/lda/ - LDA visualizations\")\n",
        "        print(f\"   • {config.COMPARISON_DIR}/comparison_visualizations/ - Comparison dashboards\")\n",
        "    else:\n",
        "        print(\"\\n Enhanced pipeline execution failed\")"
      ],
      "metadata": {
        "id": "CIOJiGy9ZMvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKCcfkJ8C8JB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "output_dir = \"/content/domain_modeling_results\"\n",
        "zip_path = f\"{output_dir}.zip\"\n",
        "\n",
        "# Zip the directory\n",
        "!zip -r \"$zip_path\" \"$output_dir\"\n",
        "\n",
        "# Download the zip file\n",
        "if os.path.exists(zip_path):\n",
        "  files.download(zip_path)\n",
        "else:\n",
        "  print(f\"Zip file not found at {zip_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7LwXsa-wfWS"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nfquejEwilD"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PHASE 2: MULTI-DIMENSIONAL DATA CITATION ANALYSIS - UPDATED WITH FILTERING\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, AutoModelForTokenClassification,\n",
        "    AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        ")\n",
        "import spacy\n",
        "import re\n",
        "import json\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "from typing import List, Dict, Tuple, Any\n",
        "import warnings\n",
        "import os\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Phase 2 imports completed successfully!\")\n",
        "\n",
        "# =============================================================================\n",
        "# PHASE 2 CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "class Phase2Config:\n",
        "    # File paths\n",
        "    PROCESSED_TEXTS_CSV = \"updated.csv\"\n",
        "    TRAIN_LABELS_CSV = \"train_labels.csv\"\n",
        "    OUTPUT_DIR = \"phase2_data_citation_analysis\"\n",
        "\n",
        "    # Model configurations\n",
        "    NER_MODEL_NAME = \"allenai/scibert_scivocab_uncased\"\n",
        "    CLASSIFICATION_MODEL_NAME = \"roberta-base\"\n",
        "    SPACY_MODEL = \"en_core_web_sm\"\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 16\n",
        "    LEARNING_RATE = 2e-5\n",
        "    NUM_EPOCHS = 4\n",
        "    MAX_LENGTH = 256\n",
        "\n",
        "    # NER parameters\n",
        "    CONTEXT_WINDOW = 150\n",
        "\n",
        "    # Usage type classes\n",
        "    USAGE_TYPES = [\"primary\", \"secondary\", \"missing\"]\n",
        "    PURPOSE_TYPES = [\"training\", \"validation\", \"testing\", \"evaluation\", \"benchmarking\"]\n",
        "\n",
        "    # Dataset patterns\n",
        "    DATASET_PATTERNS = [\n",
        "        r'\\bdataset[s]?\\b',\n",
        "        r'\\bdata\\s+set[s]?\\b',\n",
        "        r'\\bcorpus\\b',\n",
        "        r'\\bcollection\\b',\n",
        "        r'\\barchive\\b',\n",
        "        r'\\brepository\\b',\n",
        "        r'\\bbenchmark\\b',\n",
        "        r'\\bDB\\d+\\b',\n",
        "        r'\\b[A-Z]{2,}\\d*\\s+dataset\\b',\n",
        "    ]\n",
        "\n",
        "    def __init__(self):\n",
        "        os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "config = Phase2Config()\n",
        "print(f\" Phase 2 configuration initialized\")\n",
        "\n",
        "# =============================================================================\n",
        "# ENHANCED DATA LOADER WITH FILE TYPE FILTERING\n",
        "# =============================================================================\n",
        "\n",
        "class FilteredDataCitationDataLoader:\n",
        "    def __init__(self):\n",
        "        self.processed_texts_df = None\n",
        "        self.train_labels_df = None\n",
        "        self.merged_data = None\n",
        "        self.original_label_distribution = None\n",
        "\n",
        "    def load_and_validate_data(self):\n",
        "        \"\"\"Load and validate both data files with file type filtering and proper filename cleaning\"\"\"\n",
        "        print(\" Loading and validating Phase 2 data...\")\n",
        "\n",
        "        try:\n",
        "            # Load processed texts\n",
        "            self.processed_texts_df = pd.read_csv(config.PROCESSED_TEXTS_CSV)\n",
        "            print(f\" Loaded {len(self.processed_texts_df)} documents from {config.PROCESSED_TEXTS_CSV}\")\n",
        "\n",
        "            # Load training labels\n",
        "            self.train_labels_df = pd.read_csv(config.TRAIN_LABELS_CSV)\n",
        "            print(f\" Loaded {len(self.train_labels_df)} labels from {config.TRAIN_LABELS_CSV}\")\n",
        "\n",
        "            # Store original label distribution for comparison\n",
        "            self.original_label_distribution = self.train_labels_df['type'].value_counts().to_dict()\n",
        "            print(f\" Original label distribution in train_labels: {self.original_label_distribution}\")\n",
        "\n",
        "            # Step 1: Filter by file_type (keep only PDF and XML)\n",
        "            self._filter_by_file_type()\n",
        "\n",
        "            # Step 2: Clean filenames in both datasets\n",
        "            self._clean_filenames()\n",
        "\n",
        "            # Step 3: Merge datasets\n",
        "            self._merge_datasets()\n",
        "\n",
        "            print(f\" Final dataset distribution:\")\n",
        "            final_distribution = self.merged_data['type'].value_counts().to_dict()\n",
        "            print(f\"   - Usage types: {final_distribution}\")\n",
        "\n",
        "            # Compare distributions\n",
        "            self._compare_label_distributions()\n",
        "\n",
        "            return self.merged_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Data loading failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            # Fallback: use processed texts only\n",
        "            print(\" Fallback: Using processed texts only\")\n",
        "            self.merged_data = self.processed_texts_df.copy()\n",
        "            self.merged_data['type'] = 'primary'  # Default type\n",
        "            self.merged_data['label_source'] = 'fallback'\n",
        "            return self.merged_data\n",
        "\n",
        "    def _filter_by_file_type(self):\n",
        "        \"\"\"Filter processed texts to keep only PDF and XML files\"\"\"\n",
        "        print(\"\\n FILTERING BY FILE TYPE:\")\n",
        "\n",
        "        # Check if file_type column exists\n",
        "        if 'file_type' not in self.processed_texts_df.columns:\n",
        "            print(\" 'file_type' column not found in processed_combined_texts.csv\")\n",
        "            print(\" Proceeding without file type filtering\")\n",
        "            return\n",
        "\n",
        "        # Get initial counts\n",
        "        initial_count = len(self.processed_texts_df)\n",
        "        file_type_counts = self.processed_texts_df['file_type'].value_counts().to_dict()\n",
        "        print(f\"   • Initial file type distribution: {file_type_counts}\")\n",
        "\n",
        "        # Filter to keep only PDF and XML files\n",
        "        valid_file_types = ['pdf', 'xml', 'PDF', 'XML']\n",
        "        self.processed_texts_df = self.processed_texts_df[\n",
        "            self.processed_texts_df['file_type'].isin(valid_file_types)\n",
        "        ]\n",
        "\n",
        "        # Get filtered counts\n",
        "        filtered_count = len(self.processed_texts_df)\n",
        "        filtered_file_type_counts = self.processed_texts_df['file_type'].value_counts().to_dict()\n",
        "\n",
        "        print(f\"   • After filtering (PDF/XML only): {filtered_file_type_counts}\")\n",
        "        print(f\"   • Removed {initial_count - filtered_count} documents, kept {filtered_count} documents\")\n",
        "\n",
        "    def _clean_filenames(self):\n",
        "        \"\"\"Clean filenames by removing extensions and standardizing format\"\"\"\n",
        "        print(\"\\n CLEANING FILENAMES:\")\n",
        "\n",
        "        # Clean processed texts filenames (remove .pdf, .xml extensions)\n",
        "        self.processed_texts_df['filename_clean'] = self.processed_texts_df['filename'].apply(\n",
        "            lambda x: self._remove_file_extensions(str(x))\n",
        "        )\n",
        "\n",
        "        # Clean train labels filenames (already clean, but standardize)\n",
        "        self.train_labels_df['filename_clean'] = self.train_labels_df['filename'].apply(\n",
        "            lambda x: str(x).strip().lower()\n",
        "        )\n",
        "\n",
        "        print(f\"   • Processed texts sample (cleaned): {self.processed_texts_df['filename_clean'].head(3).tolist()}\")\n",
        "        print(f\"   • Train labels sample (cleaned): {self.train_labels_df['filename_clean'].head(3).tolist()}\")\n",
        "\n",
        "    def _remove_file_extensions(self, filename):\n",
        "        \"\"\"Remove common file extensions from filename\"\"\"\n",
        "        # Remove .pdf, .xml, .txt, etc. and convert to lowercase\n",
        "        cleaned = filename.lower().strip()\n",
        "        # Remove common extensions\n",
        "        extensions = ['.pdf', '.xml', '.txt', '.csv', '.json', '.html']\n",
        "        for ext in extensions:\n",
        "            if cleaned.endswith(ext):\n",
        "                cleaned = cleaned[:-len(ext)]\n",
        "                break  # Remove only one extension\n",
        "        return cleaned\n",
        "\n",
        "    def _merge_datasets(self):\n",
        "        \"\"\"Merge datasets on cleaned filenames\"\"\"\n",
        "        print(\"\\n MERGING DATASETS:\")\n",
        "\n",
        "        # Find common filenames\n",
        "        common_filenames = set(self.processed_texts_df['filename_clean']).intersection(\n",
        "            set(self.train_labels_df['filename_clean'])\n",
        "        )\n",
        "\n",
        "        print(f\"   • Common filenames found: {len(common_filenames)}\")\n",
        "\n",
        "        if len(common_filenames) > 0:\n",
        "            # Merge on cleaned filenames\n",
        "            self.merged_data = pd.merge(\n",
        "                self.processed_texts_df,\n",
        "                self.train_labels_df,\n",
        "                left_on='filename_clean',\n",
        "                right_on='filename_clean',\n",
        "                how='inner',\n",
        "                suffixes=('_text', '_label')\n",
        "            )\n",
        "            self.merged_data['label_source'] = 'original'\n",
        "            print(f\" Successfully merged {len(self.merged_data)} documents with original labels\")\n",
        "\n",
        "            # Show sample of matched filenames\n",
        "            print(f\" Sample matched filenames (first 5):\")\n",
        "            for f in list(common_filenames)[:5]:\n",
        "                print(f\"   - {f}\")\n",
        "        else:\n",
        "            print(\" No common filenames found after cleaning. Using enhanced inference.\")\n",
        "            # Use processed texts with enhanced synthetic labels\n",
        "            self.merged_data = self.processed_texts_df.copy()\n",
        "            # Add enhanced synthetic usage types\n",
        "            self.merged_data['type'] = self.merged_data['processed_text'].apply(\n",
        "                lambda x: self._enhanced_infer_usage_type(x) if pd.notna(x) else 'unknown'\n",
        "            )\n",
        "            self.merged_data['label_source'] = 'inferred'\n",
        "            print(f\" Using enhanced inference: {len(self.merged_data)} documents\")\n",
        "\n",
        "    def _enhanced_infer_usage_type(self, text):\n",
        "        \"\"\"Enhanced usage type inference with better patterns\"\"\"\n",
        "        if not isinstance(text, str) or len(text.strip()) < 50:\n",
        "            return 'unknown'\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Enhanced heuristic-based inference\n",
        "        primary_indicators = [\n",
        "            'our dataset', 'we collected', 'our collection', 'we compiled',\n",
        "            'collected by us', 'gathered by the authors', 'original dataset',\n",
        "            'new dataset', 'novel dataset', 'this work introduces', 'we created',\n",
        "            'developed by us', 'constructed by the authors', 'custom dataset'\n",
        "        ]\n",
        "\n",
        "        secondary_indicators = [\n",
        "            'existing dataset', 'previous work', 'benchmark dataset',\n",
        "            'standard dataset', 'publicly available', 'downloaded from',\n",
        "            'obtained from', 'provided by', 'well-known dataset', 'established dataset',\n",
        "            'widely used', 'popular dataset', 'reference dataset', 'public dataset'\n",
        "        ]\n",
        "\n",
        "        missing_indicators = [\n",
        "            'no dataset', 'without data', 'data not available', 'lack of data',\n",
        "            'unavailable data', 'restricted access', 'proprietary data',\n",
        "            'confidential data', 'cannot share', 'not publicly available'\n",
        "        ]\n",
        "\n",
        "        # Check for primary indicators (highest priority)\n",
        "        primary_score = sum(1 for indicator in primary_indicators if indicator in text_lower)\n",
        "\n",
        "        # Check for secondary indicators\n",
        "        secondary_score = sum(1 for indicator in secondary_indicators if indicator in text_lower)\n",
        "\n",
        "        # Check for missing indicators\n",
        "        missing_score = sum(1 for indicator in missing_indicators if indicator in text_lower)\n",
        "\n",
        "        # Determine the type based on scores\n",
        "        if primary_score > 0 and primary_score >= secondary_score:\n",
        "            return 'primary'\n",
        "        elif secondary_score > 0 and secondary_score > primary_score:\n",
        "            return 'secondary'\n",
        "        elif missing_score > 2:  # Need multiple indicators for missing\n",
        "            return 'missing'\n",
        "        else:\n",
        "            return 'unknown'\n",
        "\n",
        "    def _compare_label_distributions(self):\n",
        "        \"\"\"Compare original and final label distributions\"\"\"\n",
        "        if self.original_label_distribution and 'label_source' in self.merged_data.columns:\n",
        "            original_labels = self.merged_data[self.merged_data['label_source'] == 'original']['type'].value_counts().to_dict()\n",
        "            inferred_labels = self.merged_data[self.merged_data['label_source'] == 'inferred']['type'].value_counts().to_dict() if 'inferred' in self.merged_data['label_source'].values else {}\n",
        "\n",
        "            print(f\"\\n LABEL DISTRIBUTION COMPARISON:\")\n",
        "            print(f\"   Original labels in train_labels.csv: {self.original_label_distribution}\")\n",
        "            if original_labels:\n",
        "                print(f\"   After merge - Original labels used: {original_labels}\")\n",
        "            if inferred_labels:\n",
        "                print(f\"   After merge - Inferred labels: {inferred_labels}\")\n",
        "\n",
        "    def get_texts_for_analysis(self):\n",
        "        \"\"\"Get processed texts for analysis with fallback\"\"\"\n",
        "        if self.merged_data is None:\n",
        "            self.load_and_validate_data()\n",
        "\n",
        "        # Use processed_text if available, otherwise use other text columns\n",
        "        text_columns = ['processed_text', 'final_cleaned_text', 'raw_text']\n",
        "        for col in text_columns:\n",
        "            if col in self.merged_data.columns:\n",
        "                texts = self.merged_data[col].fillna('').tolist()\n",
        "                break\n",
        "        else:\n",
        "            # If no text columns found, create empty list\n",
        "            texts = [''] * len(self.merged_data)\n",
        "\n",
        "        # Use the cleaned filename for consistency\n",
        "        filenames = self.merged_data['filename_clean'].tolist()\n",
        "        usage_types = self.merged_data['type'].tolist()\n",
        "        label_sources = self.merged_data['label_source'].tolist()\n",
        "\n",
        "        print(f\" Texts for analysis: {len(texts)} documents\")\n",
        "        print(f\" Label sources: {Counter(label_sources)}\")\n",
        "        print(f\" Text length stats: avg={np.mean([len(str(t)) for t in texts]):.0f}, \"\n",
        "              f\"min={np.min([len(str(t)) for t in texts])}, max={np.max([len(str(t)) for t in texts])}\")\n",
        "\n",
        "        return texts, filenames, usage_types, label_sources\n",
        "\n",
        "\n",
        "class HybridDatasetNER:\n",
        "    def __init__(self):\n",
        "        self.rule_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in config.DATASET_PATTERNS]\n",
        "        self.spacy_nlp = None\n",
        "        self.initialize_models()\n",
        "\n",
        "    def initialize_models(self):\n",
        "        \"\"\"Initialize models with error handling\"\"\"\n",
        "        try:\n",
        "            self.spacy_nlp = spacy.load(config.SPACY_MODEL)\n",
        "            print(f\" spaCy model loaded: {config.SPACY_MODEL}\")\n",
        "        except OSError:\n",
        "            print(f\" spaCy model not found. Using rule-based only.\")\n",
        "            self.spacy_nlp = None\n",
        "\n",
        "    def rule_based_dataset_detection(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Enhanced rule-based dataset mention detection\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return []\n",
        "\n",
        "        mentions = []\n",
        "\n",
        "        # Basic pattern matching\n",
        "        for pattern in self.rule_patterns:\n",
        "            for match in pattern.finditer(text):\n",
        "                start, end = match.span()\n",
        "                mentions.append({\n",
        "                    'text': text[start:end],\n",
        "                    'start': start,\n",
        "                    'end': end,\n",
        "                    'type': 'DATASET',\n",
        "                    'source': 'rule_based',\n",
        "                    'confidence': 0.7\n",
        "                })\n",
        "\n",
        "        # Enhanced contextual patterns\n",
        "        enhanced_patterns = [\n",
        "            (r'\\b(using|using the|using our|employed|utilized)\\s+([A-Za-z0-9\\s\\-]+?\\s+(?:dataset|corpus|collection))', 2),\n",
        "            (r'\\b(dataset|corpus|collection|benchmark)\\s+of\\s+([A-Z][A-Za-z0-9\\s\\-]+)', 0),\n",
        "            (r'\\b([A-Z]{2,}(?:\\-\\d+)?\\s+(?:dataset|corpus))', 0),\n",
        "            (r'\\b(ImageNet|CIFAR|MNIST|COCO|SQuAD|GLUE|UCI)\\b', 0),  # Common dataset names\n",
        "        ]\n",
        "\n",
        "        for pattern, group_idx in enhanced_patterns:\n",
        "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
        "                mention_text = match.group(group_idx) if group_idx > 0 else match.group(0)\n",
        "                if mention_text:\n",
        "                    mentions.append({\n",
        "                        'text': mention_text.strip(),\n",
        "                        'start': match.start(),\n",
        "                        'end': match.end(),\n",
        "                        'type': 'DATASET_MENTION',\n",
        "                        'source': 'enhanced_rule',\n",
        "                        'confidence': 0.8\n",
        "                    })\n",
        "\n",
        "        return mentions\n",
        "\n",
        "    def detect_dataset_mentions(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Main method to detect dataset mentions\"\"\"\n",
        "        if not text or len(text.strip()) < 20:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Get rule-based mentions\n",
        "            rule_mentions = self.rule_based_dataset_detection(text)\n",
        "\n",
        "            # Apply additional filtering and confidence scoring\n",
        "            filtered_mentions = []\n",
        "            for mention in rule_mentions:\n",
        "                # Enhance confidence based on context\n",
        "                enhanced_confidence = self._enhance_confidence(mention, text)\n",
        "                mention['confidence'] = enhanced_confidence\n",
        "\n",
        "                # Only keep mentions with reasonable confidence\n",
        "                if enhanced_confidence > 0.3:\n",
        "                    # Extract context\n",
        "                    mention.update(self.extract_context_around_mention(text, mention))\n",
        "                    filtered_mentions.append(mention)\n",
        "\n",
        "            return filtered_mentions\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Dataset mention detection failed: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _enhance_confidence(self, mention: Dict, text: str) -> float:\n",
        "        \"\"\"Enhance confidence score based on contextual clues\"\"\"\n",
        "        confidence = mention.get('confidence', 0.5)\n",
        "\n",
        "        mention_text = mention['text'].lower()\n",
        "        context = text[max(0, mention['start']-50):min(len(text), mention['end']+50)].lower()\n",
        "\n",
        "        # Boost confidence for specific patterns\n",
        "        if any(word in mention_text for word in ['imagenet', 'cifar', 'mnist', 'coco', 'squad', 'glue']):\n",
        "            confidence += 0.3\n",
        "\n",
        "        # Boost for technical context\n",
        "        technical_terms = ['train', 'test', 'validate', 'evaluate', 'accuracy', 'performance']\n",
        "        if any(term in context for term in technical_terms):\n",
        "            confidence += 0.2\n",
        "\n",
        "        # Penalize very short mentions without context\n",
        "        if len(mention_text) < 5 and 'dataset' not in mention_text:\n",
        "            confidence -= 0.2\n",
        "\n",
        "        return min(max(confidence, 0.1), 1.0)\n",
        "\n",
        "    def extract_context_around_mention(self, text: str, mention: Dict) -> Dict:\n",
        "        \"\"\"Extract context around dataset mention\"\"\"\n",
        "        start, end = mention['start'], mention['end']\n",
        "\n",
        "        context_start = max(0, start - config.CONTEXT_WINDOW)\n",
        "        context_end = min(len(text), end + config.CONTEXT_WINDOW)\n",
        "\n",
        "        left_context = text[context_start:start]\n",
        "        right_context = text[end:context_end]\n",
        "        mention_text = text[start:end]\n",
        "\n",
        "        return {\n",
        "            'left_context': left_context.strip(),\n",
        "            'mention_text': mention_text,\n",
        "            'right_context': right_context.strip(),\n",
        "            'full_context': f\"{left_context} {mention_text} {right_context}\".strip(),\n",
        "            'context_start': context_start,\n",
        "            'context_end': context_end\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class PurposeClassifier:\n",
        "    def __init__(self):\n",
        "        self.purpose_keywords = self._initialize_purpose_keywords()\n",
        "\n",
        "    def _initialize_purpose_keywords(self) -> Dict[str, List[str]]:\n",
        "        \"\"\"Initialize purpose-specific keywords with enhanced coverage\"\"\"\n",
        "        return {\n",
        "            'training': [\n",
        "                'train', 'training', 'learn', 'learning', 'fit', 'fitting', 'optimize',\n",
        "                'optimization', 'parameter', 'weight', 'model training', 'train set',\n",
        "                'training data', 'learning algorithm', 'backpropagation', 'gradient descent'\n",
        "            ],\n",
        "            'validation': [\n",
        "                'validate', 'validation', 'tune', 'tuning', 'hyperparameter', 'development set',\n",
        "                'dev set', 'validation set', 'model selection', 'parameter tuning',\n",
        "                'cross validation', 'early stopping', 'hyperparameter optimization'\n",
        "            ],\n",
        "            'testing': [\n",
        "                'test', 'testing', 'evaluate', 'evaluation', 'assess', 'assessment',\n",
        "                'test set', 'testing data', 'performance', 'accuracy', 'result',\n",
        "                'benchmark', 'comparison', 'final evaluation', 'generalization'\n",
        "            ],\n",
        "            'evaluation': [\n",
        "                'evaluate', 'evaluation', 'assess', 'assessment', 'measure', 'metric',\n",
        "                'performance', 'accuracy', 'precision', 'recall', 'f1', 'auc', 'roc',\n",
        "                'metrics', 'quantitative analysis', 'qualitative analysis'\n",
        "            ],\n",
        "            'benchmarking': [\n",
        "                'benchmark', 'benchmarking', 'compare', 'comparison', 'baseline',\n",
        "                'state of the art', 'sota', 'competitive', 'performance comparison',\n",
        "                'leaderboard', 'standard benchmark', 'reference implementation'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def classify_purpose(self, context: Dict) -> Dict:\n",
        "        \"\"\"Enhanced purpose classification with contextual analysis\"\"\"\n",
        "        full_context = context['full_context'].lower()\n",
        "\n",
        "        purpose_scores = {purpose: 0.0 for purpose in self.purpose_keywords.keys()}\n",
        "\n",
        "        # Keyword-based scoring with position weighting\n",
        "        for purpose, keywords in self.purpose_keywords.items():\n",
        "            for keyword in keywords:\n",
        "                if keyword in full_context:\n",
        "                    # Calculate position-based weight\n",
        "                    weight = 1.0\n",
        "                    keyword_pos = full_context.find(keyword)\n",
        "\n",
        "                    # Higher weight for keywords closer to mention\n",
        "                    mention_pos = len(context['left_context'])\n",
        "                    distance = abs(keyword_pos - mention_pos)\n",
        "                    if distance < 50:\n",
        "                        weight *= 1.5\n",
        "                    elif distance < 100:\n",
        "                        weight *= 1.2\n",
        "\n",
        "                    purpose_scores[purpose] += weight\n",
        "\n",
        "        # Normalize scores\n",
        "        total_score = sum(purpose_scores.values())\n",
        "        if total_score > 0:\n",
        "            for purpose in purpose_scores:\n",
        "                purpose_scores[purpose] /= total_score\n",
        "\n",
        "        # Determine primary purpose\n",
        "        if purpose_scores:\n",
        "            primary_purpose = max(purpose_scores.items(), key=lambda x: x[1])\n",
        "            confidence = primary_purpose[1]\n",
        "            primary_purpose = primary_purpose[0]\n",
        "        else:\n",
        "            primary_purpose = 'unknown'\n",
        "            confidence = 0.0\n",
        "\n",
        "        return {\n",
        "            'primary_purpose': primary_purpose,\n",
        "            'confidence': confidence,\n",
        "            'purpose_scores': purpose_scores\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class DatasetImpactAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.dataset_usage_stats = defaultdict(lambda: {\n",
        "            'mention_count': 0,\n",
        "            'papers_count': 0,\n",
        "            'usage_types': Counter(),\n",
        "            'purposes': Counter(),\n",
        "            'contexts': []\n",
        "        })\n",
        "        self.label_source_stats = defaultdict(Counter)\n",
        "\n",
        "    def analyze_dataset_impact(self, analysis_results: List[Dict]) -> Dict:\n",
        "        \"\"\"Analyze impact metrics with robust error handling\"\"\"\n",
        "        print(\" Analyzing dataset impact metrics...\")\n",
        "\n",
        "        if not analysis_results:\n",
        "            return self._get_empty_impact_analysis()\n",
        "\n",
        "        try:\n",
        "            # Aggregate usage statistics\n",
        "            for result in analysis_results:\n",
        "                if 'dataset_mentions' not in result:\n",
        "                    continue\n",
        "\n",
        "                filename = result.get('filename', 'unknown')\n",
        "                label_source = result.get('label_source', 'unknown')\n",
        "                mentions = result['dataset_mentions']\n",
        "\n",
        "                for mention in mentions:\n",
        "                    dataset_name = mention.get('text', 'unknown_dataset')\n",
        "                    if dataset_name == 'unknown_dataset':\n",
        "                        continue\n",
        "\n",
        "                    stats = self.dataset_usage_stats[dataset_name]\n",
        "\n",
        "                    stats['mention_count'] += 1\n",
        "                    usage_type = mention.get('usage_type', 'unknown')\n",
        "                    stats['usage_types'][usage_type] += 1\n",
        "                    stats['purposes'][mention.get('purpose', {}).get('primary_purpose', 'unknown')] += 1\n",
        "                    stats['contexts'].append({\n",
        "                        'filename': filename,\n",
        "                        'usage_type': usage_type,\n",
        "                        'purpose': mention.get('purpose', {}).get('primary_purpose', 'unknown'),\n",
        "                        'label_source': label_source\n",
        "                    })\n",
        "\n",
        "                    # Track label source distribution\n",
        "                    self.label_source_stats[label_source][usage_type] += 1\n",
        "\n",
        "            # Remove duplicates for papers count\n",
        "            for dataset_name, stats in self.dataset_usage_stats.items():\n",
        "                unique_papers = set(context['filename'] for context in stats['contexts'])\n",
        "                stats['papers_count'] = len(unique_papers)\n",
        "\n",
        "            # Calculate impact metrics\n",
        "            impact_metrics = {}\n",
        "            for dataset_name, stats in self.dataset_usage_stats.items():\n",
        "                impact_metrics[dataset_name] = self._calculate_dataset_metrics(dataset_name, stats)\n",
        "\n",
        "            reproducibility_score = self._calculate_reproducibility_score(impact_metrics)\n",
        "\n",
        "            return {\n",
        "                'dataset_impact_metrics': impact_metrics,\n",
        "                'reproducibility_score': reproducibility_score,\n",
        "                'total_datasets_analyzed': len(impact_metrics),\n",
        "                'total_mentions': sum(stats['mention_count'] for stats in self.dataset_usage_stats.values()),\n",
        "                'label_source_distribution': dict(self.label_source_stats),\n",
        "                'usage_type_by_source': self._analyze_usage_by_source()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Impact analysis failed: {e}\")\n",
        "            return self._get_empty_impact_analysis()\n",
        "\n",
        "    def _analyze_usage_by_source(self) -> Dict:\n",
        "        \"\"\"Analyze usage type distribution by label source\"\"\"\n",
        "        usage_by_source = {}\n",
        "        for source, counter in self.label_source_stats.items():\n",
        "            usage_by_source[source] = dict(counter)\n",
        "        return usage_by_source\n",
        "\n",
        "    def _get_empty_impact_analysis(self):\n",
        "        \"\"\"Return empty impact analysis when no data is available\"\"\"\n",
        "        return {\n",
        "            'dataset_impact_metrics': {},\n",
        "            'reproducibility_score': 0.0,\n",
        "            'total_datasets_analyzed': 0,\n",
        "            'total_mentions': 0,\n",
        "            'label_source_distribution': {},\n",
        "            'usage_type_by_source': {}\n",
        "        }\n",
        "\n",
        "    def _calculate_dataset_metrics(self, dataset_name: str, stats: Dict) -> Dict:\n",
        "        \"\"\"Calculate comprehensive metrics for a dataset\"\"\"\n",
        "        total_mentions = stats['mention_count']\n",
        "        total_papers = stats['papers_count']\n",
        "\n",
        "        # Usage diversity\n",
        "        usage_diversity = len(stats['usage_types']) / len(config.USAGE_TYPES)\n",
        "\n",
        "        # Purpose diversity\n",
        "        purpose_diversity = len(stats['purposes']) / len(config.PURPOSE_TYPES)\n",
        "\n",
        "        # Impact score\n",
        "        frequency_score = min(1.0, total_mentions / 10)\n",
        "        diversity_score = (usage_diversity + purpose_diversity) / 2\n",
        "        impact_score = 0.6 * frequency_score + 0.4 * diversity_score\n",
        "\n",
        "        # Reproducibility indicators\n",
        "        reproducibility_indicators = {\n",
        "            'multiple_usage_contexts': len(stats['usage_types']) > 1,\n",
        "            'multiple_papers': total_papers > 1,\n",
        "            'consistent_purposes': len(stats['purposes']) >= 1\n",
        "        }\n",
        "\n",
        "        reproducibility_score = sum(reproducibility_indicators.values()) / len(reproducibility_indicators)\n",
        "\n",
        "        return {\n",
        "            'dataset_name': dataset_name,\n",
        "            'mention_count': total_mentions,\n",
        "            'papers_count': total_papers,\n",
        "            'usage_type_distribution': dict(stats['usage_types']),\n",
        "            'purpose_distribution': dict(stats['purposes']),\n",
        "            'usage_diversity': usage_diversity,\n",
        "            'purpose_diversity': purpose_diversity,\n",
        "            'impact_score': impact_score,\n",
        "            'reproducibility_score': reproducibility_score,\n",
        "            'reproducibility_indicators': reproducibility_indicators\n",
        "        }\n",
        "\n",
        "    def _calculate_reproducibility_score(self, impact_metrics: Dict) -> float:\n",
        "        \"\"\"Calculate overall reproducibility score\"\"\"\n",
        "        if not impact_metrics:\n",
        "            return 0.0\n",
        "\n",
        "        total_reproducibility = sum(metric['reproducibility_score'] for metric in impact_metrics.values())\n",
        "        return total_reproducibility / len(impact_metrics)\n",
        "\n",
        "\n",
        "\n",
        "class FixedResultsComparator:\n",
        "    def __init__(self):\n",
        "        self.comparison_results = {}\n",
        "\n",
        "    def compare_with_original_labels(self, analysis_results: List[Dict], original_labels_df: pd.DataFrame):\n",
        "        \"\"\"Compare detected results with original labels using cleaned filenames\"\"\"\n",
        "        print(\"\\n COMPARING RESULTS WITH ORIGINAL LABELS\")\n",
        "\n",
        "        # Clean original labels filenames for comparison\n",
        "        original_labels_df_clean = original_labels_df.copy()\n",
        "        original_labels_df_clean['filename_clean'] = original_labels_df_clean['filename'].apply(\n",
        "            lambda x: str(x).strip().lower()\n",
        "        )\n",
        "\n",
        "        # Extract detected usage types\n",
        "        detected_usage_types = []\n",
        "        original_usage_types = []\n",
        "        matched_files = []\n",
        "\n",
        "        for result in analysis_results:\n",
        "            filename = result.get('filename', '')  # This should be the cleaned filename\n",
        "            detected_type = result.get('usage_type', 'unknown')\n",
        "\n",
        "            # Find corresponding original label using cleaned filename\n",
        "            original_row = original_labels_df_clean[original_labels_df_clean['filename_clean'] == filename]\n",
        "            if not original_row.empty:\n",
        "                original_type = original_row.iloc[0]['type']\n",
        "                detected_usage_types.append(detected_type)\n",
        "                original_usage_types.append(original_type)\n",
        "                matched_files.append(filename)\n",
        "\n",
        "        if not detected_usage_types:\n",
        "            print(\" No matching files found for comparison\")\n",
        "            print(\" This might indicate a filename cleaning issue\")\n",
        "            return None\n",
        "\n",
        "        # Calculate accuracy and metrics\n",
        "        accuracy = accuracy_score(original_usage_types, detected_usage_types)\n",
        "        class_report = classification_report(original_usage_types, detected_usage_types, output_dict=True)\n",
        "        cm = confusion_matrix(original_usage_types, detected_usage_types, labels=['Primary', 'Secondary', 'Missing'])\n",
        "\n",
        "        self.comparison_results = {\n",
        "            'accuracy': accuracy,\n",
        "            'classification_report': class_report,\n",
        "            'confusion_matrix': cm.tolist(),\n",
        "            'labels': ['Primary', 'Secondary', 'Missing'],\n",
        "            'sample_size': len(detected_usage_types),\n",
        "            'matched_files_count': len(matched_files)\n",
        "        }\n",
        "\n",
        "        print(f\" Comparison completed:\")\n",
        "        print(f\"   • Sample size: {len(detected_usage_types)} files\")\n",
        "        print(f\"   • Accuracy: {accuracy:.3f}\")\n",
        "        print(f\"   • Matched files: {len(matched_files)}\")\n",
        "\n",
        "        return self.comparison_results\n",
        "\n",
        "    def generate_comparison_report(self, output_dir: str):\n",
        "        \"\"\"Generate comprehensive comparison report\"\"\"\n",
        "        if not self.comparison_results:\n",
        "            print(\" No comparison results available\")\n",
        "            return\n",
        "\n",
        "        report_path = os.path.join(output_dir, \"label_comparison_report.txt\")\n",
        "\n",
        "        with open(report_path, 'w') as f:\n",
        "            f.write(\"LABEL COMPARISON REPORT\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "            f.write(f\"Sample Size: {self.comparison_results['sample_size']} files\\n\")\n",
        "            f.write(f\"Overall Accuracy: {self.comparison_results['accuracy']:.3f}\\n\")\n",
        "            f.write(f\"Matched Files: {self.comparison_results['matched_files_count']}\\n\\n\")\n",
        "\n",
        "            f.write(\"Classification Report:\\n\")\n",
        "            for class_name, metrics in self.comparison_results['classification_report'].items():\n",
        "                if class_name in ['Primary', 'Secondary', 'Missing']:\n",
        "                    f.write(f\"  {class_name}:\\n\")\n",
        "                    f.write(f\"    Precision: {metrics['precision']:.3f}\\n\")\n",
        "                    f.write(f\"    Recall: {metrics['recall']:.3f}\\n\")\n",
        "                    f.write(f\"    F1-Score: {metrics['f1-score']:.3f}\\n\")\n",
        "                    f.write(f\"    Support: {metrics['support']}\\n\\n\")\n",
        "\n",
        "        print(f\" Comparison report saved to: {report_path}\")\n",
        "\n",
        "\n",
        "\n",
        "class FilteredDataCitationAnalysisPipeline:\n",
        "    def __init__(self):\n",
        "        self.data_loader = FilteredDataCitationDataLoader()\n",
        "        self.ner_system = HybridDatasetNER()\n",
        "        self.purpose_classifier = PurposeClassifier()\n",
        "        self.impact_analyzer = DatasetImpactAnalyzer()\n",
        "        self.comparator = FixedResultsComparator()\n",
        "\n",
        "    def run_complete_analysis(self):\n",
        "        \"\"\"Run the complete Phase 2 analysis pipeline with file type filtering\"\"\"\n",
        "        print(\" STARTING PHASE 2: MULTI-DIMENSIONAL DATA CITATION ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        try:\n",
        "            # Step 1: Load and prepare data with filtering\n",
        "            print(\"\\n STEP 1: DATA LOADING AND PREPARATION\")\n",
        "            data = self.data_loader.load_and_validate_data()\n",
        "            texts, filenames, usage_types, label_sources = self.data_loader.get_texts_for_analysis()\n",
        "\n",
        "            if len(texts) == 0:\n",
        "                print(\" No texts available for analysis\")\n",
        "                return None, None\n",
        "\n",
        "            # Step 2: Dataset mention detection\n",
        "            print(\"\\n STEP 2: HYBRID DATASET MENTION DETECTION\")\n",
        "            dataset_mentions_results = self._detect_dataset_mentions(texts, filenames, usage_types, label_sources)\n",
        "\n",
        "            # Step 3: Purpose classification\n",
        "            print(\"\\n STEP 3: PURPOSE CLASSIFICATION AND METHODOLOGY EXTRACTION\")\n",
        "            purpose_analysis_results = self._analyze_purposes(dataset_mentions_results)\n",
        "\n",
        "            # Step 4: Impact analysis\n",
        "            print(\"\\n STEP 4: DATASET IMPACT ANALYSIS\")\n",
        "            impact_analysis = self.impact_analyzer.analyze_dataset_impact(purpose_analysis_results)\n",
        "\n",
        "            # Step 5: Compare with original labels\n",
        "            print(\"\\n STEP 5: COMPARISON WITH ORIGINAL LABELS\")\n",
        "            comparison_results = self.comparator.compare_with_original_labels(\n",
        "                purpose_analysis_results, self.data_loader.train_labels_df\n",
        "            )\n",
        "\n",
        "            # Step 6: Save results\n",
        "            print(\"\\n STEP 6: SAVING COMPREHENSIVE RESULTS\")\n",
        "            self._save_comprehensive_results(purpose_analysis_results, impact_analysis, comparison_results)\n",
        "\n",
        "            print(\"\\n PHASE 2 COMPLETED SUCCESSFULLY!\")\n",
        "            return purpose_analysis_results, impact_analysis\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Phase 2 pipeline failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None, None\n",
        "\n",
        "    def _detect_dataset_mentions(self, texts: List[str], filenames: List[str], usage_types: List[str], label_sources: List[str]) -> List[Dict]:\n",
        "        \"\"\"Detect dataset mentions in all texts\"\"\"\n",
        "        results = []\n",
        "        total_mentions = 0\n",
        "\n",
        "        for i, (text, filename, usage_type, label_source) in enumerate(tqdm(\n",
        "            zip(texts, filenames, usage_types, label_sources),\n",
        "            total=len(texts),\n",
        "            desc=\"Detecting dataset mentions\"\n",
        "        )):\n",
        "            if not text or len(text.strip()) < 50:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Detect dataset mentions\n",
        "                mentions = self.ner_system.detect_dataset_mentions(text)\n",
        "\n",
        "                # Add usage type to each mention\n",
        "                for mention in mentions:\n",
        "                    mention['usage_type'] = usage_type\n",
        "\n",
        "                results.append({\n",
        "                    'filename': filename,\n",
        "                    'text_length': len(text),\n",
        "                    'dataset_mentions': mentions,\n",
        "                    'mentions_count': len(mentions),\n",
        "                    'usage_type': usage_type,\n",
        "                    'label_source': label_source\n",
        "                })\n",
        "\n",
        "                total_mentions += len(mentions)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\" Error processing document {filename}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\" Dataset mention detection completed: {total_mentions} mentions found in {len(results)} documents\")\n",
        "        return results\n",
        "\n",
        "    def _analyze_purposes(self, mention_results: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Analyze purposes for dataset mentions\"\"\"\n",
        "        analyzed_results = []\n",
        "\n",
        "        for result in tqdm(mention_results, desc=\"Analyzing purposes\"):\n",
        "            analyzed_mentions = []\n",
        "\n",
        "            for mention in result['dataset_mentions']:\n",
        "                try:\n",
        "                    # Classify purpose\n",
        "                    purpose_analysis = self.purpose_classifier.classify_purpose(mention)\n",
        "                    mention['purpose'] = purpose_analysis\n",
        "                    analyzed_mentions.append(mention)\n",
        "                except Exception as e:\n",
        "                    print(f\" Purpose analysis failed for mention: {e}\")\n",
        "                    continue\n",
        "\n",
        "            result['dataset_mentions'] = analyzed_mentions\n",
        "            analyzed_results.append(result)\n",
        "\n",
        "        print(f\" Purpose analysis completed for {len(analyzed_results)} documents\")\n",
        "        return analyzed_results\n",
        "\n",
        "    def _save_comprehensive_results(self, analysis_results: List[Dict], impact_analysis: Dict, comparison_results: Dict):\n",
        "        \"\"\"Save all Phase 2 results with robust error handling\"\"\"\n",
        "        output_dir = config.OUTPUT_DIR\n",
        "\n",
        "        try:\n",
        "            # Save detailed analysis results\n",
        "            detailed_results = []\n",
        "            for result in analysis_results:\n",
        "                for mention in result.get('dataset_mentions', []):\n",
        "                    detailed_results.append({\n",
        "                        'filename': result.get('filename', 'unknown'),\n",
        "                        'dataset_name': mention.get('text', 'unknown'),\n",
        "                        'mention_confidence': mention.get('confidence', 0),\n",
        "                        'usage_type': mention.get('usage_type', 'unknown'),\n",
        "                        'primary_purpose': mention.get('purpose', {}).get('primary_purpose', 'unknown'),\n",
        "                        'purpose_confidence': mention.get('purpose', {}).get('confidence', 0),\n",
        "                        'left_context': mention.get('left_context', '')[:200],\n",
        "                        'right_context': mention.get('right_context', '')[:200],\n",
        "                        'detection_source': mention.get('source', 'unknown'),\n",
        "                        'label_source': result.get('label_source', 'unknown')\n",
        "                    })\n",
        "\n",
        "            if detailed_results:\n",
        "                detailed_df = pd.DataFrame(detailed_results)\n",
        "                detailed_file = os.path.join(output_dir, \"detailed_citation_analysis.csv\")\n",
        "                detailed_df.to_csv(detailed_file, index=False)\n",
        "                print(f\" Detailed analysis saved to: {detailed_file}\")\n",
        "\n",
        "                # Save impact analysis\n",
        "                impact_file = os.path.join(output_dir, \"dataset_impact_analysis.json\")\n",
        "                with open(impact_file, 'w') as f:\n",
        "                    json.dump(impact_analysis, f, indent=2, default=str)\n",
        "                print(f\" Impact analysis saved to: {impact_file}\")\n",
        "\n",
        "                # Save impact report\n",
        "                report_file = os.path.join(output_dir, \"impact_analysis_report.txt\")\n",
        "                report = self._generate_impact_report(impact_analysis, len(detailed_results))\n",
        "                with open(report_file, 'w') as f:\n",
        "                    f.write(report)\n",
        "                print(f\" Impact report saved to: {report_file}\")\n",
        "\n",
        "                # Save comparison results\n",
        "                if comparison_results:\n",
        "                    comparison_file = os.path.join(output_dir, \"label_comparison_results.json\")\n",
        "                    with open(comparison_file, 'w') as f:\n",
        "                        json.dump(comparison_results, f, indent=2)\n",
        "                    print(f\" Comparison results saved to: {comparison_file}\")\n",
        "\n",
        "                    # Generate comparison report\n",
        "                    self.comparator.generate_comparison_report(output_dir)\n",
        "            else:\n",
        "                print(\" No detailed results to save\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error saving results: {e}\")\n",
        "\n",
        "    def _generate_impact_report(self, impact_analysis: Dict, total_mentions: int) -> str:\n",
        "        \"\"\"Generate impact report\"\"\"\n",
        "        report = []\n",
        "        report.append(\"=\" * 80)\n",
        "        report.append(\"DATASET IMPACT ANALYSIS REPORT\")\n",
        "        report.append(\"=\" * 80)\n",
        "\n",
        "        report.append(f\"\\n OVERVIEW:\")\n",
        "        report.append(f\"   • Total datasets analyzed: {impact_analysis['total_datasets_analyzed']}\")\n",
        "        report.append(f\"   • Total mentions: {impact_analysis['total_mentions']}\")\n",
        "        report.append(f\"   • Overall reproducibility score: {impact_analysis['reproducibility_score']:.3f}\")\n",
        "\n",
        "        # Label source distribution\n",
        "        if 'label_source_distribution' in impact_analysis:\n",
        "            report.append(f\"\\n LABEL SOURCE DISTRIBUTION:\")\n",
        "            for source, count in impact_analysis['label_source_distribution'].items():\n",
        "                report.append(f\"   • {source}: {sum(count.values())} mentions\")\n",
        "\n",
        "        # Usage type by source\n",
        "        if 'usage_type_by_source' in impact_analysis:\n",
        "            report.append(f\"\\n USAGE TYPE BY SOURCE:\")\n",
        "            for source, usage_dist in impact_analysis['usage_type_by_source'].items():\n",
        "                report.append(f\"   • {source}: {usage_dist}\")\n",
        "\n",
        "        if impact_analysis['dataset_impact_metrics']:\n",
        "            report.append(f\"\\n TOP DATASETS BY IMPACT SCORE:\")\n",
        "            top_datasets = sorted(\n",
        "                impact_analysis['dataset_impact_metrics'].items(),\n",
        "                key=lambda x: x[1]['impact_score'],\n",
        "                reverse=True\n",
        "            )[:5]\n",
        "\n",
        "            for dataset_name, metrics in top_datasets:\n",
        "                report.append(f\"   • {dataset_name}:\")\n",
        "                report.append(f\"     - Impact Score: {metrics['impact_score']:.3f}\")\n",
        "                report.append(f\"     - Mentions: {metrics['mention_count']}\")\n",
        "                report.append(f\"     - Papers: {metrics['papers_count']}\")\n",
        "                report.append(f\"     - Reproducibility: {metrics['reproducibility_score']:.3f}\")\n",
        "                report.append(f\"     - Usage Types: {metrics['usage_type_distribution']}\")\n",
        "        else:\n",
        "            report.append(f\"\\n No dataset impact metrics available\")\n",
        "\n",
        "        return \"\\n\".join(report)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\" PHASE 2: MULTI-DIMENSIONAL DATA CITATION ANALYSIS - FILTERED VERSION\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Execute Phase 2 pipeline\n",
        "    phase2_pipeline = FilteredDataCitationAnalysisPipeline()\n",
        "    analysis_results, impact_analysis = phase2_pipeline.run_complete_analysis()\n",
        "\n",
        "    if analysis_results is not None:\n",
        "        # Print final summary\n",
        "        total_mentions = sum(len(result['dataset_mentions']) for result in analysis_results)\n",
        "        unique_datasets = set()\n",
        "        label_sources = Counter()\n",
        "\n",
        "        for result in analysis_results:\n",
        "            for mention in result['dataset_mentions']:\n",
        "                unique_datasets.add(mention['text'])\n",
        "            label_sources[result.get('label_source', 'unknown')] += 1\n",
        "\n",
        "        print(f\"\\n PHASE 2 FINAL SUMMARY:\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\" Detection Results:\")\n",
        "        print(f\"   • Documents analyzed: {len(analysis_results)}\")\n",
        "        print(f\"   • Dataset mentions found: {total_mentions}\")\n",
        "        print(f\"   • Unique datasets: {len(unique_datasets)}\")\n",
        "        print(f\"   • Label sources: {dict(label_sources)}\")\n",
        "\n",
        "        if impact_analysis:\n",
        "            print(f\" Impact Analysis:\")\n",
        "            print(f\"   • Overall reproducibility: {impact_analysis['reproducibility_score']:.3f}\")\n",
        "            print(f\"   • Total datasets with impact metrics: {impact_analysis['total_datasets_analyzed']}\")\n",
        "\n",
        "        print(f\"\\n Results saved to: {config.OUTPUT_DIR}\")\n",
        "        print(\" Phase 2 completed successfully!\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n Phase 2 execution failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jln_6OWq1u80"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "output_dir = \"/content/phase2_data_citation_analysis\"\n",
        "zip_path = f\"{output_dir}.zip\"\n",
        "\n",
        "# Zip the directory\n",
        "!zip -r \"$zip_path\" \"$output_dir\"\n",
        "\n",
        "# Download the zip file\n",
        "if os.path.exists(zip_path):\n",
        "  files.download(zip_path)\n",
        "else:\n",
        "  print(f\"Zip file not found at {zip_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNJboLCjpXLp4QzHprgWd6w",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}